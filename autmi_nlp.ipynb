{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTMI Seminar 2019/2020 Spring\n",
    "\n",
    "## Natural Language Processing\n",
    "\n",
    "## April , 2020\n",
    "\n",
    "# Text representations and analysis\n",
    "\n",
    "## Preparation\n",
    "\n",
    "[Download GLOVE](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "## Representations\n",
    "\n",
    "For humans meaningful representation are strings, but the computer needs numerical representations to be able to run machine learning algorithms. The easiest approach is to create a `word ---> id` mapping that is going to map words to integer ids starting from 0. Different words should have a different id.\n",
    "\n",
    "This is called **one-hot encoding**. Let's encode the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"yesterday the lazy dog went to the store to buy food\".split(\" \")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = dict()\n",
    "max_id = 0\n",
    "\n",
    "for word in sentence:\n",
    "    # a word we have not seen before\n",
    "    if word not in mapping:\n",
    "        # assign the smallest unused id\n",
    "        mapping[word] = max_id\n",
    "        # increment the id for the next word\n",
    "        max_id = max_id + 1\n",
    "        \n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "- When representing words with id's we assign them to the words in the order of the encounter. \n",
    "- This means that we will assign different vectors to the words each time we run the algorithm.\n",
    "- We also have no concept of similarity, intuitively: `similarity(cat, dog) > similarity(cat, computer)`\n",
    "- The representation is very sparse and could have very high dimension, which would also slow the computations.\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "- map each word to a small dimensional (around 100-300) continuous vectors.\n",
    "- this means that similar words should have similar vectors.\n",
    "    - what do we mean by word similarity ?\n",
    "    \n",
    "    \n",
    "### Cosine similarity\n",
    "\n",
    "- Now that we have word vectors, we need a way to quantify the similarity between individual words, according to these vectors. One such metric is cosine-similarity. We will be using this to find words that are \"close\" and \"far\" from one another.\n",
    "\n",
    "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$ \n",
    "\n",
    "    \n",
    "### Creating word embeddings\n",
    "\n",
    "\"a word is characterized by the company it keeps\" -- popularized by Firth\n",
    "\n",
    "- A popular theory is that words are as similar as their context is\n",
    "- Word embeddings are also created with neural networks\n",
    "    1. predict a missing word based on its context\n",
    "    2. predict a word's context given the word itself\n",
    "\n",
    "### Most famous embeddings for English\n",
    "\n",
    "- Word2vec\n",
    "- GLOVE\n",
    "\n",
    "Both have various versions, we will use a GLOVE embedding of 100 dimensional vectors trained on 6B tokens.\n",
    "\n",
    "[Download GLOVE](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-994-9aac71cb56a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# embed words in sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflair_embedding_forward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# The sentence objects holds a sentence that we may want to embed or tag\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "\n",
    "# init embedding\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "\n",
    "# create a sentence\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# embed words in sentence\n",
    "embeddings = flair_embedding_forward.embed(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence: \"so what 's your budget for two thousand twenty fourteen thousand dollars i see and when do you plan to implement the product in in april okay that 's perfect to want to we planned for a april and can we sign the contract in may yes please .\" - 49 Tokens, Sentence: \"yeah that 's what i 'm trying to find out do you have that information for me you know okay\" - 20 Tokens]\n"
     ]
    }
   ],
   "source": [
    "# your text of many sentences\n",
    "text = \" so what's your budget for two thousand twenty fourteen thousand dollars i see and when do you plan to implement the product in in april okay that's perfect to want to we planned for a april and can we sign the contract in may yes please. yeah that's what i'm trying to find out do you have that information for me you know okay\"\n",
    "\n",
    "# use a library to split into sentences\n",
    "from segtok.segmenter import split_single\n",
    "\n",
    "sentences = [Sentence(sent, use_tokenizer=True) for sent in split_single(text)]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "embedding_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embedding = gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_vector = embedding[\"dog\"]\n",
    "type(dog_vector), dog_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.most_similar(\"president\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.similarity(\"woman\", \"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model, size=500):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "    for word in model.wv.vocab:\n",
    "        if len(tokens) > size:\n",
    "            break\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(embedding, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "matplotlib.pyplot.rcParams['figure.figsize'] = (16, 10)\n",
    "matplotlib.pyplot.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.pyplot.rcParams['font.size'] = 20\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /userhome/student/adaamko/miniconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/userhome/student/adaamko/miniconda3/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/userhome/student/adaamko/miniconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analyzation\n",
    "\n",
    "- we use nlp frameworks for the basic tasks\n",
    "- for the preprocessing tasks (lemmatization, tokenization) we use [spaCy](https://spacy.io/)\n",
    "- for keyword extraction and various text analyzation tasks we use [textacy](https://github.com/chartbeat-labs/textacy)\n",
    "- textacy builds on spaCy output\n",
    "- both are open source ython libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext.datasets import text_classification\n",
    "import os\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./data', ngrams=NGRAMS, vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"./data/ag_news_csv/train.csv\",quotechar='\"', names=['label', 'title', 'description'])\n",
    "test_data = pd.read_csv(\"./data/ag_news_csv/test.csv\",quotechar='\"', names=['label', 'title', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"text\"] = train_data.title +  \",\" + train_data.description\n",
    "train_data = train_data.drop(\"title\", axis=1)\n",
    "train_data = train_data.drop(\"description\", axis=1)\n",
    "\n",
    "test_data[\"text\"] = test_data.title +  \",\" + test_data.description\n",
    "test_data = test_data.drop(\"title\", axis=1)\n",
    "test_data = test_data.drop(\"description\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7fbe2ed9ffd0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 975,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.groupby(train_data.label).size().plot.pie(subplots=True,figsize=(5, 10),autopct=\"%.0lf%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Donald Trump called and asked me to serve as his running mate and Vice Presidential nominee.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in doc:\n",
    "    print(tok.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 100})\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sports = train_data[train_data.label == 2]\n",
    "\n",
    "text = \" \".join(text_sports.text.tolist())\n",
    "doc_text = nlp(text[:1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gold medal', 384),\n",
       " ('new york', 243),\n",
       " ('united states', 234),\n",
       " ('athens olympics', 121),\n",
       " ('red sox', 119),\n",
       " ('olympic games', 114),\n",
       " ('olympic gold', 110),\n",
       " ('ryder cup', 102),\n",
       " ('sports network', 102),\n",
       " ('paul hamm', 86)]"
      ]
     },
     "execution_count": 914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textacy\n",
    "from textacy.extract import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "Counter([ng.text.lower() for n in [2,4] for ng in ngrams(doc_text, n)]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textacy can use graph based keyword extraction methods.\n",
    "\n",
    "* TextRank (focuses on words)\n",
    "* SingleRank (focueses on phrases)\n",
    "* [SGRank](http://www.aclweb.org/anthology/S15-1013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AP),AP', 0.0055519957775050244),\n",
       " ('ATHENS', 0.0042262513450477355),\n",
       " ('olympic', 0.0042167489415089006),\n",
       " ('night', 0.003950307258474872),\n",
       " ('team', 0.003912641706514775),\n",
       " ('gold', 0.0038847526698520553),\n",
       " ('Olympics', 0.003518805681814526),\n",
       " ('year', 0.0035067900688865797),\n",
       " ('game', 0.0034290486710939635),\n",
       " ('Athens', 0.0033117943012753843)]"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy import keyterms\n",
    "\n",
    "keyterms.textrank(\n",
    "    doc_text,\n",
    "    normalize = \"lemma\",\n",
    "    n_keyterms=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textacy.keyterms.singlerank(\n",
    "    doc_text,\n",
    "    normalize = \"lemma\",\n",
    "    n_keyterms=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textacy.keyterms.sgrank(\n",
    "    doc_text, \n",
    "    normalize=\"lemma\",\n",
    "    ngrams = (2,3,4,5,6),\n",
    "    n_keyterms=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract entities from the doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter \n",
    "words = [tok for tok in doc_text if tok.is_alpha and not tok.is_stop]\n",
    "word_probs = {tok.text.lower(): tok.prob for tok in words}\n",
    "\n",
    "freqs = Counter(tok.text for tok in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "print(len(freqs))\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=30, scale=1.5).generate_from_frequencies(freqs)\n",
    "image = wordcloud.to_image()\n",
    "image.save(\"./wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename='./wordcloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 925,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = train_data.groupby('label').apply(lambda x: x.sample(frac=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a new column to the table which will contain the cleaned and preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "clean_text = []\n",
    "for text in tqdm(sample_df['text']):\n",
    "    doc = nlp(text)\n",
    "    words = []\n",
    "    for tok in doc:\n",
    "        if not tok.is_stop and tok.is_alpha:\n",
    "            words.append(tok.lemma_)\n",
    "    clean_text.append(words)\n",
    "\n",
    "# Add cleaned text to dataframe\n",
    "sample_df['clean_text'] = clean_text\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 979,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set variables for dependent and independent variables\n",
    "labels = sample_df.label.tolist()\n",
    "data = sample_df['clean_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the pretrained glove embedding\n",
    "# To handle the Seq2Vec method, we take the mean of the word-vectors\n",
    "def vectorize(tr_data, tst_data):\n",
    "    print('\\nLoading existing glove model...')\n",
    "    embedding_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=False)\n",
    "    vectorizer = model.wv\n",
    "    vocab_length = len(model.wv.vocab)\n",
    "    \n",
    "    tr_vectors = [\n",
    "        np.array(np.mean([vectorizer[word] if word in model else np.zeros((100,)) for word in article], axis=0)) for article in tqdm(tr_data,'Vectorizing')\n",
    "    ]\n",
    "    \n",
    "    tst_vectors = [\n",
    "        np.array(np.mean([vectorizer[word] if word in model else np.zeros((100,)) for word in article], axis=0)) for article in tqdm(tst_data,'Vectorizing')\n",
    "    ]\n",
    "    \n",
    "    return tr_vectors, tst_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(data, labels):\n",
    "    tr_data,tst_data,tr_labels,tst_labels = split(data,labels,test_size=0.3)\n",
    "    \n",
    "    tst_vecs = []\n",
    "    tr_vecs = []\n",
    "    tr_vecs, tst_vecs = vectorize(tr_data, tst_data)    \n",
    "    return tr_vecs, tr_labels, tst_vecs, tst_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading existing glove model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/student/adaamko/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/userhome/student/adaamko/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "\n",
      "\n",
      "Vectorizing:   0%|          | 0/16800 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  10%|█         | 1740/16800 [00:00<00:00, 17395.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  21%|██        | 3458/16800 [00:00<00:00, 17327.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  31%|███       | 5198/16800 [00:00<00:00, 17347.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  41%|████▏     | 6962/16800 [00:00<00:00, 17433.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  52%|█████▏    | 8711/16800 [00:00<00:00, 17447.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  62%|██████▏   | 10442/16800 [00:00<00:00, 17405.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  73%|███████▎  | 12201/16800 [00:00<00:00, 17458.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  83%|████████▎ | 13965/16800 [00:00<00:00, 17510.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  94%|█████████▎| 15737/16800 [00:00<00:00, 17570.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing: 100%|██████████| 16800/16800 [00:00<00:00, 17286.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:   0%|          | 0/7200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  24%|██▍       | 1725/7200 [00:00<00:00, 17246.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  48%|████▊     | 3432/7200 [00:00<00:00, 17189.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  72%|███████▏  | 5151/7200 [00:00<00:00, 17189.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing:  96%|█████████▌| 6903/7200 [00:00<00:00, 17285.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "Vectorizing: 100%|██████████| 7200/7200 [00:00<00:00, 17175.00it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "tr_vecs, tr_labels, tst_vecs, tst_labels = get_features_and_labels(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can try different classifiers as well\n",
    "- Multiple are available from [scikit-learn](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf  =  RandomForestClassifier(n_estimators=100, verbose=True, n_jobs=-1)\n",
    "svc = SVC()\n",
    "lr  = LogisticRegression(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.0s finished\n",
      "/userhome/student/adaamko/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/userhome/student/adaamko/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/userhome/student/adaamko/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/userhome/student/adaamko/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1544: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=-1, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 972,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(tr_vecs, tr_labels)\n",
    "svc.fit(tr_vecs, tr_labels)\n",
    "lr.fit(tr_vecs, tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test accuracy : 0.8320833333333333\n",
      "SVC Test accuracy : 0.8391666666666666\n",
      "Logistic Regression Test accuracy : 0.8426388888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(type(tst_vecs))\n",
    "rf_pred = rf.predict(tst_vecs)\n",
    "svc_pred = svc.predict(tst_vecs)\n",
    "lr_pred = lr.predict(tst_vecs)\n",
    "print(\"Random Forest Test accuracy : {}\".format(accuracy_score(tst_labels, rf_pred)))\n",
    "print(\"SVC Test accuracy : {}\".format(accuracy_score(tst_labels, svc_pred)))\n",
    "print(\"Logistic Regression Test accuracy : {}\".format(accuracy_score(tst_labels, lr_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data,tst_data,tr_labels,tst_labels = split(data,labels,test_size=0.3)\n",
    "\n",
    "with open(\"train.txt\", \"w+\") as f:\n",
    "    for sample, label in zip(tr_data, tr_labels):\n",
    "        f.write(\" \".join(sample) + \"\\t\" + str(label) + \"\\n\")\n",
    "        \n",
    "with open(\"dev.txt\", \"w+\") as f:\n",
    "    for sample, label in zip(tst_data, tst_labels):\n",
    "        f.write(\" \".join(sample) + \"\\t\" + str(label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Learning model with pytorch and torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"dataset_test.csv\", index=False)\n",
    "train_data.to_csv(\"dataset_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('label',LABEL),('text', TEXT)]\n",
    "\n",
    "train, test = data.TabularDataset.splits(\n",
    "                                        path = '.',\n",
    "                                        train = 'dataset_train.csv',\n",
    "                                        test = 'dataset_test.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '3', 'text': ['Wall', 'St.', 'Bears', 'Claw', 'Back', 'Into', 'the', 'Black', '(', 'Reuters),Reuters', '-', 'Short', '-', 'sellers', ',', 'Wall', 'Street', \"'s\", 'dwindling\\\\band', 'of', 'ultra', '-', 'cynics', ',', 'are', 'seeing', 'green', 'again', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train, valid = train.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 84000\n",
      "Number of validation examples: 36000\n",
      "Number of testing examples: 7600\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train)}')\n",
    "print(f'Number of validation examples: {len(valid)}')\n",
    "print(f'Number of testing examples: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors =\"glove.6B.100d\")  \n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train, valid, test), batch_size = BATCH_SIZE,\n",
    "                                                                           sort_key = lambda x: len(x.text),\n",
    "                                                                           sort_within_batch = False,\n",
    "                                                                           device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        self.embedding.weight.requires_grad=False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.lstm(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "                \n",
    "        y = self.fc(hidden[-1])\n",
    "        \n",
    "        log_probs = F.log_softmax(y.squeeze(0))\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 4\n",
    "\n",
    "model = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    target_names = ['class 0', 'class 1', 'class 2', 'class 3']\n",
    "    print(classification_report(rounded_preds.cpu().numpy(), y.cpu().numpy(), target_names=target_names))\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.text)\n",
    "\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/student/adaamko/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.426 | Train Acc: 85.65%\n",
      "\t Val. Loss: 0.577 |  Val. Acc: 77.96%\n",
      "Epoch: 02 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.385 | Train Acc: 86.58%\n",
      "\t Val. Loss: 0.505 |  Val. Acc: 82.42%\n",
      "Epoch: 03 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.359 | Train Acc: 87.44%\n",
      "\t Val. Loss: 0.496 |  Val. Acc: 81.07%\n",
      "Epoch: 04 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.339 | Train Acc: 88.11%\n",
      "\t Val. Loss: 0.456 |  Val. Acc: 84.37%\n",
      "Epoch: 05 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.321 | Train Acc: 88.71%\n",
      "\t Val. Loss: 0.488 |  Val. Acc: 82.33%\n",
      "Epoch: 06 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.304 | Train Acc: 89.31%\n",
      "\t Val. Loss: 0.441 |  Val. Acc: 83.48%\n",
      "Epoch: 07 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.289 | Train Acc: 89.83%\n",
      "\t Val. Loss: 0.422 |  Val. Acc: 85.32%\n",
      "Epoch: 08 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.275 | Train Acc: 90.34%\n",
      "\t Val. Loss: 0.400 |  Val. Acc: 85.52%\n",
      "Epoch: 09 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.262 | Train Acc: 90.84%\n",
      "\t Val. Loss: 0.411 |  Val. Acc: 85.39%\n",
      "Epoch: 10 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.250 | Train Acc: 91.29%\n",
      "\t Val. Loss: 0.391 |  Val. Acc: 86.58%\n",
      "Epoch: 11 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.247 | Train Acc: 91.41%\n",
      "\t Val. Loss: 0.396 |  Val. Acc: 85.83%\n",
      "Epoch: 12 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.225 | Train Acc: 92.15%\n",
      "\t Val. Loss: 0.441 |  Val. Acc: 84.21%\n",
      "Epoch: 13 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.215 | Train Acc: 92.57%\n",
      "\t Val. Loss: 0.418 |  Val. Acc: 85.42%\n",
      "Epoch: 14 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.205 | Train Acc: 92.86%\n",
      "\t Val. Loss: 0.403 |  Val. Acc: 86.06%\n",
      "Epoch: 15 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.197 | Train Acc: 93.21%\n",
      "\t Val. Loss: 0.390 |  Val. Acc: 86.25%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/student/adaamko/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.25      0.40         4\n",
      "     class 1       0.62      0.67      0.64        12\n",
      "     class 2       0.76      0.80      0.78        35\n",
      "     class 3       0.62      0.62      0.62        13\n",
      "\n",
      "    accuracy                           0.70        64\n",
      "   macro avg       0.75      0.58      0.61        64\n",
      "weighted avg       0.72      0.70      0.70        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.89      0.94         9\n",
      "     class 1       0.84      0.89      0.86        18\n",
      "     class 2       0.86      0.83      0.84        29\n",
      "     class 3       0.56      0.62      0.59         8\n",
      "\n",
      "    accuracy                           0.83        64\n",
      "   macro avg       0.81      0.81      0.81        64\n",
      "weighted avg       0.84      0.83      0.83        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.70      0.82        10\n",
      "     class 1       0.84      1.00      0.91        21\n",
      "     class 2       0.86      0.72      0.78        25\n",
      "     class 3       0.55      0.75      0.63         8\n",
      "\n",
      "    accuracy                           0.81        64\n",
      "   macro avg       0.81      0.79      0.79        64\n",
      "weighted avg       0.83      0.81      0.81        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.78      0.88      0.82         8\n",
      "     class 1       0.65      0.85      0.73        13\n",
      "     class 2       0.83      0.78      0.81        32\n",
      "     class 3       0.50      0.36      0.42        11\n",
      "\n",
      "    accuracy                           0.73        64\n",
      "   macro avg       0.69      0.72      0.70        64\n",
      "weighted avg       0.73      0.73      0.73        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.91      0.91      0.91        11\n",
      "     class 1       0.96      0.85      0.90        26\n",
      "     class 2       0.86      0.90      0.88        21\n",
      "     class 3       0.50      0.67      0.57         6\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.81      0.83      0.82        64\n",
      "weighted avg       0.88      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.67      0.57      0.62         7\n",
      "     class 1       0.63      0.86      0.73        22\n",
      "     class 2       0.86      0.69      0.77        26\n",
      "     class 3       0.43      0.33      0.38         9\n",
      "\n",
      "    accuracy                           0.69        64\n",
      "   macro avg       0.65      0.62      0.62        64\n",
      "weighted avg       0.70      0.69      0.68        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.90      0.90        10\n",
      "     class 1       0.65      0.87      0.74        15\n",
      "     class 2       0.76      0.62      0.68        26\n",
      "     class 3       0.54      0.54      0.54        13\n",
      "\n",
      "    accuracy                           0.70        64\n",
      "   macro avg       0.71      0.73      0.72        64\n",
      "weighted avg       0.71      0.70      0.70        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        13\n",
      "     class 1       0.67      1.00      0.80        10\n",
      "     class 2       0.95      0.75      0.84        28\n",
      "     class 3       0.79      0.85      0.81        13\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.85      0.90      0.86        64\n",
      "weighted avg       0.88      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.82      0.82      0.82        11\n",
      "     class 1       0.60      0.90      0.72        10\n",
      "     class 2       0.82      0.79      0.81        29\n",
      "     class 3       0.80      0.57      0.67        14\n",
      "\n",
      "    accuracy                           0.77        64\n",
      "   macro avg       0.76      0.77      0.75        64\n",
      "weighted avg       0.78      0.77      0.76        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.77      0.91      0.83        11\n",
      "     class 1       0.67      0.62      0.65        16\n",
      "     class 2       0.79      0.79      0.79        24\n",
      "     class 3       0.58      0.54      0.56        13\n",
      "\n",
      "    accuracy                           0.72        64\n",
      "   macro avg       0.70      0.72      0.71        64\n",
      "weighted avg       0.71      0.72      0.72        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.85      0.85      0.85        20\n",
      "     class 1       0.79      0.94      0.86        16\n",
      "     class 2       0.83      0.71      0.77        14\n",
      "     class 3       0.85      0.79      0.81        14\n",
      "\n",
      "    accuracy                           0.83        64\n",
      "   macro avg       0.83      0.82      0.82        64\n",
      "weighted avg       0.83      0.83      0.83        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.85      0.94      0.89        18\n",
      "     class 1       0.75      0.90      0.82        10\n",
      "     class 2       0.93      0.70      0.80        20\n",
      "     class 3       0.82      0.88      0.85        16\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.84      0.85      0.84        64\n",
      "weighted avg       0.85      0.84      0.84        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.86      0.88        21\n",
      "     class 1       0.67      0.91      0.77        11\n",
      "     class 2       0.69      0.65      0.67        17\n",
      "     class 3       0.92      0.80      0.86        15\n",
      "\n",
      "    accuracy                           0.80        64\n",
      "   macro avg       0.79      0.80      0.79        64\n",
      "weighted avg       0.81      0.80      0.80        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.88      0.95      0.91        22\n",
      "     class 1       1.00      0.82      0.90        11\n",
      "     class 2       0.93      0.74      0.82        19\n",
      "     class 3       0.62      0.83      0.71        12\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.86      0.84      0.84        64\n",
      "weighted avg       0.87      0.84      0.85        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.93      0.88      0.90        16\n",
      "     class 1       0.88      0.83      0.86        18\n",
      "     class 2       0.89      0.80      0.84        20\n",
      "     class 3       0.64      0.90      0.75        10\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.84      0.85      0.84        64\n",
      "weighted avg       0.86      0.84      0.85        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        16\n",
      "     class 1       1.00      0.79      0.88        14\n",
      "     class 2       0.84      0.89      0.86        18\n",
      "     class 3       0.78      0.88      0.82        16\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.90      0.89      0.89        64\n",
      "weighted avg       0.90      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      1.00      0.96        22\n",
      "     class 1       0.67      0.80      0.73        10\n",
      "     class 2       0.92      0.63      0.75        19\n",
      "     class 3       0.67      0.77      0.71        13\n",
      "\n",
      "    accuracy                           0.81        64\n",
      "   macro avg       0.79      0.80      0.79        64\n",
      "weighted avg       0.83      0.81      0.81        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.89      0.94      0.92        18\n",
      "     class 1       0.89      0.94      0.91        17\n",
      "     class 2       0.87      0.93      0.90        14\n",
      "     class 3       0.92      0.73      0.81        15\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.89      0.89      0.89        64\n",
      "weighted avg       0.89      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        16\n",
      "     class 1       0.94      1.00      0.97        17\n",
      "     class 2       0.89      0.81      0.85        21\n",
      "     class 3       0.73      0.80      0.76        10\n",
      "\n",
      "    accuracy                           0.91        64\n",
      "   macro avg       0.89      0.90      0.90        64\n",
      "weighted avg       0.91      0.91      0.91        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.79      1.00      0.88        19\n",
      "     class 1       0.83      0.91      0.87        11\n",
      "     class 2       0.78      0.61      0.68        23\n",
      "     class 3       0.60      0.55      0.57        11\n",
      "\n",
      "    accuracy                           0.77        64\n",
      "   macro avg       0.75      0.77      0.75        64\n",
      "weighted avg       0.76      0.77      0.76        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.93      1.00      0.97        14\n",
      "     class 1       0.88      0.93      0.90        15\n",
      "     class 2       1.00      0.71      0.83        28\n",
      "     class 3       0.54      1.00      0.70         7\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.84      0.91      0.85        64\n",
      "weighted avg       0.91      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.95      0.95        19\n",
      "     class 1       0.86      0.92      0.89        13\n",
      "     class 2       0.94      0.79      0.86        19\n",
      "     class 3       0.73      0.85      0.79        13\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.87      0.88      0.87        64\n",
      "weighted avg       0.88      0.88      0.88        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      1.00      0.96        24\n",
      "     class 1       0.85      1.00      0.92        11\n",
      "     class 2       0.77      0.67      0.71        15\n",
      "     class 3       0.92      0.79      0.85        14\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.86      0.86      0.86        64\n",
      "weighted avg       0.87      0.88      0.87        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.78      1.00      0.88        14\n",
      "     class 1       0.67      0.92      0.77        13\n",
      "     class 2       0.71      0.67      0.69        15\n",
      "     class 3       0.86      0.55      0.67        22\n",
      "\n",
      "    accuracy                           0.75        64\n",
      "   macro avg       0.75      0.78      0.75        64\n",
      "weighted avg       0.77      0.75      0.74        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.88      1.00      0.94        15\n",
      "     class 1       0.85      0.92      0.88        12\n",
      "     class 2       0.95      0.75      0.84        24\n",
      "     class 3       0.67      0.77      0.71        13\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.84      0.86      0.84        64\n",
      "weighted avg       0.86      0.84      0.84        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.92      0.92        13\n",
      "     class 1       0.75      0.83      0.79        18\n",
      "     class 2       0.93      0.65      0.76        20\n",
      "     class 3       0.59      0.77      0.67        13\n",
      "\n",
      "    accuracy                           0.78        64\n",
      "   macro avg       0.80      0.79      0.79        64\n",
      "weighted avg       0.81      0.78      0.78        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.95      0.95        19\n",
      "     class 1       0.80      0.73      0.76        11\n",
      "     class 2       0.94      0.83      0.88        18\n",
      "     class 3       0.74      0.88      0.80        16\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.86      0.85      0.85        64\n",
      "weighted avg       0.87      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      1.00      0.96        12\n",
      "     class 1       0.86      0.92      0.89        13\n",
      "     class 2       0.88      0.71      0.79        21\n",
      "     class 3       0.75      0.83      0.79        18\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.85      0.87      0.86        64\n",
      "weighted avg       0.85      0.84      0.84        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.86      0.92      0.89        13\n",
      "     class 1       1.00      0.94      0.97        16\n",
      "     class 2       0.86      0.69      0.77        26\n",
      "     class 3       0.43      0.67      0.52         9\n",
      "\n",
      "    accuracy                           0.80        64\n",
      "   macro avg       0.79      0.80      0.79        64\n",
      "weighted avg       0.83      0.80      0.81        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      1.00      0.97        17\n",
      "     class 1       0.71      0.71      0.71        14\n",
      "     class 2       0.64      0.64      0.64        14\n",
      "     class 3       0.78      0.74      0.76        19\n",
      "\n",
      "    accuracy                           0.78        64\n",
      "   macro avg       0.77      0.77      0.77        64\n",
      "weighted avg       0.78      0.78      0.78        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.95      0.97        20\n",
      "     class 1       0.92      0.92      0.92        12\n",
      "     class 2       0.88      0.78      0.82        18\n",
      "     class 3       0.76      0.93      0.84        14\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.89      0.89      0.89        64\n",
      "weighted avg       0.90      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.87      0.72      0.79        18\n",
      "     class 1       0.82      0.82      0.82        11\n",
      "     class 2       0.85      0.89      0.87        19\n",
      "     class 3       0.72      0.81      0.76        16\n",
      "\n",
      "    accuracy                           0.81        64\n",
      "   macro avg       0.81      0.81      0.81        64\n",
      "weighted avg       0.82      0.81      0.81        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.95      0.95        20\n",
      "     class 1       0.88      0.93      0.90        15\n",
      "     class 2       0.94      0.84      0.89        19\n",
      "     class 3       0.55      0.60      0.57        10\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.83      0.83      0.83        64\n",
      "weighted avg       0.87      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.86      0.90        22\n",
      "     class 1       0.89      0.89      0.89         9\n",
      "     class 2       0.83      0.75      0.79        20\n",
      "     class 3       0.65      0.85      0.73        13\n",
      "\n",
      "    accuracy                           0.83        64\n",
      "   macro avg       0.83      0.84      0.83        64\n",
      "weighted avg       0.84      0.83      0.83        64\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.85      0.89      0.87        19\n",
      "     class 1       0.88      1.00      0.93        14\n",
      "     class 2       0.94      0.71      0.81        24\n",
      "     class 3       0.60      0.86      0.71         7\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.82      0.87      0.83        64\n",
      "weighted avg       0.86      0.84      0.84        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.91      0.83      0.87        24\n",
      "     class 1       0.88      0.78      0.82         9\n",
      "     class 2       0.87      0.93      0.90        14\n",
      "     class 3       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.86      0.86      0.86        64\n",
      "weighted avg       0.86      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      0.94      0.94        18\n",
      "     class 1       0.95      0.86      0.90        21\n",
      "     class 2       0.64      0.70      0.67        10\n",
      "     class 3       0.75      0.80      0.77        15\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.82      0.83      0.82        64\n",
      "weighted avg       0.85      0.84      0.85        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.79      0.92      0.85        12\n",
      "     class 1       0.75      0.90      0.82        10\n",
      "     class 2       0.94      0.68      0.79        22\n",
      "     class 3       0.77      0.85      0.81        20\n",
      "\n",
      "    accuracy                           0.81        64\n",
      "   macro avg       0.81      0.84      0.82        64\n",
      "weighted avg       0.83      0.81      0.81        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      0.83      0.88        18\n",
      "     class 1       0.83      0.91      0.87        11\n",
      "     class 2       0.85      0.85      0.85        20\n",
      "     class 3       0.81      0.87      0.84        15\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.86      0.86      0.86        64\n",
      "weighted avg       0.86      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      0.94      0.94        16\n",
      "     class 1       0.64      0.88      0.74         8\n",
      "     class 2       0.82      0.67      0.74        21\n",
      "     class 3       0.75      0.79      0.77        19\n",
      "\n",
      "    accuracy                           0.80        64\n",
      "   macro avg       0.79      0.82      0.80        64\n",
      "weighted avg       0.81      0.80      0.80        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.88      0.91        24\n",
      "     class 1       0.56      0.83      0.67         6\n",
      "     class 2       1.00      0.68      0.81        19\n",
      "     class 3       0.70      0.93      0.80        15\n",
      "\n",
      "    accuracy                           0.83        64\n",
      "   macro avg       0.80      0.83      0.80        64\n",
      "weighted avg       0.87      0.83      0.83        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.80      0.94      0.86        17\n",
      "     class 1       0.80      0.86      0.83        14\n",
      "     class 2       0.87      0.62      0.72        21\n",
      "     class 3       0.64      0.75      0.69        12\n",
      "\n",
      "    accuracy                           0.78        64\n",
      "   macro avg       0.78      0.79      0.78        64\n",
      "weighted avg       0.79      0.78      0.78        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.82      0.93      0.87        15\n",
      "     class 1       0.90      0.56      0.69        16\n",
      "     class 2       0.76      0.68      0.72        19\n",
      "     class 3       0.60      0.86      0.71        14\n",
      "\n",
      "    accuracy                           0.75        64\n",
      "   macro avg       0.77      0.76      0.75        64\n",
      "weighted avg       0.78      0.75      0.75        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.94      0.97        17\n",
      "     class 1       0.82      0.82      0.82        11\n",
      "     class 2       0.94      0.80      0.86        20\n",
      "     class 3       0.80      1.00      0.89        16\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.89      0.89      0.89        64\n",
      "weighted avg       0.90      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.89      1.00      0.94        16\n",
      "     class 1       0.89      0.89      0.89        18\n",
      "     class 2       0.82      0.69      0.75        13\n",
      "     class 3       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.87      0.87      0.87        64\n",
      "weighted avg       0.87      0.88      0.87        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        12\n",
      "     class 1       0.72      0.87      0.79        15\n",
      "     class 2       0.90      0.86      0.88        21\n",
      "     class 3       0.86      0.75      0.80        16\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.87      0.87      0.87        64\n",
      "weighted avg       0.87      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.89      0.89      0.89         9\n",
      "     class 1       0.94      0.89      0.91        18\n",
      "     class 2       0.85      0.85      0.85        20\n",
      "     class 3       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.89      0.89      0.89        64\n",
      "weighted avg       0.89      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      1.00      0.97        18\n",
      "     class 1       0.93      0.82      0.87        17\n",
      "     class 2       0.80      0.92      0.86        13\n",
      "     class 3       0.87      0.81      0.84        16\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.89      0.89      0.89        64\n",
      "weighted avg       0.89      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      0.89      0.92        19\n",
      "     class 1       0.93      0.82      0.87        17\n",
      "     class 2       0.79      0.88      0.83        17\n",
      "     class 3       0.83      0.91      0.87        11\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.88      0.88      0.87        64\n",
      "weighted avg       0.88      0.88      0.88        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.94      0.97        17\n",
      "     class 1       0.90      0.90      0.90        21\n",
      "     class 2       0.88      0.78      0.82         9\n",
      "     class 3       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.91        64\n",
      "   macro avg       0.91      0.89      0.90        64\n",
      "weighted avg       0.91      0.91      0.91        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.90      0.90        20\n",
      "     class 1       0.67      0.92      0.77        13\n",
      "     class 2       0.87      0.65      0.74        20\n",
      "     class 3       0.82      0.82      0.82        11\n",
      "\n",
      "    accuracy                           0.81        64\n",
      "   macro avg       0.81      0.82      0.81        64\n",
      "weighted avg       0.83      0.81      0.81        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      0.94      0.94        17\n",
      "     class 1       0.89      0.89      0.89        19\n",
      "     class 2       0.79      0.79      0.79        14\n",
      "     class 3       0.71      0.71      0.71        14\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.83      0.83      0.83        64\n",
      "weighted avg       0.84      0.84      0.84        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.96      0.98        24\n",
      "     class 1       0.79      0.92      0.85        12\n",
      "     class 2       0.79      0.92      0.85        12\n",
      "     class 3       1.00      0.81      0.90        16\n",
      "\n",
      "    accuracy                           0.91        64\n",
      "   macro avg       0.89      0.90      0.89        64\n",
      "weighted avg       0.92      0.91      0.91        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.88      0.94        17\n",
      "     class 1       0.81      0.93      0.87        14\n",
      "     class 2       0.86      0.92      0.89        13\n",
      "     class 3       0.89      0.85      0.87        20\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.89      0.90      0.89        64\n",
      "weighted avg       0.90      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      1.00      0.97        17\n",
      "     class 1       0.71      0.91      0.80        11\n",
      "     class 2       0.93      0.64      0.76        22\n",
      "     class 3       0.71      0.86      0.77        14\n",
      "\n",
      "    accuracy                           0.83        64\n",
      "   macro avg       0.82      0.85      0.83        64\n",
      "weighted avg       0.85      0.83      0.83        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      1.00      0.97        19\n",
      "     class 1       1.00      0.92      0.96        13\n",
      "     class 2       0.92      0.75      0.83        16\n",
      "     class 3       0.74      0.88      0.80        16\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.90      0.89      0.89        64\n",
      "weighted avg       0.90      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.94      0.97        18\n",
      "     class 1       0.86      0.80      0.83        15\n",
      "     class 2       0.83      0.67      0.74        15\n",
      "     class 3       0.62      0.81      0.70        16\n",
      "\n",
      "    accuracy                           0.81        64\n",
      "   macro avg       0.83      0.81      0.81        64\n",
      "weighted avg       0.83      0.81      0.82        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.82      1.00      0.90         9\n",
      "     class 1       0.88      0.79      0.83        19\n",
      "     class 2       0.67      0.80      0.73        15\n",
      "     class 3       0.83      0.71      0.77        21\n",
      "\n",
      "    accuracy                           0.80        64\n",
      "   macro avg       0.80      0.83      0.81        64\n",
      "weighted avg       0.81      0.80      0.80        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      1.00      0.98        20\n",
      "     class 1       0.71      0.83      0.77        12\n",
      "     class 2       0.88      0.88      0.88        16\n",
      "     class 3       0.92      0.75      0.83        16\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.87      0.86      0.86        64\n",
      "weighted avg       0.88      0.88      0.87        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.96      1.00      0.98        22\n",
      "     class 1       1.00      0.90      0.95        10\n",
      "     class 2       1.00      0.73      0.85        15\n",
      "     class 3       0.81      1.00      0.89        17\n",
      "\n",
      "    accuracy                           0.92        64\n",
      "   macro avg       0.94      0.91      0.92        64\n",
      "weighted avg       0.93      0.92      0.92        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.89      0.84      0.86        19\n",
      "     class 1       0.93      1.00      0.97        14\n",
      "     class 2       0.93      0.81      0.87        16\n",
      "     class 3       0.76      0.87      0.81        15\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.88      0.88      0.88        64\n",
      "weighted avg       0.88      0.88      0.88        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.95      0.95        21\n",
      "     class 1       0.92      0.85      0.88        13\n",
      "     class 2       1.00      0.94      0.97        16\n",
      "     class 3       0.81      0.93      0.87        14\n",
      "\n",
      "    accuracy                           0.92        64\n",
      "   macro avg       0.92      0.92      0.92        64\n",
      "weighted avg       0.93      0.92      0.92        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.81      1.00      0.89        17\n",
      "     class 1       0.94      0.88      0.91        17\n",
      "     class 2       0.93      0.72      0.81        18\n",
      "     class 3       0.77      0.83      0.80        12\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.86      0.86      0.85        64\n",
      "weighted avg       0.87      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      1.00      0.97        19\n",
      "     class 1       0.73      0.73      0.73        11\n",
      "     class 2       0.86      0.82      0.84        22\n",
      "     class 3       0.75      0.75      0.75        12\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.82      0.82      0.82        64\n",
      "weighted avg       0.84      0.84      0.84        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      0.94      0.94        18\n",
      "     class 1       0.93      0.93      0.93        14\n",
      "     class 2       0.88      0.78      0.82        18\n",
      "     class 3       0.81      0.93      0.87        14\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.89      0.89      0.89        64\n",
      "weighted avg       0.89      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.92      0.96        13\n",
      "     class 1       0.71      0.83      0.77        12\n",
      "     class 2       0.87      0.72      0.79        18\n",
      "     class 3       0.91      1.00      0.95        21\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.87      0.87      0.87        64\n",
      "weighted avg       0.88      0.88      0.87        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.89      0.94        18\n",
      "     class 1       0.92      0.80      0.86        15\n",
      "     class 2       0.67      0.57      0.62        14\n",
      "     class 3       0.74      1.00      0.85        17\n",
      "\n",
      "    accuracy                           0.83        64\n",
      "   macro avg       0.83      0.82      0.82        64\n",
      "weighted avg       0.84      0.83      0.83        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.90      0.92        20\n",
      "     class 1       0.80      0.94      0.86        17\n",
      "     class 2       1.00      0.77      0.87        13\n",
      "     class 3       0.87      0.93      0.90        14\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.90      0.88      0.89        64\n",
      "weighted avg       0.90      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.78      0.88         9\n",
      "     class 1       0.91      0.95      0.93        22\n",
      "     class 2       0.89      0.67      0.76        12\n",
      "     class 3       0.80      0.95      0.87        21\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.90      0.84      0.86        64\n",
      "weighted avg       0.88      0.88      0.87        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.82      0.86        11\n",
      "     class 1       0.85      0.79      0.81        14\n",
      "     class 2       0.75      0.88      0.81        17\n",
      "     class 3       0.90      0.86      0.88        22\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.85      0.84      0.84        64\n",
      "weighted avg       0.85      0.84      0.84        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.88      0.93        16\n",
      "     class 1       0.77      0.83      0.80        12\n",
      "     class 2       0.81      0.68      0.74        19\n",
      "     class 3       0.76      0.94      0.84        17\n",
      "\n",
      "    accuracy                           0.83        64\n",
      "   macro avg       0.84      0.83      0.83        64\n",
      "weighted avg       0.84      0.83      0.83        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.90      0.95        21\n",
      "     class 1       0.81      1.00      0.90        13\n",
      "     class 2       0.93      0.78      0.85        18\n",
      "     class 3       0.71      0.83      0.77        12\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.87      0.88      0.87        64\n",
      "weighted avg       0.89      0.88      0.88        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.93      0.96        14\n",
      "     class 1       0.92      1.00      0.96        22\n",
      "     class 2       0.90      0.75      0.82        12\n",
      "     class 3       0.88      0.94      0.91        16\n",
      "\n",
      "    accuracy                           0.92        64\n",
      "   macro avg       0.92      0.90      0.91        64\n",
      "weighted avg       0.92      0.92      0.92        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.82      0.93      0.87        15\n",
      "     class 1       0.94      0.89      0.91        18\n",
      "     class 2       0.67      0.62      0.64        13\n",
      "     class 3       0.83      0.83      0.83        18\n",
      "\n",
      "    accuracy                           0.83        64\n",
      "   macro avg       0.82      0.82      0.82        64\n",
      "weighted avg       0.83      0.83      0.83        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        14\n",
      "     class 1       0.78      0.88      0.82        16\n",
      "     class 2       0.88      0.94      0.91        16\n",
      "     class 3       0.87      0.72      0.79        18\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.88      0.88      0.88        64\n",
      "weighted avg       0.88      0.88      0.87        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        15\n",
      "     class 1       0.88      1.00      0.93        21\n",
      "     class 2       1.00      0.89      0.94        18\n",
      "     class 3       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.94        64\n",
      "   macro avg       0.94      0.92      0.93        64\n",
      "weighted avg       0.94      0.94      0.94        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.86      1.00      0.92        12\n",
      "     class 1       0.85      1.00      0.92        17\n",
      "     class 2       1.00      0.71      0.83        21\n",
      "     class 3       0.93      1.00      0.97        14\n",
      "\n",
      "    accuracy                           0.91        64\n",
      "   macro avg       0.91      0.93      0.91        64\n",
      "weighted avg       0.92      0.91      0.90        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        11\n",
      "     class 1       0.96      0.96      0.96        25\n",
      "     class 2       0.83      0.71      0.77         7\n",
      "     class 3       0.91      0.95      0.93        21\n",
      "\n",
      "    accuracy                           0.94        64\n",
      "   macro avg       0.93      0.91      0.91        64\n",
      "weighted avg       0.94      0.94      0.94        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      1.00      0.97        16\n",
      "     class 1       0.91      1.00      0.95        10\n",
      "     class 2       1.00      0.88      0.94        17\n",
      "     class 3       1.00      1.00      1.00        21\n",
      "\n",
      "    accuracy                           0.97        64\n",
      "   macro avg       0.96      0.97      0.96        64\n",
      "weighted avg       0.97      0.97      0.97        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.88      1.00      0.94        15\n",
      "     class 1       1.00      0.94      0.97        18\n",
      "     class 2       0.94      0.89      0.91        18\n",
      "     class 3       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.95        64\n",
      "   macro avg       0.96      0.96      0.96        64\n",
      "weighted avg       0.96      0.95      0.95        64\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        13\n",
      "     class 1       0.94      1.00      0.97        17\n",
      "     class 2       0.89      0.89      0.89        18\n",
      "     class 3       0.87      0.81      0.84        16\n",
      "\n",
      "    accuracy                           0.92        64\n",
      "   macro avg       0.92      0.93      0.92        64\n",
      "weighted avg       0.92      0.92      0.92        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.93      1.00      0.96        13\n",
      "     class 1       0.86      0.92      0.89        13\n",
      "     class 2       0.81      0.85      0.83        20\n",
      "     class 3       0.87      0.72      0.79        18\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.87      0.87      0.87        64\n",
      "weighted avg       0.86      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.88      0.94        17\n",
      "     class 1       0.86      0.92      0.89        13\n",
      "     class 2       0.93      0.76      0.84        17\n",
      "     class 3       0.76      0.94      0.84        17\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.89      0.88      0.88        64\n",
      "weighted avg       0.89      0.88      0.88        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      1.00      0.98        21\n",
      "     class 1       1.00      0.80      0.89        20\n",
      "     class 2       0.82      0.90      0.86        10\n",
      "     class 3       0.80      0.92      0.86        13\n",
      "\n",
      "    accuracy                           0.91        64\n",
      "   macro avg       0.89      0.91      0.89        64\n",
      "weighted avg       0.92      0.91      0.91        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.94      0.97        16\n",
      "     class 1       0.96      0.88      0.92        26\n",
      "     class 2       0.64      0.88      0.74         8\n",
      "     class 3       0.86      0.86      0.86        14\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.86      0.89      0.87        64\n",
      "weighted avg       0.91      0.89      0.90        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        17\n",
      "     class 1       0.94      0.80      0.86        20\n",
      "     class 2       0.92      0.92      0.92        13\n",
      "     class 3       0.82      1.00      0.90        14\n",
      "\n",
      "    accuracy                           0.92        64\n",
      "   macro avg       0.92      0.93      0.92        64\n",
      "weighted avg       0.93      0.92      0.92        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.94      0.97        17\n",
      "     class 1       0.75      0.82      0.78        11\n",
      "     class 2       0.93      0.62      0.74        21\n",
      "     class 3       0.68      1.00      0.81        15\n",
      "\n",
      "    accuracy                           0.83        64\n",
      "   macro avg       0.84      0.84      0.83        64\n",
      "weighted avg       0.86      0.83      0.83        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      1.00      0.98        21\n",
      "     class 1       0.93      0.93      0.93        15\n",
      "     class 2       0.85      0.73      0.79        15\n",
      "     class 3       0.79      0.85      0.81        13\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.88      0.88      0.88        64\n",
      "weighted avg       0.89      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.95      0.95        20\n",
      "     class 1       0.93      1.00      0.96        13\n",
      "     class 2       0.82      0.60      0.69        15\n",
      "     class 3       0.79      0.94      0.86        16\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.87      0.87      0.87        64\n",
      "weighted avg       0.87      0.88      0.87        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.88      0.94        17\n",
      "     class 1       0.74      1.00      0.85        17\n",
      "     class 2       1.00      0.82      0.90        11\n",
      "     class 3       1.00      0.89      0.94        19\n",
      "\n",
      "    accuracy                           0.91        64\n",
      "   macro avg       0.93      0.90      0.91        64\n",
      "weighted avg       0.93      0.91      0.91        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.93      0.97        15\n",
      "     class 1       0.88      1.00      0.93        14\n",
      "     class 2       0.94      0.85      0.89        20\n",
      "     class 3       0.81      0.87      0.84        15\n",
      "\n",
      "    accuracy                           0.91        64\n",
      "   macro avg       0.91      0.91      0.91        64\n",
      "weighted avg       0.91      0.91      0.91        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.91      0.95      0.93        21\n",
      "     class 1       0.93      0.93      0.93        15\n",
      "     class 2       1.00      0.78      0.88        18\n",
      "     class 3       0.69      0.90      0.78        10\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.88      0.89      0.88        64\n",
      "weighted avg       0.91      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      1.00      0.96        11\n",
      "     class 1       0.94      1.00      0.97        16\n",
      "     class 2       0.75      0.75      0.75        16\n",
      "     class 3       0.84      0.76      0.80        21\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.86      0.88      0.87        64\n",
      "weighted avg       0.86      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00         9\n",
      "     class 1       0.85      1.00      0.92        17\n",
      "     class 2       0.89      0.85      0.87        20\n",
      "     class 3       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.90      0.91      0.90        64\n",
      "weighted avg       0.89      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        13\n",
      "     class 1       0.88      0.95      0.91        22\n",
      "     class 2       1.00      0.69      0.82        13\n",
      "     class 3       0.83      0.94      0.88        16\n",
      "\n",
      "    accuracy                           0.91        64\n",
      "   macro avg       0.93      0.90      0.90        64\n",
      "weighted avg       0.92      0.91      0.90        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      1.00      0.97        17\n",
      "     class 1       1.00      1.00      1.00        13\n",
      "     class 2       0.93      0.82      0.87        17\n",
      "     class 3       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.92        64\n",
      "   macro avg       0.93      0.93      0.93        64\n",
      "weighted avg       0.92      0.92      0.92        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.86      0.89        14\n",
      "     class 1       0.81      1.00      0.90        13\n",
      "     class 2       0.94      0.89      0.92        19\n",
      "     class 3       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.89      0.90      0.89        64\n",
      "weighted avg       0.90      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        10\n",
      "     class 1       0.90      1.00      0.95        18\n",
      "     class 2       0.89      0.89      0.89        18\n",
      "     class 3       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.92        64\n",
      "   macro avg       0.93      0.93      0.93        64\n",
      "weighted avg       0.92      0.92      0.92        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.93      0.96        14\n",
      "     class 1       0.96      0.96      0.96        24\n",
      "     class 2       0.89      0.67      0.76        12\n",
      "     class 3       0.78      1.00      0.88        14\n",
      "\n",
      "    accuracy                           0.91        64\n",
      "   macro avg       0.91      0.89      0.89        64\n",
      "weighted avg       0.91      0.91      0.90        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.94      0.97        17\n",
      "     class 1       0.95      0.90      0.93        21\n",
      "     class 2       0.90      0.75      0.82        12\n",
      "     class 3       0.72      0.93      0.81        14\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.89      0.88      0.88        64\n",
      "weighted avg       0.90      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.83      0.91      0.87        11\n",
      "     class 1       0.94      1.00      0.97        15\n",
      "     class 2       0.93      0.78      0.85        18\n",
      "     class 3       0.86      0.90      0.88        20\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.89      0.90      0.89        64\n",
      "weighted avg       0.89      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.92      0.92        13\n",
      "     class 1       0.92      0.86      0.89        14\n",
      "     class 2       0.88      0.78      0.82        18\n",
      "     class 3       0.86      1.00      0.93        19\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.90      0.89      0.89        64\n",
      "weighted avg       0.89      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.95      0.95        19\n",
      "     class 1       1.00      0.94      0.97        17\n",
      "     class 2       0.92      0.92      0.92        13\n",
      "     class 3       0.88      0.93      0.90        15\n",
      "\n",
      "    accuracy                           0.94        64\n",
      "   macro avg       0.94      0.94      0.94        64\n",
      "weighted avg       0.94      0.94      0.94        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.88      0.93        16\n",
      "     class 1       0.85      0.94      0.89        18\n",
      "     class 2       0.92      0.85      0.88        13\n",
      "     class 3       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.90      0.89      0.89        64\n",
      "weighted avg       0.90      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.96      0.92      0.94        24\n",
      "     class 1       0.89      0.94      0.92        18\n",
      "     class 2       0.88      0.70      0.78        10\n",
      "     class 3       0.79      0.92      0.85        12\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.88      0.87      0.87        64\n",
      "weighted avg       0.89      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.87      1.00      0.93        13\n",
      "     class 1       0.96      0.88      0.92        25\n",
      "     class 2       0.82      0.82      0.82        11\n",
      "     class 3       0.80      0.80      0.80        15\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.86      0.87      0.87        64\n",
      "weighted avg       0.88      0.88      0.87        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        18\n",
      "     class 1       0.86      0.95      0.90        19\n",
      "     class 2       1.00      0.88      0.93         8\n",
      "     class 3       0.89      0.84      0.86        19\n",
      "\n",
      "    accuracy                           0.92        64\n",
      "   macro avg       0.94      0.92      0.92        64\n",
      "weighted avg       0.92      0.92      0.92        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.92      0.96        13\n",
      "     class 1       0.89      0.94      0.91        17\n",
      "     class 2       1.00      0.87      0.93        15\n",
      "     class 3       0.81      0.89      0.85        19\n",
      "\n",
      "    accuracy                           0.91        64\n",
      "   macro avg       0.92      0.91      0.91        64\n",
      "weighted avg       0.91      0.91      0.91        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        14\n",
      "     class 1       0.85      0.92      0.88        12\n",
      "     class 2       0.88      0.88      0.88        16\n",
      "     class 3       0.86      0.82      0.84        22\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.89      0.90      0.90        64\n",
      "weighted avg       0.89      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.94      0.97        16\n",
      "     class 1       0.76      1.00      0.86        16\n",
      "     class 2       0.92      0.79      0.85        14\n",
      "     class 3       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.88        64\n",
      "   macro avg       0.89      0.88      0.88        64\n",
      "weighted avg       0.89      0.88      0.87        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        17\n",
      "     class 1       0.91      0.95      0.93        22\n",
      "     class 2       1.00      0.90      0.95        10\n",
      "     class 3       0.87      0.87      0.87        15\n",
      "\n",
      "    accuracy                           0.94        64\n",
      "   macro avg       0.94      0.93      0.94        64\n",
      "weighted avg       0.94      0.94      0.94        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.94      0.97        18\n",
      "     class 1       1.00      0.94      0.97        18\n",
      "     class 2       0.89      0.94      0.91        17\n",
      "     class 3       0.83      0.91      0.87        11\n",
      "\n",
      "    accuracy                           0.94        64\n",
      "   macro avg       0.93      0.93      0.93        64\n",
      "weighted avg       0.94      0.94      0.94        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.90      0.95        20\n",
      "     class 1       0.91      0.77      0.83        13\n",
      "     class 2       0.82      1.00      0.90        14\n",
      "     class 3       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.92        64\n",
      "   macro avg       0.92      0.92      0.91        64\n",
      "weighted avg       0.93      0.92      0.92        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.82      0.90        11\n",
      "     class 1       0.70      0.88      0.78         8\n",
      "     class 2       0.90      0.90      0.90        21\n",
      "     class 3       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.88      0.88      0.87        64\n",
      "weighted avg       0.90      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.94      0.97        17\n",
      "     class 1       0.93      0.87      0.90        15\n",
      "     class 2       0.77      0.89      0.83        19\n",
      "     class 3       0.92      0.85      0.88        13\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.90      0.89      0.89        64\n",
      "weighted avg       0.90      0.89      0.89        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      1.00      0.97        16\n",
      "     class 1       1.00      0.89      0.94         9\n",
      "     class 2       1.00      0.89      0.94        18\n",
      "     class 3       0.91      1.00      0.95        21\n",
      "\n",
      "    accuracy                           0.95        64\n",
      "   macro avg       0.96      0.94      0.95        64\n",
      "weighted avg       0.96      0.95      0.95        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      0.89      0.91        18\n",
      "     class 1       1.00      0.88      0.93        16\n",
      "     class 2       0.68      0.87      0.76        15\n",
      "     class 3       0.86      0.80      0.83        15\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.87      0.86      0.86        64\n",
      "weighted avg       0.88      0.86      0.86        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      0.79      0.86        19\n",
      "     class 1       0.83      0.87      0.85        23\n",
      "     class 2       0.79      0.85      0.81        13\n",
      "     class 3       0.80      0.89      0.84         9\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.84      0.85      0.84        64\n",
      "weighted avg       0.85      0.84      0.84        64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.83      0.91         6\n",
      "     class 1       0.86      0.75      0.80         8\n",
      "     class 2       0.94      0.91      0.92        33\n",
      "     class 3       0.25      1.00      0.40         1\n",
      "\n",
      "    accuracy                           0.88        48\n",
      "   macro avg       0.76      0.87      0.76        48\n",
      "weighted avg       0.92      0.88      0.89        48\n",
      "\n",
      "Test Loss: 0.391 | Test Acc: 86.19%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Learning model with AllenNLP\n",
    "\n",
    "- AllenNLP is a research library built on PyTorch\n",
    "- makes it easy to use state-of-the-art models\n",
    "- easy to generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import LabelField, TextField\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import WordTokenizer\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules import Embedding, Seq2VecEncoder, TextFieldEmbedder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from allennlp.data.iterators import BucketIterator, BasicIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "from allennlp.data import DatasetReader, Instance, Vocabulary\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.predictors import Predictor\n",
    "from overrides import overrides\n",
    "from typing import Dict, Union, Iterable, Iterator, List, Optional, Tuple, Deque\n",
    "from allennlp.common import JsonDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DatasetReader.register('classification_simple2')\n",
    "class ClassificationReader(DatasetReader):\n",
    "    def __init__(self):\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = WordTokenizer()\n",
    "        self.token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        with open(file_path, 'r') as lines:\n",
    "            for line in lines:\n",
    "                text, label = line.strip().split('\\t')\n",
    "                text_field = TextField(self.tokenizer.tokenize(text),\n",
    "                                       self.token_indexers)\n",
    "                label_field = LabelField(label)\n",
    "                fields = {'text': text_field, 'label': label_field}\n",
    "                yield Instance(fields)\n",
    "                \n",
    "    def text_to_instance(self, text_list):\n",
    "        text_field = TextField(self.tokenizer.tokenize(\" \".join(text_list)), self.token_indexers)\n",
    "        fields = {'text': text_field}\n",
    "        return Instance(fields)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Model.register(\"lstm_classifier_simple\")\n",
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        # Embeddings convert words to their vector representations\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # The fully-connected linear layer extracts the features from the vector\n",
    "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                      out_features=vocab.get_vocab_size('labels'))\n",
    "\n",
    "        # Define the metrics\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "        # We use the cross entropy loss because this is a classification task.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    def forward(self,\n",
    "                text: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(text)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(text)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.linear(encoder_out)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {'accuracy': self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "119it [00:00, 1186.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "303it [00:00, 1327.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "525it [00:00, 1509.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "769it [00:00, 1703.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "1031it [00:00, 1902.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "1301it [00:00, 2087.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "1579it [00:00, 2254.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "1875it [00:00, 2427.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "2156it [00:00, 2529.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "2446it [00:01, 2626.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "2736it [00:01, 2700.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "3023it [00:01, 2748.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "3313it [00:01, 2791.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "3608it [00:01, 2835.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "3898it [00:01, 2852.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "4187it [00:01, 2862.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "4480it [00:01, 2880.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "4769it [00:01, 2873.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "5057it [00:01, 2866.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "5347it [00:02, 2874.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "5644it [00:02, 2899.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "5942it [00:02, 2922.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "6235it [00:02, 2913.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "6527it [00:02, 2890.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "6817it [00:02, 2861.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "7107it [00:02, 2872.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "7403it [00:02, 2895.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "7693it [00:02, 2884.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "7985it [00:02, 2893.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "8278it [00:03, 2902.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "8569it [00:03, 2890.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "8859it [00:03, 2883.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "9152it [00:03, 2896.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "9443it [00:03, 2900.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "9734it [00:03, 2887.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "10023it [00:03, 2873.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "10311it [00:03, 2864.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "10602it [00:03, 2875.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "10893it [00:03, 2885.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "11183it [00:04, 2887.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "11475it [00:04, 2894.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "11765it [00:04, 2895.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "12055it [00:04, 2895.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "12345it [00:04, 2881.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "12634it [00:06, 419.39it/s] \u001b[A\u001b[A\n",
      "\n",
      "12922it [00:06, 563.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "13211it [00:06, 743.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "13502it [00:06, 956.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "13793it [00:06, 1197.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "14070it [00:06, 1443.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "14360it [00:07, 1699.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "14647it [00:07, 1935.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "14941it [00:07, 2156.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "15233it [00:07, 2338.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "15520it [00:07, 2470.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "15810it [00:07, 2583.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "16100it [00:07, 2670.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "16388it [00:07, 2723.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "16676it [00:07, 2763.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "16800it [00:07, 2115.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "268it [00:00, 2674.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "552it [00:00, 2721.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "834it [00:00, 2747.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "1115it [00:00, 2764.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "1401it [00:00, 2790.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "1686it [00:00, 2806.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "1980it [00:00, 2844.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "2270it [00:00, 2859.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "2558it [00:00, 2864.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "2839it [00:01, 2846.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "3124it [00:01, 2846.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "3406it [00:01, 2837.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "3689it [00:01, 2833.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "3973it [00:01, 2835.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "4260it [00:01, 2845.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "4553it [00:01, 2869.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "4840it [00:01, 2868.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "5133it [00:01, 2886.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "5422it [00:01, 2883.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "5711it [00:02, 2845.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "5996it [00:02, 2844.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "6286it [00:02, 2859.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "6572it [00:02, 2859.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "6862it [00:02, 2869.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "7149it [00:02, 2842.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "7200it [00:02, 2843.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/24000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 6694/24000 [00:00<00:00, 66932.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 13206/24000 [00:00<00:00, 66377.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 19732/24000 [00:00<00:00, 66036.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 24000/24000 [00:00<00:00, 65364.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/840 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3000, loss: 1.3843 ||:   0%|          | 1/840 [00:00<06:23,  2.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2167, loss: 1.3859 ||:   0%|          | 3/840 [00:00<04:45,  2.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2700, loss: 1.3837 ||:   1%|          | 5/840 [00:00<03:35,  3.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2500, loss: 1.3879 ||:   1%|          | 7/840 [00:00<02:44,  5.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2611, loss: 1.3857 ||:   1%|          | 9/840 [00:00<02:09,  6.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2545, loss: 1.3866 ||:   1%|▏         | 11/840 [00:01<01:46,  7.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2538, loss: 1.3856 ||:   2%|▏         | 13/840 [00:01<01:29,  9.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2500, loss: 1.3872 ||:   2%|▏         | 15/840 [00:01<01:18, 10.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2382, loss: 1.3875 ||:   2%|▏         | 17/840 [00:01<01:08, 12.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2447, loss: 1.3873 ||:   2%|▏         | 19/840 [00:01<01:03, 12.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2452, loss: 1.3869 ||:   2%|▎         | 21/840 [00:01<01:00, 13.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2500, loss: 1.3870 ||:   3%|▎         | 23/840 [00:01<00:55, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2500, loss: 1.3870 ||:   3%|▎         | 25/840 [00:01<00:53, 15.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2519, loss: 1.3867 ||:   3%|▎         | 27/840 [00:02<00:51, 15.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2466, loss: 1.3872 ||:   3%|▎         | 29/840 [00:02<00:52, 15.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2500, loss: 1.3870 ||:   4%|▎         | 31/840 [00:02<00:50, 16.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2545, loss: 1.3870 ||:   4%|▍         | 33/840 [00:02<00:48, 16.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2529, loss: 1.3870 ||:   4%|▍         | 35/840 [00:02<00:50, 16.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2527, loss: 1.3866 ||:   4%|▍         | 37/840 [00:02<00:48, 16.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2526, loss: 1.3867 ||:   5%|▍         | 39/840 [00:02<00:49, 16.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2500, loss: 1.3867 ||:   5%|▍         | 41/840 [00:02<00:47, 16.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2477, loss: 1.3870 ||:   5%|▌         | 43/840 [00:03<00:47, 16.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2511, loss: 1.3867 ||:   5%|▌         | 45/840 [00:03<00:46, 17.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2543, loss: 1.3866 ||:   6%|▌         | 47/840 [00:03<00:45, 17.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2551, loss: 1.3866 ||:   6%|▌         | 49/840 [00:03<00:45, 17.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2559, loss: 1.3864 ||:   6%|▌         | 51/840 [00:03<00:44, 17.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2500, loss: 1.3867 ||:   6%|▋         | 53/840 [00:03<00:43, 17.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2527, loss: 1.3867 ||:   7%|▋         | 55/840 [00:03<00:45, 17.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2579, loss: 1.3864 ||:   7%|▋         | 57/840 [00:03<00:44, 17.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2542, loss: 1.3868 ||:   7%|▋         | 59/840 [00:03<00:43, 17.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2541, loss: 1.3868 ||:   7%|▋         | 61/840 [00:04<00:45, 17.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2563, loss: 1.3866 ||:   8%|▊         | 63/840 [00:04<00:43, 17.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2562, loss: 1.3867 ||:   8%|▊         | 65/840 [00:04<00:43, 17.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2575, loss: 1.3864 ||:   8%|▊         | 67/840 [00:04<00:45, 17.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2543, loss: 1.3865 ||:   8%|▊         | 69/840 [00:04<00:44, 17.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2542, loss: 1.3864 ||:   8%|▊         | 71/840 [00:04<00:43, 17.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2521, loss: 1.3865 ||:   9%|▊         | 73/840 [00:04<00:42, 18.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2540, loss: 1.3863 ||:   9%|▉         | 75/840 [00:04<00:41, 18.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2532, loss: 1.3863 ||:   9%|▉         | 77/840 [00:04<00:42, 17.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2519, loss: 1.3863 ||:   9%|▉         | 79/840 [00:05<00:43, 17.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2525, loss: 1.3862 ||:  10%|▉         | 81/840 [00:05<00:45, 16.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2500, loss: 1.3864 ||:  10%|▉         | 83/840 [00:05<00:45, 16.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2494, loss: 1.3863 ||:  10%|█         | 85/840 [00:05<00:44, 16.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2500, loss: 1.3864 ||:  10%|█         | 87/840 [00:05<00:46, 16.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2500, loss: 1.3864 ||:  11%|█         | 89/840 [00:05<00:44, 16.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2478, loss: 1.3866 ||:  11%|█         | 91/840 [00:05<00:44, 17.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2505, loss: 1.3864 ||:  11%|█         | 93/840 [00:05<00:43, 17.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2526, loss: 1.3863 ||:  11%|█▏        | 95/840 [00:05<00:42, 17.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2536, loss: 1.3861 ||:  12%|█▏        | 97/840 [00:06<00:43, 16.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2535, loss: 1.3862 ||:  12%|█▏        | 99/840 [00:06<00:42, 17.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2559, loss: 1.3861 ||:  12%|█▏        | 101/840 [00:06<00:46, 15.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2568, loss: 1.3862 ||:  12%|█▏        | 103/840 [00:06<00:44, 16.45it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.2595, loss: 1.3860 ||:  12%|█▎        | 105/840 [00:06<00:45, 16.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2631, loss: 1.3860 ||:  13%|█▎        | 107/840 [00:06<00:44, 16.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2651, loss: 1.3860 ||:  13%|█▎        | 109/840 [00:06<00:44, 16.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2689, loss: 1.3859 ||:  13%|█▎        | 111/840 [00:06<00:45, 16.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2699, loss: 1.3858 ||:  13%|█▎        | 113/840 [00:07<00:43, 16.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2722, loss: 1.3858 ||:  14%|█▎        | 115/840 [00:07<00:41, 17.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2726, loss: 1.3859 ||:  14%|█▍        | 117/840 [00:07<00:40, 17.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2731, loss: 1.3860 ||:  14%|█▍        | 119/840 [00:07<00:41, 17.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2736, loss: 1.3860 ||:  14%|█▍        | 121/840 [00:07<00:41, 17.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2748, loss: 1.3860 ||:  15%|█▍        | 123/840 [00:07<00:41, 17.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2744, loss: 1.3861 ||:  15%|█▍        | 125/840 [00:07<00:40, 17.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2732, loss: 1.3861 ||:  15%|█▌        | 127/840 [00:07<00:39, 17.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2738, loss: 1.3860 ||:  15%|█▌        | 130/840 [00:08<00:38, 18.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2746, loss: 1.3859 ||:  16%|█▌        | 132/840 [00:08<00:38, 18.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2739, loss: 1.3860 ||:  16%|█▌        | 134/840 [00:08<00:38, 18.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2735, loss: 1.3860 ||:  16%|█▌        | 136/840 [00:08<00:40, 17.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2750, loss: 1.3859 ||:  16%|█▋        | 138/840 [00:08<00:41, 17.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2746, loss: 1.3859 ||:  17%|█▋        | 140/840 [00:08<00:39, 17.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2754, loss: 1.3858 ||:  17%|█▋        | 142/840 [00:08<00:39, 17.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2753, loss: 1.3858 ||:  17%|█▋        | 144/840 [00:08<00:40, 17.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2757, loss: 1.3858 ||:  17%|█▋        | 146/840 [00:08<00:39, 17.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2760, loss: 1.3857 ||:  18%|█▊        | 148/840 [00:09<00:38, 17.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2757, loss: 1.3857 ||:  18%|█▊        | 150/840 [00:09<00:38, 18.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2732, loss: 1.3858 ||:  18%|█▊        | 153/840 [00:09<00:36, 18.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2745, loss: 1.3857 ||:  18%|█▊        | 155/840 [00:09<00:36, 18.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2739, loss: 1.3857 ||:  19%|█▊        | 157/840 [00:09<00:36, 18.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2730, loss: 1.3857 ||:  19%|█▉        | 159/840 [00:09<00:37, 18.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2739, loss: 1.3856 ||:  19%|█▉        | 161/840 [00:09<00:37, 17.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2739, loss: 1.3856 ||:  19%|█▉        | 163/840 [00:09<00:38, 17.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2733, loss: 1.3856 ||:  20%|█▉        | 165/840 [00:09<00:38, 17.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2716, loss: 1.3856 ||:  20%|█▉        | 167/840 [00:10<00:37, 17.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2719, loss: 1.3855 ||:  20%|██        | 169/840 [00:10<00:38, 17.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2716, loss: 1.3856 ||:  20%|██        | 171/840 [00:10<00:39, 16.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2717, loss: 1.3856 ||:  21%|██        | 173/840 [00:10<00:38, 17.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2711, loss: 1.3856 ||:  21%|██        | 175/840 [00:10<00:38, 17.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2709, loss: 1.3856 ||:  21%|██        | 177/840 [00:10<00:36, 17.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2696, loss: 1.3856 ||:  21%|██▏       | 179/840 [00:10<00:35, 18.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2682, loss: 1.3856 ||:  22%|██▏       | 181/840 [00:10<00:35, 18.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2678, loss: 1.3856 ||:  22%|██▏       | 183/840 [00:10<00:34, 18.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2681, loss: 1.3856 ||:  22%|██▏       | 185/840 [00:11<00:34, 18.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2701, loss: 1.3855 ||:  22%|██▏       | 187/840 [00:11<00:34, 19.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2706, loss: 1.3854 ||:  22%|██▎       | 189/840 [00:11<00:35, 18.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2702, loss: 1.3854 ||:  23%|██▎       | 191/840 [00:11<00:35, 18.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2689, loss: 1.3854 ||:  23%|██▎       | 193/840 [00:11<00:35, 18.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2674, loss: 1.3854 ||:  23%|██▎       | 195/840 [00:11<00:36, 17.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2678, loss: 1.3854 ||:  23%|██▎       | 197/840 [00:11<00:36, 17.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2678, loss: 1.3854 ||:  24%|██▎       | 199/840 [00:11<00:35, 18.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2669, loss: 1.3854 ||:  24%|██▍       | 201/840 [00:12<00:37, 16.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2663, loss: 1.3854 ||:  24%|██▍       | 203/840 [00:12<00:36, 17.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2663, loss: 1.3854 ||:  24%|██▍       | 205/840 [00:12<00:36, 17.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2676, loss: 1.3853 ||:  25%|██▍       | 207/840 [00:12<00:34, 18.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2675, loss: 1.3853 ||:  25%|██▍       | 209/840 [00:12<00:35, 17.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2685, loss: 1.3852 ||:  25%|██▌       | 211/840 [00:12<00:36, 17.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2685, loss: 1.3852 ||:  25%|██▌       | 213/840 [00:12<00:37, 16.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2686, loss: 1.3851 ||:  26%|██▌       | 215/840 [00:12<00:35, 17.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2682, loss: 1.3851 ||:  26%|██▌       | 217/840 [00:12<00:34, 18.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2678, loss: 1.3851 ||:  26%|██▌       | 219/840 [00:13<00:35, 17.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2681, loss: 1.3851 ||:  26%|██▋       | 221/840 [00:13<00:35, 17.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2679, loss: 1.3851 ||:  27%|██▋       | 223/840 [00:13<00:34, 17.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2676, loss: 1.3850 ||:  27%|██▋       | 225/840 [00:13<00:33, 18.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2672, loss: 1.3850 ||:  27%|██▋       | 227/840 [00:13<00:34, 17.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2670, loss: 1.3850 ||:  27%|██▋       | 229/840 [00:13<00:34, 17.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2667, loss: 1.3850 ||:  28%|██▊       | 231/840 [00:13<00:35, 17.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2670, loss: 1.3849 ||:  28%|██▊       | 233/840 [00:13<00:34, 17.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2672, loss: 1.3849 ||:  28%|██▊       | 235/840 [00:13<00:34, 17.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2681, loss: 1.3848 ||:  28%|██▊       | 237/840 [00:14<00:34, 17.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2676, loss: 1.3848 ||:  28%|██▊       | 239/840 [00:14<00:33, 17.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2680, loss: 1.3847 ||:  29%|██▉       | 242/840 [00:14<00:32, 18.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2684, loss: 1.3846 ||:  29%|██▉       | 244/840 [00:14<00:32, 18.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2689, loss: 1.3846 ||:  29%|██▉       | 246/840 [00:14<00:32, 18.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2683, loss: 1.3846 ||:  30%|██▉       | 248/840 [00:14<00:31, 18.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2674, loss: 1.3846 ||:  30%|██▉       | 250/840 [00:14<00:32, 18.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2673, loss: 1.3845 ||:  30%|███       | 252/840 [00:14<00:31, 18.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2665, loss: 1.3845 ||:  30%|███       | 254/840 [00:14<00:31, 18.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2664, loss: 1.3845 ||:  30%|███       | 256/840 [00:15<00:32, 17.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2665, loss: 1.3844 ||:  31%|███       | 258/840 [00:15<00:32, 18.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2669, loss: 1.3844 ||:  31%|███       | 260/840 [00:15<00:31, 18.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2670, loss: 1.3843 ||:  31%|███       | 262/840 [00:15<00:32, 17.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2670, loss: 1.3842 ||:  31%|███▏      | 264/840 [00:15<00:31, 18.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2675, loss: 1.3842 ||:  32%|███▏      | 266/840 [00:15<00:31, 18.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2666, loss: 1.3842 ||:  32%|███▏      | 268/840 [00:15<00:32, 17.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2657, loss: 1.3842 ||:  32%|███▏      | 270/840 [00:15<00:31, 18.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2667, loss: 1.3841 ||:  32%|███▏      | 272/840 [00:15<00:31, 18.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2672, loss: 1.3841 ||:  33%|███▎      | 274/840 [00:16<00:30, 18.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2678, loss: 1.3840 ||:  33%|███▎      | 276/840 [00:16<00:30, 18.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2687, loss: 1.3839 ||:  33%|███▎      | 278/840 [00:16<00:31, 17.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2698, loss: 1.3838 ||:  33%|███▎      | 280/840 [00:16<00:30, 18.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2706, loss: 1.3837 ||:  34%|███▎      | 282/840 [00:16<00:30, 18.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2711, loss: 1.3836 ||:  34%|███▍      | 284/840 [00:16<00:30, 18.33it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.2715, loss: 1.3836 ||:  34%|███▍      | 286/840 [00:16<00:30, 18.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2720, loss: 1.3835 ||:  34%|███▍      | 288/840 [00:16<00:30, 17.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2724, loss: 1.3835 ||:  35%|███▍      | 290/840 [00:16<00:30, 18.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2729, loss: 1.3834 ||:  35%|███▍      | 292/840 [00:17<00:30, 17.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2736, loss: 1.3833 ||:  35%|███▌      | 294/840 [00:17<00:31, 17.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2752, loss: 1.3832 ||:  35%|███▌      | 296/840 [00:17<00:30, 17.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2753, loss: 1.3832 ||:  35%|███▌      | 298/840 [00:17<00:30, 17.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2760, loss: 1.3831 ||:  36%|███▌      | 300/840 [00:17<00:31, 16.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2762, loss: 1.3831 ||:  36%|███▌      | 302/840 [00:17<00:31, 17.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2770, loss: 1.3830 ||:  36%|███▌      | 304/840 [00:17<00:30, 17.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2775, loss: 1.3830 ||:  36%|███▋      | 306/840 [00:17<00:29, 18.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2773, loss: 1.3830 ||:  37%|███▋      | 308/840 [00:17<00:28, 18.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2782, loss: 1.3829 ||:  37%|███▋      | 310/840 [00:18<00:28, 18.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2780, loss: 1.3829 ||:  37%|███▋      | 312/840 [00:18<00:29, 17.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2783, loss: 1.3828 ||:  37%|███▋      | 314/840 [00:18<00:30, 17.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2793, loss: 1.3827 ||:  38%|███▊      | 316/840 [00:18<00:30, 17.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2799, loss: 1.3826 ||:  38%|███▊      | 318/840 [00:18<00:29, 17.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2800, loss: 1.3826 ||:  38%|███▊      | 320/840 [00:18<00:29, 17.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2801, loss: 1.3825 ||:  38%|███▊      | 322/840 [00:18<00:29, 17.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2801, loss: 1.3824 ||:  39%|███▊      | 324/840 [00:18<00:28, 17.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2813, loss: 1.3823 ||:  39%|███▉      | 326/840 [00:18<00:28, 17.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2831, loss: 1.3822 ||:  39%|███▉      | 328/840 [00:19<00:27, 18.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2839, loss: 1.3821 ||:  39%|███▉      | 330/840 [00:19<00:27, 18.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2852, loss: 1.3820 ||:  40%|███▉      | 332/840 [00:19<00:28, 18.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2865, loss: 1.3818 ||:  40%|███▉      | 334/840 [00:19<00:28, 17.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2878, loss: 1.3817 ||:  40%|████      | 336/840 [00:19<00:28, 17.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2898, loss: 1.3815 ||:  40%|████      | 338/840 [00:19<00:28, 17.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2913, loss: 1.3814 ||:  40%|████      | 340/840 [00:19<00:30, 16.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2923, loss: 1.3813 ||:  41%|████      | 342/840 [00:19<00:30, 16.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2939, loss: 1.3810 ||:  41%|████      | 344/840 [00:20<00:28, 17.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2952, loss: 1.3809 ||:  41%|████      | 346/840 [00:20<00:27, 17.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2970, loss: 1.3806 ||:  41%|████▏     | 348/840 [00:20<00:26, 18.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2973, loss: 1.3805 ||:  42%|████▏     | 350/840 [00:20<00:26, 18.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2976, loss: 1.3802 ||:  42%|████▏     | 352/840 [00:20<00:26, 18.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2983, loss: 1.3799 ||:  42%|████▏     | 354/840 [00:20<00:27, 17.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2992, loss: 1.3794 ||:  42%|████▏     | 356/840 [00:20<00:27, 17.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2993, loss: 1.3788 ||:  43%|████▎     | 358/840 [00:20<00:27, 17.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2989, loss: 1.3783 ||:  43%|████▎     | 360/840 [00:20<00:26, 18.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2992, loss: 1.3773 ||:  43%|████▎     | 362/840 [00:20<00:25, 18.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2990, loss: 1.3766 ||:  43%|████▎     | 364/840 [00:21<00:25, 18.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.2996, loss: 1.3754 ||:  44%|████▎     | 366/840 [00:21<00:25, 18.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3000, loss: 1.3745 ||:  44%|████▍     | 368/840 [00:21<00:25, 18.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3009, loss: 1.3735 ||:  44%|████▍     | 370/840 [00:21<00:25, 18.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3023, loss: 1.3718 ||:  44%|████▍     | 372/840 [00:21<00:25, 18.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3036, loss: 1.3703 ||:  45%|████▍     | 374/840 [00:21<00:26, 17.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3051, loss: 1.3693 ||:  45%|████▍     | 376/840 [00:21<00:26, 17.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3067, loss: 1.3681 ||:  45%|████▌     | 378/840 [00:21<00:26, 17.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3082, loss: 1.3673 ||:  45%|████▌     | 380/840 [00:22<00:26, 17.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3103, loss: 1.3655 ||:  45%|████▌     | 382/840 [00:22<00:26, 17.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3118, loss: 1.3640 ||:  46%|████▌     | 384/840 [00:22<00:25, 17.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3128, loss: 1.3626 ||:  46%|████▌     | 386/840 [00:22<00:26, 17.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3146, loss: 1.3610 ||:  46%|████▌     | 388/840 [00:22<00:25, 17.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3154, loss: 1.3594 ||:  46%|████▋     | 390/840 [00:22<00:25, 17.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3171, loss: 1.3577 ||:  47%|████▋     | 392/840 [00:22<00:24, 18.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3189, loss: 1.3555 ||:  47%|████▋     | 394/840 [00:22<00:23, 18.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3201, loss: 1.3531 ||:  47%|████▋     | 396/840 [00:22<00:23, 18.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3214, loss: 1.3519 ||:  47%|████▋     | 398/840 [00:22<00:23, 18.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3222, loss: 1.3508 ||:  48%|████▊     | 400/840 [00:23<00:27, 16.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3235, loss: 1.3494 ||:  48%|████▊     | 402/840 [00:23<00:26, 16.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3241, loss: 1.3477 ||:  48%|████▊     | 404/840 [00:23<00:26, 16.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3245, loss: 1.3466 ||:  48%|████▊     | 406/840 [00:23<00:25, 17.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3254, loss: 1.3459 ||:  49%|████▊     | 408/840 [00:23<00:25, 16.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3265, loss: 1.3439 ||:  49%|████▉     | 410/840 [00:23<00:25, 16.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3278, loss: 1.3426 ||:  49%|████▉     | 412/840 [00:23<00:24, 17.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3287, loss: 1.3411 ||:  49%|████▉     | 414/840 [00:23<00:25, 16.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3292, loss: 1.3398 ||:  50%|████▉     | 416/840 [00:24<00:24, 17.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3301, loss: 1.3380 ||:  50%|████▉     | 418/840 [00:24<00:24, 17.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3305, loss: 1.3368 ||:  50%|█████     | 420/840 [00:24<00:23, 17.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3314, loss: 1.3354 ||:  50%|█████     | 422/840 [00:24<00:24, 17.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3317, loss: 1.3341 ||:  50%|█████     | 424/840 [00:24<00:23, 17.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3324, loss: 1.3323 ||:  51%|█████     | 426/840 [00:24<00:23, 17.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3334, loss: 1.3306 ||:  51%|█████     | 428/840 [00:24<00:22, 18.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3349, loss: 1.3278 ||:  51%|█████     | 430/840 [00:24<00:23, 17.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3356, loss: 1.3260 ||:  51%|█████▏    | 432/840 [00:24<00:23, 17.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3371, loss: 1.3240 ||:  52%|█████▏    | 434/840 [00:25<00:23, 17.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3384, loss: 1.3228 ||:  52%|█████▏    | 436/840 [00:25<00:23, 17.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3390, loss: 1.3220 ||:  52%|█████▏    | 438/840 [00:25<00:23, 17.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3395, loss: 1.3207 ||:  52%|█████▏    | 440/840 [00:25<00:23, 16.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3406, loss: 1.3188 ||:  53%|█████▎    | 442/840 [00:25<00:23, 16.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3414, loss: 1.3174 ||:  53%|█████▎    | 444/840 [00:25<00:23, 16.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3424, loss: 1.3158 ||:  53%|█████▎    | 446/840 [00:25<00:22, 17.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3433, loss: 1.3148 ||:  53%|█████▎    | 448/840 [00:25<00:22, 17.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3443, loss: 1.3131 ||:  54%|█████▎    | 450/840 [00:26<00:22, 17.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3446, loss: 1.3118 ||:  54%|█████▍    | 452/840 [00:26<00:21, 17.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3454, loss: 1.3100 ||:  54%|█████▍    | 454/840 [00:26<00:21, 17.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3462, loss: 1.3076 ||:  54%|█████▍    | 456/840 [00:26<00:21, 18.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3471, loss: 1.3066 ||:  55%|█████▍    | 458/840 [00:26<00:20, 18.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3477, loss: 1.3054 ||:  55%|█████▍    | 460/840 [00:26<00:20, 18.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3492, loss: 1.3035 ||:  55%|█████▌    | 462/840 [00:26<00:21, 17.55it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.3505, loss: 1.3015 ||:  55%|█████▌    | 464/840 [00:26<00:21, 17.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3516, loss: 1.2998 ||:  55%|█████▌    | 466/840 [00:26<00:22, 16.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3528, loss: 1.2977 ||:  56%|█████▌    | 468/840 [00:27<00:22, 16.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3537, loss: 1.2960 ||:  56%|█████▌    | 470/840 [00:27<00:22, 16.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3551, loss: 1.2941 ||:  56%|█████▌    | 472/840 [00:27<00:21, 17.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3561, loss: 1.2932 ||:  56%|█████▋    | 474/840 [00:27<00:21, 17.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3571, loss: 1.2914 ||:  57%|█████▋    | 476/840 [00:27<00:21, 17.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3584, loss: 1.2896 ||:  57%|█████▋    | 478/840 [00:27<00:20, 17.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3592, loss: 1.2878 ||:  57%|█████▋    | 480/840 [00:27<00:20, 17.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3601, loss: 1.2864 ||:  57%|█████▋    | 482/840 [00:27<00:20, 17.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3607, loss: 1.2854 ||:  58%|█████▊    | 484/840 [00:27<00:19, 18.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3615, loss: 1.2834 ||:  58%|█████▊    | 486/840 [00:28<00:19, 18.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3619, loss: 1.2819 ||:  58%|█████▊    | 489/840 [00:28<00:18, 18.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3628, loss: 1.2804 ||:  58%|█████▊    | 491/840 [00:28<00:18, 18.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3640, loss: 1.2789 ||:  59%|█████▊    | 493/840 [00:28<00:18, 18.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3646, loss: 1.2775 ||:  59%|█████▉    | 495/840 [00:28<00:18, 18.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3652, loss: 1.2766 ||:  59%|█████▉    | 497/840 [00:28<00:19, 17.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3660, loss: 1.2751 ||:  59%|█████▉    | 499/840 [00:28<00:18, 18.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3670, loss: 1.2738 ||:  60%|█████▉    | 501/840 [00:28<00:20, 16.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3677, loss: 1.2727 ||:  60%|█████▉    | 503/840 [00:29<00:19, 16.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3688, loss: 1.2714 ||:  60%|██████    | 505/840 [00:29<00:19, 16.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3692, loss: 1.2699 ||:  60%|██████    | 507/840 [00:29<00:19, 17.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3703, loss: 1.2680 ||:  61%|██████    | 509/840 [00:29<00:18, 17.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3709, loss: 1.2672 ||:  61%|██████    | 511/840 [00:29<00:20, 16.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3717, loss: 1.2662 ||:  61%|██████    | 513/840 [00:29<00:20, 16.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3737, loss: 1.2642 ||:  61%|██████▏   | 516/840 [00:29<00:18, 17.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3745, loss: 1.2635 ||:  62%|██████▏   | 518/840 [00:29<00:18, 17.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3748, loss: 1.2624 ||:  62%|██████▏   | 520/840 [00:30<00:18, 17.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3758, loss: 1.2612 ||:  62%|██████▏   | 522/840 [00:30<00:17, 17.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3768, loss: 1.2598 ||:  62%|██████▏   | 524/840 [00:30<00:18, 17.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3782, loss: 1.2581 ||:  63%|██████▎   | 526/840 [00:30<00:18, 17.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3796, loss: 1.2563 ||:  63%|██████▎   | 528/840 [00:30<00:18, 17.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3807, loss: 1.2548 ||:  63%|██████▎   | 530/840 [00:30<00:17, 17.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3820, loss: 1.2531 ||:  63%|██████▎   | 532/840 [00:30<00:16, 18.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3828, loss: 1.2516 ||:  64%|██████▎   | 534/840 [00:30<00:16, 18.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3843, loss: 1.2497 ||:  64%|██████▍   | 536/840 [00:30<00:16, 17.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3850, loss: 1.2483 ||:  64%|██████▍   | 538/840 [00:31<00:16, 18.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3865, loss: 1.2465 ||:  64%|██████▍   | 540/840 [00:31<00:16, 18.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3875, loss: 1.2446 ||:  65%|██████▍   | 542/840 [00:31<00:16, 18.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3883, loss: 1.2431 ||:  65%|██████▍   | 544/840 [00:31<00:16, 17.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3893, loss: 1.2415 ||:  65%|██████▌   | 546/840 [00:31<00:16, 17.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3901, loss: 1.2404 ||:  65%|██████▌   | 548/840 [00:31<00:16, 18.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3911, loss: 1.2390 ||:  65%|██████▌   | 550/840 [00:31<00:16, 17.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3918, loss: 1.2378 ||:  66%|██████▌   | 552/840 [00:31<00:16, 17.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3924, loss: 1.2364 ||:  66%|██████▌   | 554/840 [00:31<00:16, 17.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3927, loss: 1.2353 ||:  66%|██████▌   | 556/840 [00:32<00:16, 17.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3937, loss: 1.2340 ||:  66%|██████▋   | 558/840 [00:32<00:15, 17.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3943, loss: 1.2327 ||:  67%|██████▋   | 560/840 [00:32<00:15, 17.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3952, loss: 1.2310 ||:  67%|██████▋   | 562/840 [00:32<00:15, 17.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3963, loss: 1.2297 ||:  67%|██████▋   | 564/840 [00:32<00:15, 17.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3970, loss: 1.2283 ||:  67%|██████▋   | 566/840 [00:32<00:15, 17.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3977, loss: 1.2268 ||:  68%|██████▊   | 568/840 [00:32<00:15, 17.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3988, loss: 1.2252 ||:  68%|██████▊   | 570/840 [00:32<00:14, 18.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.3997, loss: 1.2236 ||:  68%|██████▊   | 572/840 [00:32<00:14, 18.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4007, loss: 1.2221 ||:  68%|██████▊   | 574/840 [00:33<00:14, 18.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4018, loss: 1.2206 ||:  69%|██████▊   | 576/840 [00:33<00:14, 18.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4029, loss: 1.2186 ||:  69%|██████▉   | 578/840 [00:33<00:14, 17.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4039, loss: 1.2166 ||:  69%|██████▉   | 580/840 [00:33<00:14, 17.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4050, loss: 1.2149 ||:  69%|██████▉   | 582/840 [00:33<00:14, 18.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4059, loss: 1.2132 ||:  70%|██████▉   | 584/840 [00:33<00:14, 17.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4067, loss: 1.2120 ||:  70%|██████▉   | 586/840 [00:33<00:14, 16.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4071, loss: 1.2111 ||:  70%|███████   | 588/840 [00:33<00:15, 16.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4077, loss: 1.2096 ||:  70%|███████   | 590/840 [00:33<00:14, 17.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4084, loss: 1.2078 ||:  70%|███████   | 592/840 [00:34<00:14, 17.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4092, loss: 1.2061 ||:  71%|███████   | 594/840 [00:34<00:14, 17.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4098, loss: 1.2045 ||:  71%|███████   | 596/840 [00:34<00:13, 17.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4107, loss: 1.2030 ||:  71%|███████   | 598/840 [00:34<00:13, 18.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4113, loss: 1.2018 ||:  71%|███████▏  | 600/840 [00:34<00:13, 17.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4116, loss: 1.2001 ||:  72%|███████▏  | 602/840 [00:34<00:13, 17.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4127, loss: 1.1983 ||:  72%|███████▏  | 604/840 [00:34<00:13, 17.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4133, loss: 1.1965 ||:  72%|███████▏  | 606/840 [00:34<00:13, 16.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4138, loss: 1.1950 ||:  72%|███████▏  | 608/840 [00:35<00:13, 17.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4145, loss: 1.1939 ||:  73%|███████▎  | 610/840 [00:35<00:13, 17.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4154, loss: 1.1923 ||:  73%|███████▎  | 612/840 [00:35<00:12, 17.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4162, loss: 1.1908 ||:  73%|███████▎  | 614/840 [00:35<00:12, 18.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4170, loss: 1.1895 ||:  73%|███████▎  | 616/840 [00:35<00:12, 18.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4179, loss: 1.1878 ||:  74%|███████▎  | 618/840 [00:35<00:12, 18.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4187, loss: 1.1861 ||:  74%|███████▍  | 620/840 [00:35<00:11, 18.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4198, loss: 1.1845 ||:  74%|███████▍  | 622/840 [00:35<00:12, 17.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4202, loss: 1.1828 ||:  74%|███████▍  | 624/840 [00:35<00:12, 17.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4212, loss: 1.1809 ||:  75%|███████▍  | 626/840 [00:35<00:11, 18.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4224, loss: 1.1791 ||:  75%|███████▍  | 628/840 [00:36<00:11, 18.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4239, loss: 1.1769 ||:  75%|███████▌  | 631/840 [00:36<00:10, 19.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4247, loss: 1.1752 ||:  75%|███████▌  | 633/840 [00:36<00:10, 19.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4255, loss: 1.1736 ||:  76%|███████▌  | 635/840 [00:36<00:10, 19.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4265, loss: 1.1718 ||:  76%|███████▌  | 637/840 [00:36<00:10, 19.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4272, loss: 1.1703 ||:  76%|███████▌  | 639/840 [00:36<00:10, 19.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4281, loss: 1.1688 ||:  76%|███████▋  | 641/840 [00:36<00:10, 18.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4285, loss: 1.1674 ||:  77%|███████▋  | 643/840 [00:36<00:10, 18.47it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4289, loss: 1.1660 ||:  77%|███████▋  | 645/840 [00:36<00:11, 17.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4298, loss: 1.1646 ||:  77%|███████▋  | 647/840 [00:37<00:10, 17.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4303, loss: 1.1628 ||:  77%|███████▋  | 649/840 [00:37<00:10, 17.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4310, loss: 1.1613 ||:  78%|███████▊  | 651/840 [00:37<00:10, 18.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4315, loss: 1.1601 ||:  78%|███████▊  | 653/840 [00:37<00:10, 18.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4325, loss: 1.1588 ||:  78%|███████▊  | 655/840 [00:37<00:10, 17.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4330, loss: 1.1575 ||:  78%|███████▊  | 657/840 [00:37<00:10, 18.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4339, loss: 1.1559 ||:  78%|███████▊  | 659/840 [00:37<00:10, 17.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4348, loss: 1.1543 ||:  79%|███████▊  | 661/840 [00:37<00:10, 17.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4354, loss: 1.1525 ||:  79%|███████▉  | 663/840 [00:38<00:09, 17.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4361, loss: 1.1512 ||:  79%|███████▉  | 665/840 [00:38<00:09, 17.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4370, loss: 1.1495 ||:  79%|███████▉  | 667/840 [00:38<00:09, 17.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4380, loss: 1.1477 ||:  80%|███████▉  | 669/840 [00:38<00:09, 17.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4392, loss: 1.1462 ||:  80%|███████▉  | 671/840 [00:38<00:09, 18.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4404, loss: 1.1444 ||:  80%|████████  | 673/840 [00:38<00:09, 17.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4410, loss: 1.1430 ||:  80%|████████  | 675/840 [00:38<00:09, 17.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4418, loss: 1.1413 ||:  81%|████████  | 677/840 [00:38<00:09, 17.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4429, loss: 1.1402 ||:  81%|████████  | 679/840 [00:38<00:09, 17.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4439, loss: 1.1385 ||:  81%|████████  | 681/840 [00:39<00:09, 17.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4447, loss: 1.1365 ||:  81%|████████▏ | 683/840 [00:39<00:09, 17.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4455, loss: 1.1344 ||:  82%|████████▏ | 685/840 [00:39<00:08, 17.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4463, loss: 1.1329 ||:  82%|████████▏ | 687/840 [00:39<00:08, 17.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4471, loss: 1.1315 ||:  82%|████████▏ | 689/840 [00:39<00:08, 18.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4479, loss: 1.1300 ||:  82%|████████▏ | 691/840 [00:39<00:08, 18.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4484, loss: 1.1289 ||:  82%|████████▎ | 693/840 [00:39<00:08, 18.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4489, loss: 1.1277 ||:  83%|████████▎ | 695/840 [00:39<00:07, 18.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4499, loss: 1.1262 ||:  83%|████████▎ | 697/840 [00:39<00:07, 18.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4505, loss: 1.1246 ||:  83%|████████▎ | 699/840 [00:39<00:07, 18.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4511, loss: 1.1233 ||:  83%|████████▎ | 701/840 [00:40<00:08, 17.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4517, loss: 1.1219 ||:  84%|████████▎ | 703/840 [00:40<00:07, 17.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4524, loss: 1.1203 ||:  84%|████████▍ | 705/840 [00:40<00:07, 17.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4531, loss: 1.1192 ||:  84%|████████▍ | 707/840 [00:40<00:07, 17.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4537, loss: 1.1177 ||:  84%|████████▍ | 709/840 [00:40<00:07, 17.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4544, loss: 1.1166 ||:  85%|████████▍ | 711/840 [00:40<00:07, 17.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4552, loss: 1.1152 ||:  85%|████████▍ | 713/840 [00:40<00:07, 17.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4557, loss: 1.1136 ||:  85%|████████▌ | 715/840 [00:40<00:06, 18.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4560, loss: 1.1130 ||:  85%|████████▌ | 717/840 [00:41<00:06, 18.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4564, loss: 1.1117 ||:  86%|████████▌ | 719/840 [00:41<00:06, 18.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4570, loss: 1.1105 ||:  86%|████████▌ | 721/840 [00:41<00:06, 18.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4577, loss: 1.1090 ||:  86%|████████▌ | 723/840 [00:41<00:06, 18.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4590, loss: 1.1069 ||:  86%|████████▋ | 726/840 [00:41<00:05, 19.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4596, loss: 1.1060 ||:  87%|████████▋ | 728/840 [00:41<00:05, 19.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4599, loss: 1.1047 ||:  87%|████████▋ | 730/840 [00:41<00:06, 17.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4605, loss: 1.1035 ||:  87%|████████▋ | 732/840 [00:41<00:05, 18.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4612, loss: 1.1019 ||:  87%|████████▋ | 734/840 [00:41<00:05, 18.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4618, loss: 1.1008 ||:  88%|████████▊ | 736/840 [00:42<00:05, 18.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4627, loss: 1.0991 ||:  88%|████████▊ | 738/840 [00:42<00:05, 18.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4630, loss: 1.0978 ||:  88%|████████▊ | 740/840 [00:42<00:05, 17.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4637, loss: 1.0968 ||:  88%|████████▊ | 742/840 [00:42<00:05, 17.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4644, loss: 1.0953 ||:  89%|████████▊ | 744/840 [00:42<00:05, 17.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4649, loss: 1.0937 ||:  89%|████████▉ | 746/840 [00:42<00:05, 17.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4656, loss: 1.0926 ||:  89%|████████▉ | 748/840 [00:42<00:05, 17.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4659, loss: 1.0912 ||:  89%|████████▉ | 750/840 [00:42<00:05, 17.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4662, loss: 1.0901 ||:  90%|████████▉ | 752/840 [00:42<00:04, 17.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4666, loss: 1.0896 ||:  90%|████████▉ | 754/840 [00:43<00:04, 18.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4671, loss: 1.0881 ||:  90%|█████████ | 756/840 [00:43<00:04, 18.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4681, loss: 1.0863 ||:  90%|█████████ | 758/840 [00:43<00:04, 17.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4688, loss: 1.0846 ||:  90%|█████████ | 760/840 [00:43<00:04, 17.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4694, loss: 1.0839 ||:  91%|█████████ | 762/840 [00:43<00:04, 17.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4701, loss: 1.0826 ||:  91%|█████████ | 764/840 [00:43<00:04, 17.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4710, loss: 1.0812 ||:  91%|█████████ | 766/840 [00:43<00:04, 18.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4716, loss: 1.0803 ||:  91%|█████████▏| 768/840 [00:43<00:03, 18.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4725, loss: 1.0791 ||:  92%|█████████▏| 770/840 [00:43<00:03, 18.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4733, loss: 1.0783 ||:  92%|█████████▏| 772/840 [00:44<00:03, 18.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4740, loss: 1.0768 ||:  92%|█████████▏| 774/840 [00:44<00:03, 18.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4744, loss: 1.0753 ||:  92%|█████████▏| 776/840 [00:44<00:03, 17.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4751, loss: 1.0739 ||:  93%|█████████▎| 778/840 [00:44<00:03, 17.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4760, loss: 1.0727 ||:  93%|█████████▎| 780/840 [00:44<00:03, 17.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4767, loss: 1.0712 ||:  93%|█████████▎| 782/840 [00:44<00:03, 18.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4776, loss: 1.0698 ||:  93%|█████████▎| 784/840 [00:44<00:03, 17.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4786, loss: 1.0686 ||:  94%|█████████▎| 786/840 [00:44<00:03, 17.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4794, loss: 1.0672 ||:  94%|█████████▍| 788/840 [00:44<00:03, 17.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4802, loss: 1.0660 ||:  94%|█████████▍| 790/840 [00:45<00:02, 16.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4810, loss: 1.0645 ||:  94%|█████████▍| 792/840 [00:45<00:02, 17.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4819, loss: 1.0628 ||:  95%|█████████▍| 794/840 [00:45<00:02, 18.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4827, loss: 1.0615 ||:  95%|█████████▍| 796/840 [00:45<00:02, 18.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4834, loss: 1.0604 ||:  95%|█████████▌| 798/840 [00:45<00:02, 18.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4838, loss: 1.0595 ||:  95%|█████████▌| 800/840 [00:45<00:02, 17.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4845, loss: 1.0584 ||:  95%|█████████▌| 802/840 [00:45<00:02, 17.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4854, loss: 1.0574 ||:  96%|█████████▌| 804/840 [00:45<00:02, 17.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4858, loss: 1.0562 ||:  96%|█████████▌| 806/840 [00:45<00:01, 17.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4865, loss: 1.0550 ||:  96%|█████████▌| 808/840 [00:46<00:01, 17.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4869, loss: 1.0540 ||:  96%|█████████▋| 810/840 [00:46<00:01, 16.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4874, loss: 1.0532 ||:  97%|█████████▋| 812/840 [00:46<00:01, 16.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4874, loss: 1.0530 ||:  97%|█████████▋| 814/840 [00:46<00:01, 16.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4881, loss: 1.0517 ||:  97%|█████████▋| 816/840 [00:46<00:01, 17.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4889, loss: 1.0505 ||:  97%|█████████▋| 818/840 [00:46<00:01, 16.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4898, loss: 1.0493 ||:  98%|█████████▊| 820/840 [00:46<00:01, 16.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4906, loss: 1.0484 ||:  98%|█████████▊| 822/840 [00:46<00:01, 16.86it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4911, loss: 1.0477 ||:  98%|█████████▊| 824/840 [00:47<00:00, 17.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4919, loss: 1.0466 ||:  98%|█████████▊| 826/840 [00:47<00:00, 17.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4926, loss: 1.0460 ||:  99%|█████████▊| 828/840 [00:47<00:00, 17.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4934, loss: 1.0450 ||:  99%|█████████▉| 830/840 [00:47<00:00, 18.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4941, loss: 1.0441 ||:  99%|█████████▉| 832/840 [00:47<00:00, 18.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4947, loss: 1.0435 ||:  99%|█████████▉| 834/840 [00:47<00:00, 18.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4955, loss: 1.0421 ||: 100%|█████████▉| 836/840 [00:47<00:00, 17.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4962, loss: 1.0407 ||: 100%|█████████▉| 838/840 [00:47<00:00, 17.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.4967, loss: 1.0395 ||: 100%|██████████| 840/840 [00:47<00:00, 17.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/360 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7357, loss: 0.6404 ||:   4%|▍         | 14/360 [00:00<00:02, 135.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7519, loss: 0.6186 ||:   7%|▋         | 26/360 [00:00<00:02, 128.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7641, loss: 0.6221 ||:  11%|█         | 39/360 [00:00<00:02, 128.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7774, loss: 0.6226 ||:  15%|█▍        | 53/360 [00:00<00:02, 129.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7847, loss: 0.6194 ||:  20%|██        | 72/360 [00:00<00:02, 142.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7900, loss: 0.6176 ||:  25%|██▌       | 90/360 [00:00<00:01, 150.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7926, loss: 0.6188 ||:  30%|███       | 108/360 [00:00<00:01, 158.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7896, loss: 0.6177 ||:  35%|███▍      | 125/360 [00:00<00:01, 160.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7920, loss: 0.6094 ||:  40%|███▉      | 143/360 [00:00<00:01, 165.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7954, loss: 0.6027 ||:  45%|████▌     | 162/360 [00:01<00:01, 170.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7969, loss: 0.6023 ||:  50%|█████     | 180/360 [00:01<00:01, 170.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7985, loss: 0.6009 ||:  55%|█████▍    | 197/360 [00:01<00:00, 169.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8030, loss: 0.5947 ||:  60%|█████▉    | 215/360 [00:01<00:00, 170.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8011, loss: 0.5950 ||:  65%|██████▍   | 233/360 [00:01<00:00, 172.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8000, loss: 0.5951 ||:  70%|██████▉   | 251/360 [00:01<00:00, 172.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8019, loss: 0.5986 ||:  75%|███████▍  | 269/360 [00:01<00:00, 171.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8028, loss: 0.5970 ||:  80%|███████▉  | 287/360 [00:01<00:00, 170.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8015, loss: 0.5969 ||:  85%|████████▌ | 306/360 [00:01<00:00, 174.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8003, loss: 0.5974 ||:  91%|█████████ | 326/360 [00:01<00:00, 179.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7999, loss: 0.5962 ||:  96%|█████████▌| 345/360 [00:02<00:00, 178.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8003, loss: 0.5953 ||: 100%|██████████| 360/360 [00:02<00:00, 166.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/840 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8750, loss: 0.4904 ||:   0%|          | 2/840 [00:00<00:59, 14.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8500, loss: 0.5087 ||:   0%|          | 4/840 [00:00<00:54, 15.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8167, loss: 0.5226 ||:   1%|          | 6/840 [00:00<00:53, 15.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8125, loss: 0.5612 ||:   1%|          | 8/840 [00:00<00:52, 15.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8150, loss: 0.5857 ||:   1%|          | 10/840 [00:00<00:49, 16.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7917, loss: 0.5765 ||:   1%|▏         | 12/840 [00:00<00:51, 16.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8000, loss: 0.6000 ||:   2%|▏         | 14/840 [00:00<00:49, 16.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7969, loss: 0.5794 ||:   2%|▏         | 16/840 [00:00<00:48, 17.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7972, loss: 0.5704 ||:   2%|▏         | 18/840 [00:01<00:48, 17.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8025, loss: 0.5552 ||:   2%|▏         | 20/840 [00:01<00:48, 16.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8045, loss: 0.5570 ||:   3%|▎         | 22/840 [00:01<00:47, 17.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8187, loss: 0.5422 ||:   3%|▎         | 24/840 [00:01<00:47, 17.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8250, loss: 0.5418 ||:   3%|▎         | 26/840 [00:01<00:47, 17.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8286, loss: 0.5332 ||:   3%|▎         | 28/840 [00:01<00:49, 16.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8133, loss: 0.5409 ||:   4%|▎         | 30/840 [00:01<00:47, 16.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8078, loss: 0.5411 ||:   4%|▍         | 32/840 [00:01<00:47, 16.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8044, loss: 0.5360 ||:   4%|▍         | 34/840 [00:02<00:47, 16.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7972, loss: 0.5454 ||:   4%|▍         | 36/840 [00:02<00:49, 16.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7934, loss: 0.5473 ||:   5%|▍         | 38/840 [00:02<00:49, 16.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7937, loss: 0.5435 ||:   5%|▍         | 40/840 [00:02<00:47, 16.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7940, loss: 0.5425 ||:   5%|▌         | 42/840 [00:02<00:47, 16.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7977, loss: 0.5390 ||:   5%|▌         | 44/840 [00:02<00:50, 15.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7957, loss: 0.5378 ||:   5%|▌         | 46/840 [00:02<00:49, 16.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.7979, loss: 0.5323 ||:   6%|▌         | 48/840 [00:02<00:47, 16.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8020, loss: 0.5264 ||:   6%|▌         | 50/840 [00:02<00:45, 17.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8019, loss: 0.5361 ||:   6%|▌         | 52/840 [00:03<00:44, 17.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8037, loss: 0.5345 ||:   6%|▋         | 54/840 [00:03<00:44, 17.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8054, loss: 0.5321 ||:   7%|▋         | 56/840 [00:03<00:43, 17.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8026, loss: 0.5355 ||:   7%|▋         | 58/840 [00:03<00:43, 17.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8033, loss: 0.5344 ||:   7%|▋         | 60/840 [00:03<00:47, 16.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8040, loss: 0.5317 ||:   7%|▋         | 62/840 [00:03<00:48, 16.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8078, loss: 0.5241 ||:   8%|▊         | 64/840 [00:03<00:47, 16.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8114, loss: 0.5212 ||:   8%|▊         | 66/840 [00:03<00:47, 16.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8103, loss: 0.5210 ||:   8%|▊         | 68/840 [00:04<00:48, 16.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8114, loss: 0.5193 ||:   8%|▊         | 70/840 [00:04<00:47, 16.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8125, loss: 0.5187 ||:   9%|▊         | 72/840 [00:04<00:46, 16.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8115, loss: 0.5224 ||:   9%|▉         | 74/840 [00:04<00:45, 16.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8132, loss: 0.5229 ||:   9%|▉         | 76/840 [00:04<00:45, 16.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8128, loss: 0.5207 ||:   9%|▉         | 78/840 [00:04<00:45, 16.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8150, loss: 0.5176 ||:  10%|▉         | 80/840 [00:04<00:44, 17.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8159, loss: 0.5176 ||:  10%|▉         | 82/840 [00:04<00:43, 17.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8155, loss: 0.5167 ||:  10%|█         | 84/840 [00:04<00:43, 17.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8151, loss: 0.5188 ||:  10%|█         | 86/840 [00:05<00:43, 17.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8159, loss: 0.5179 ||:  10%|█         | 88/840 [00:05<00:43, 17.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8178, loss: 0.5140 ||:  11%|█         | 90/840 [00:05<00:42, 17.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8207, loss: 0.5116 ||:  11%|█         | 92/840 [00:05<00:42, 17.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8186, loss: 0.5168 ||:  11%|█         | 94/840 [00:05<00:43, 17.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8187, loss: 0.5162 ||:  11%|█▏        | 96/840 [00:05<00:43, 16.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8199, loss: 0.5146 ||:  12%|█▏        | 98/840 [00:05<00:44, 16.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8215, loss: 0.5115 ||:  12%|█▏        | 100/840 [00:05<00:42, 17.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8225, loss: 0.5089 ||:  12%|█▏        | 102/840 [00:06<00:41, 17.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8240, loss: 0.5079 ||:  12%|█▏        | 104/840 [00:06<00:42, 17.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8241, loss: 0.5084 ||:  13%|█▎        | 106/840 [00:06<00:41, 17.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8239, loss: 0.5100 ||:  13%|█▎        | 109/840 [00:06<00:40, 17.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8252, loss: 0.5069 ||:  13%|█▎        | 111/840 [00:06<00:40, 17.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8246, loss: 0.5090 ||:  14%|█▎        | 114/840 [00:06<00:39, 18.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8250, loss: 0.5061 ||:  14%|█▍        | 116/840 [00:06<00:38, 18.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8246, loss: 0.5087 ||:  14%|█▍        | 118/840 [00:06<00:38, 18.97it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8258, loss: 0.5075 ||:  14%|█▍        | 120/840 [00:06<00:38, 18.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8250, loss: 0.5071 ||:  15%|█▍        | 122/840 [00:07<00:38, 18.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8242, loss: 0.5102 ||:  15%|█▍        | 124/840 [00:07<00:40, 17.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8238, loss: 0.5094 ||:  15%|█▌        | 126/840 [00:07<00:39, 18.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8250, loss: 0.5072 ||:  15%|█▌        | 128/840 [00:07<00:39, 18.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8246, loss: 0.5070 ||:  15%|█▌        | 130/840 [00:07<00:39, 18.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8256, loss: 0.5065 ||:  16%|█▌        | 133/840 [00:07<00:38, 18.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8259, loss: 0.5050 ||:  16%|█▌        | 135/840 [00:07<00:37, 18.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8255, loss: 0.5062 ||:  16%|█▋        | 137/840 [00:07<00:39, 17.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8255, loss: 0.5061 ||:  17%|█▋        | 139/840 [00:08<00:39, 17.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8255, loss: 0.5046 ||:  17%|█▋        | 141/840 [00:08<00:38, 17.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8248, loss: 0.5070 ||:  17%|█▋        | 143/840 [00:08<00:38, 18.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8259, loss: 0.5062 ||:  17%|█▋        | 145/840 [00:08<00:37, 18.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8272, loss: 0.5038 ||:  18%|█▊        | 147/840 [00:08<00:40, 17.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8258, loss: 0.5056 ||:  18%|█▊        | 149/840 [00:08<00:39, 17.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8248, loss: 0.5073 ||:  18%|█▊        | 151/840 [00:08<00:38, 17.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8255, loss: 0.5066 ||:  18%|█▊        | 153/840 [00:08<00:38, 17.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8248, loss: 0.5075 ||:  18%|█▊        | 155/840 [00:08<00:38, 17.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8261, loss: 0.5060 ||:  19%|█▊        | 157/840 [00:09<00:38, 17.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8258, loss: 0.5058 ||:  19%|█▉        | 159/840 [00:09<00:37, 18.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8258, loss: 0.5067 ||:  19%|█▉        | 161/840 [00:09<00:39, 17.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8261, loss: 0.5067 ||:  19%|█▉        | 163/840 [00:09<00:38, 17.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8258, loss: 0.5071 ||:  20%|█▉        | 165/840 [00:09<00:37, 18.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8254, loss: 0.5065 ||:  20%|█▉        | 167/840 [00:09<00:36, 18.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8263, loss: 0.5053 ||:  20%|██        | 169/840 [00:09<00:36, 18.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8266, loss: 0.5037 ||:  20%|██        | 171/840 [00:09<00:37, 18.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8272, loss: 0.5013 ||:  21%|██        | 173/840 [00:09<00:37, 17.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8283, loss: 0.4992 ||:  21%|██        | 175/840 [00:10<00:38, 17.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8285, loss: 0.4981 ||:  21%|██        | 177/840 [00:10<00:37, 17.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8277, loss: 0.4981 ||:  21%|██▏       | 179/840 [00:10<00:37, 17.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8287, loss: 0.4957 ||:  22%|██▏       | 181/840 [00:10<00:36, 17.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8290, loss: 0.4958 ||:  22%|██▏       | 183/840 [00:10<00:37, 17.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8292, loss: 0.4956 ||:  22%|██▏       | 185/840 [00:10<00:37, 17.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8299, loss: 0.4934 ||:  22%|██▏       | 187/840 [00:10<00:36, 17.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8302, loss: 0.4922 ||:  22%|██▎       | 189/840 [00:10<00:36, 17.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8306, loss: 0.4911 ||:  23%|██▎       | 191/840 [00:10<00:35, 18.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8311, loss: 0.4914 ||:  23%|██▎       | 193/840 [00:11<00:36, 17.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8313, loss: 0.4903 ||:  23%|██▎       | 195/840 [00:11<00:36, 17.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8320, loss: 0.4888 ||:  23%|██▎       | 197/840 [00:11<00:35, 17.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8322, loss: 0.4883 ||:  24%|██▎       | 199/840 [00:11<00:37, 17.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8323, loss: 0.4879 ||:  24%|██▍       | 201/840 [00:11<00:36, 17.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8318, loss: 0.4886 ||:  24%|██▍       | 203/840 [00:11<00:35, 18.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8327, loss: 0.4874 ||:  24%|██▍       | 205/840 [00:11<00:34, 18.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8331, loss: 0.4864 ||:  25%|██▍       | 207/840 [00:11<00:34, 18.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8330, loss: 0.4860 ||:  25%|██▍       | 209/840 [00:11<00:35, 17.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8325, loss: 0.4854 ||:  25%|██▌       | 211/840 [00:12<00:35, 17.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8326, loss: 0.4860 ||:  25%|██▌       | 213/840 [00:12<00:34, 17.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8333, loss: 0.4841 ||:  26%|██▌       | 215/840 [00:12<00:35, 17.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8343, loss: 0.4817 ||:  26%|██▌       | 217/840 [00:12<00:35, 17.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8340, loss: 0.4815 ||:  26%|██▌       | 219/840 [00:12<00:34, 17.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8335, loss: 0.4825 ||:  26%|██▋       | 221/840 [00:12<00:34, 18.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8332, loss: 0.4821 ||:  27%|██▋       | 223/840 [00:12<00:33, 18.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8338, loss: 0.4810 ||:  27%|██▋       | 225/840 [00:12<00:32, 18.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8346, loss: 0.4794 ||:  27%|██▋       | 227/840 [00:12<00:32, 18.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8345, loss: 0.4789 ||:  27%|██▋       | 229/840 [00:13<00:33, 18.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8346, loss: 0.4795 ||:  28%|██▊       | 231/840 [00:13<00:32, 18.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8354, loss: 0.4774 ||:  28%|██▊       | 233/840 [00:13<00:33, 18.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8355, loss: 0.4768 ||:  28%|██▊       | 235/840 [00:13<00:33, 18.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8354, loss: 0.4764 ||:  28%|██▊       | 237/840 [00:13<00:33, 18.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8351, loss: 0.4779 ||:  28%|██▊       | 239/840 [00:13<00:33, 17.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8349, loss: 0.4794 ||:  29%|██▊       | 241/840 [00:13<00:34, 17.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8352, loss: 0.4790 ||:  29%|██▉       | 244/840 [00:13<00:33, 18.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8348, loss: 0.4802 ||:  29%|██▉       | 246/840 [00:14<00:32, 18.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8353, loss: 0.4790 ||:  30%|██▉       | 248/840 [00:14<00:31, 18.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8356, loss: 0.4783 ||:  30%|██▉       | 250/840 [00:14<00:31, 18.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8355, loss: 0.4775 ||:  30%|███       | 252/840 [00:14<00:31, 18.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8350, loss: 0.4780 ||:  30%|███       | 254/840 [00:14<00:31, 18.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8346, loss: 0.4781 ||:  30%|███       | 256/840 [00:14<00:30, 18.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8347, loss: 0.4777 ||:  31%|███       | 258/840 [00:14<00:31, 18.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8346, loss: 0.4790 ||:  31%|███       | 260/840 [00:14<00:34, 16.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8349, loss: 0.4779 ||:  31%|███       | 262/840 [00:14<00:33, 17.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8347, loss: 0.4791 ||:  31%|███▏      | 264/840 [00:15<00:33, 17.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8353, loss: 0.4782 ||:  32%|███▏      | 266/840 [00:15<00:32, 17.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8358, loss: 0.4773 ||:  32%|███▏      | 268/840 [00:15<00:31, 18.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8361, loss: 0.4764 ||:  32%|███▏      | 270/840 [00:15<00:31, 17.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8363, loss: 0.4757 ||:  32%|███▎      | 273/840 [00:15<00:30, 18.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8371, loss: 0.4746 ||:  33%|███▎      | 275/840 [00:15<00:30, 18.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8370, loss: 0.4743 ||:  33%|███▎      | 277/840 [00:15<00:30, 18.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8375, loss: 0.4733 ||:  33%|███▎      | 279/840 [00:15<00:30, 18.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8379, loss: 0.4721 ||:  33%|███▎      | 281/840 [00:15<00:30, 18.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8378, loss: 0.4735 ||:  34%|███▎      | 283/840 [00:16<00:30, 18.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8384, loss: 0.4722 ||:  34%|███▍      | 285/840 [00:16<00:30, 18.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8387, loss: 0.4722 ||:  34%|███▍      | 287/840 [00:16<00:31, 17.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8388, loss: 0.4721 ||:  34%|███▍      | 289/840 [00:16<00:31, 17.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8388, loss: 0.4719 ||:  35%|███▍      | 291/840 [00:16<00:30, 17.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8394, loss: 0.4705 ||:  35%|███▍      | 293/840 [00:16<00:30, 17.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8393, loss: 0.4706 ||:  35%|███▌      | 295/840 [00:16<00:32, 16.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8396, loss: 0.4703 ||:  35%|███▌      | 297/840 [00:16<00:32, 16.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8396, loss: 0.4708 ||:  36%|███▌      | 299/840 [00:17<00:34, 15.47it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8402, loss: 0.4696 ||:  36%|███▌      | 301/840 [00:17<00:33, 16.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8403, loss: 0.4696 ||:  36%|███▌      | 303/840 [00:17<00:32, 16.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8405, loss: 0.4687 ||:  36%|███▋      | 305/840 [00:17<00:32, 16.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8412, loss: 0.4676 ||:  37%|███▋      | 307/840 [00:17<00:31, 16.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8405, loss: 0.4688 ||:  37%|███▋      | 309/840 [00:17<00:32, 16.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8402, loss: 0.4689 ||:  37%|███▋      | 311/840 [00:17<00:31, 16.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8404, loss: 0.4683 ||:  37%|███▋      | 313/840 [00:17<00:30, 17.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8410, loss: 0.4672 ||:  38%|███▊      | 315/840 [00:17<00:31, 16.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8404, loss: 0.4682 ||:  38%|███▊      | 317/840 [00:18<00:31, 16.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8404, loss: 0.4678 ||:  38%|███▊      | 319/840 [00:18<00:32, 16.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8403, loss: 0.4679 ||:  38%|███▊      | 321/840 [00:18<00:32, 15.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8401, loss: 0.4684 ||:  38%|███▊      | 323/840 [00:18<00:32, 16.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8398, loss: 0.4678 ||:  39%|███▊      | 325/840 [00:18<00:31, 16.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8398, loss: 0.4679 ||:  39%|███▉      | 327/840 [00:18<00:31, 16.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8404, loss: 0.4669 ||:  39%|███▉      | 329/840 [00:18<00:32, 15.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8406, loss: 0.4660 ||:  39%|███▉      | 331/840 [00:18<00:32, 15.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8405, loss: 0.4659 ||:  40%|███▉      | 333/840 [00:19<00:32, 15.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8410, loss: 0.4654 ||:  40%|███▉      | 335/840 [00:19<00:31, 16.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8412, loss: 0.4643 ||:  40%|████      | 337/840 [00:19<00:30, 16.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8409, loss: 0.4652 ||:  40%|████      | 339/840 [00:19<00:31, 15.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8413, loss: 0.4642 ||:  41%|████      | 341/840 [00:19<00:30, 16.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8417, loss: 0.4643 ||:  41%|████      | 343/840 [00:19<00:31, 16.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8419, loss: 0.4634 ||:  41%|████      | 345/840 [00:19<00:30, 16.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8425, loss: 0.4618 ||:  41%|████▏     | 347/840 [00:19<00:31, 15.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8430, loss: 0.4614 ||:  42%|████▏     | 349/840 [00:20<00:31, 15.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8432, loss: 0.4607 ||:  42%|████▏     | 351/840 [00:20<00:32, 15.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8436, loss: 0.4597 ||:  42%|████▏     | 353/840 [00:20<00:31, 15.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8441, loss: 0.4586 ||:  42%|████▏     | 355/840 [00:20<00:31, 15.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8441, loss: 0.4586 ||:  42%|████▎     | 357/840 [00:20<00:31, 15.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8437, loss: 0.4591 ||:  43%|████▎     | 359/840 [00:20<00:30, 15.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8435, loss: 0.4601 ||:  43%|████▎     | 361/840 [00:20<00:30, 15.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8435, loss: 0.4599 ||:  43%|████▎     | 363/840 [00:20<00:29, 16.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8434, loss: 0.4608 ||:  43%|████▎     | 365/840 [00:21<00:30, 15.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8437, loss: 0.4601 ||:  44%|████▎     | 367/840 [00:21<00:29, 15.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8440, loss: 0.4595 ||:  44%|████▍     | 369/840 [00:21<00:30, 15.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8441, loss: 0.4599 ||:  44%|████▍     | 371/840 [00:21<00:30, 15.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8434, loss: 0.4608 ||:  44%|████▍     | 373/840 [00:21<00:31, 14.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8429, loss: 0.4610 ||:  45%|████▍     | 375/840 [00:21<00:31, 14.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8432, loss: 0.4604 ||:  45%|████▍     | 377/840 [00:21<00:29, 15.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8433, loss: 0.4603 ||:  45%|████▌     | 379/840 [00:22<00:29, 15.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8437, loss: 0.4598 ||:  45%|████▌     | 381/840 [00:22<00:29, 15.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8437, loss: 0.4609 ||:  46%|████▌     | 383/840 [00:22<00:29, 15.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8436, loss: 0.4613 ||:  46%|████▌     | 385/840 [00:22<00:28, 15.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8441, loss: 0.4605 ||:  46%|████▌     | 387/840 [00:22<00:30, 15.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8442, loss: 0.4599 ||:  46%|████▋     | 389/840 [00:22<00:30, 14.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8442, loss: 0.4601 ||:  47%|████▋     | 391/840 [00:22<00:29, 14.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8444, loss: 0.4596 ||:  47%|████▋     | 393/840 [00:22<00:29, 14.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8443, loss: 0.4591 ||:  47%|████▋     | 395/840 [00:23<00:30, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8442, loss: 0.4590 ||:  47%|████▋     | 397/840 [00:23<00:29, 15.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8440, loss: 0.4593 ||:  48%|████▊     | 399/840 [00:23<00:29, 14.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8441, loss: 0.4591 ||:  48%|████▊     | 401/840 [00:23<00:28, 15.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8439, loss: 0.4585 ||:  48%|████▊     | 403/840 [00:23<00:29, 15.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8444, loss: 0.4575 ||:  48%|████▊     | 405/840 [00:23<00:28, 15.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8445, loss: 0.4570 ||:  48%|████▊     | 407/840 [00:23<00:28, 15.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8451, loss: 0.4555 ||:  49%|████▊     | 409/840 [00:24<00:27, 15.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8446, loss: 0.4558 ||:  49%|████▉     | 411/840 [00:24<00:27, 15.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8449, loss: 0.4554 ||:  49%|████▉     | 413/840 [00:24<00:28, 14.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8451, loss: 0.4548 ||:  49%|████▉     | 415/840 [00:24<00:28, 14.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8451, loss: 0.4543 ||:  50%|████▉     | 417/840 [00:24<00:27, 15.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8452, loss: 0.4536 ||:  50%|████▉     | 419/840 [00:24<00:27, 15.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8447, loss: 0.4548 ||:  50%|█████     | 421/840 [00:24<00:27, 15.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8453, loss: 0.4535 ||:  50%|█████     | 423/840 [00:24<00:27, 15.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8455, loss: 0.4532 ||:  51%|█████     | 425/840 [00:25<00:28, 14.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8461, loss: 0.4522 ||:  51%|█████     | 427/840 [00:25<00:26, 15.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8463, loss: 0.4514 ||:  51%|█████     | 429/840 [00:25<00:26, 15.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8464, loss: 0.4510 ||:  51%|█████▏    | 431/840 [00:25<00:26, 15.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8465, loss: 0.4505 ||:  52%|█████▏    | 433/840 [00:25<00:27, 14.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8469, loss: 0.4492 ||:  52%|█████▏    | 435/840 [00:25<00:28, 14.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8467, loss: 0.4499 ||:  52%|█████▏    | 437/840 [00:25<00:28, 14.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8470, loss: 0.4491 ||:  52%|█████▏    | 439/840 [00:26<00:27, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8471, loss: 0.4488 ||:  52%|█████▎    | 441/840 [00:26<00:26, 14.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8466, loss: 0.4499 ||:  53%|█████▎    | 443/840 [00:26<00:26, 14.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8469, loss: 0.4490 ||:  53%|█████▎    | 445/840 [00:26<00:25, 15.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8465, loss: 0.4501 ||:  53%|█████▎    | 447/840 [00:26<00:24, 15.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8470, loss: 0.4490 ||:  53%|█████▎    | 449/840 [00:26<00:25, 15.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8470, loss: 0.4489 ||:  54%|█████▎    | 451/840 [00:26<00:25, 15.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8468, loss: 0.4489 ||:  54%|█████▍    | 453/840 [00:26<00:25, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8468, loss: 0.4486 ||:  54%|█████▍    | 455/840 [00:27<00:25, 15.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8469, loss: 0.4481 ||:  54%|█████▍    | 457/840 [00:27<00:24, 15.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8471, loss: 0.4478 ||:  55%|█████▍    | 459/840 [00:27<00:24, 15.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8475, loss: 0.4473 ||:  55%|█████▍    | 461/840 [00:27<00:26, 14.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8477, loss: 0.4464 ||:  55%|█████▌    | 463/840 [00:27<00:25, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8481, loss: 0.4458 ||:  55%|█████▌    | 465/840 [00:27<00:26, 14.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8482, loss: 0.4452 ||:  56%|█████▌    | 467/840 [00:27<00:26, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8484, loss: 0.4448 ||:  56%|█████▌    | 469/840 [00:28<00:26, 14.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8485, loss: 0.4442 ||:  56%|█████▌    | 471/840 [00:28<00:27, 13.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8484, loss: 0.4444 ||:  56%|█████▋    | 473/840 [00:28<00:25, 14.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8486, loss: 0.4438 ||:  57%|█████▋    | 475/840 [00:28<00:24, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8484, loss: 0.4435 ||:  57%|█████▋    | 477/840 [00:28<00:23, 15.17it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8481, loss: 0.4440 ||:  57%|█████▋    | 479/840 [00:28<00:23, 15.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8484, loss: 0.4436 ||:  57%|█████▋    | 481/840 [00:28<00:23, 15.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8486, loss: 0.4428 ||:  57%|█████▊    | 483/840 [00:28<00:24, 14.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8487, loss: 0.4427 ||:  58%|█████▊    | 485/840 [00:29<00:24, 14.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8493, loss: 0.4415 ||:  58%|█████▊    | 487/840 [00:29<00:24, 14.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8495, loss: 0.4410 ||:  58%|█████▊    | 489/840 [00:29<00:25, 14.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8493, loss: 0.4419 ||:  58%|█████▊    | 491/840 [00:29<00:25, 13.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8495, loss: 0.4411 ||:  59%|█████▊    | 493/840 [00:29<00:25, 13.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8497, loss: 0.4405 ||:  59%|█████▉    | 495/840 [00:29<00:25, 13.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8495, loss: 0.4411 ||:  59%|█████▉    | 497/840 [00:30<00:26, 13.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8496, loss: 0.4404 ||:  59%|█████▉    | 499/840 [00:30<00:25, 13.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8500, loss: 0.4398 ||:  60%|█████▉    | 501/840 [00:30<00:25, 13.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8501, loss: 0.4395 ||:  60%|█████▉    | 503/840 [00:30<00:24, 13.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8498, loss: 0.4402 ||:  60%|██████    | 505/840 [00:30<00:23, 13.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8499, loss: 0.4401 ||:  60%|██████    | 507/840 [00:30<00:23, 14.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8504, loss: 0.4388 ||:  61%|██████    | 509/840 [00:30<00:23, 14.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8505, loss: 0.4385 ||:  61%|██████    | 511/840 [00:31<00:23, 14.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8506, loss: 0.4384 ||:  61%|██████    | 513/840 [00:31<00:24, 13.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8506, loss: 0.4385 ||:  61%|██████▏   | 515/840 [00:31<00:23, 13.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8505, loss: 0.4391 ||:  62%|██████▏   | 517/840 [00:31<00:23, 13.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8504, loss: 0.4392 ||:  62%|██████▏   | 519/840 [00:31<00:22, 14.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8502, loss: 0.4397 ||:  62%|██████▏   | 521/840 [00:31<00:22, 14.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8503, loss: 0.4392 ||:  62%|██████▏   | 523/840 [00:31<00:22, 14.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8505, loss: 0.4389 ||:  62%|██████▎   | 525/840 [00:32<00:22, 14.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8508, loss: 0.4384 ||:  63%|██████▎   | 527/840 [00:32<00:21, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8508, loss: 0.4391 ||:  63%|██████▎   | 529/840 [00:32<00:22, 13.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8509, loss: 0.4387 ||:  63%|██████▎   | 531/840 [00:32<00:21, 14.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8510, loss: 0.4385 ||:  63%|██████▎   | 533/840 [00:32<00:21, 14.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8507, loss: 0.4392 ||:  64%|██████▎   | 535/840 [00:32<00:22, 13.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8508, loss: 0.4393 ||:  64%|██████▍   | 537/840 [00:32<00:21, 13.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8509, loss: 0.4391 ||:  64%|██████▍   | 539/840 [00:33<00:22, 13.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8511, loss: 0.4388 ||:  64%|██████▍   | 541/840 [00:33<00:22, 13.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8511, loss: 0.4386 ||:  65%|██████▍   | 543/840 [00:33<00:21, 13.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8511, loss: 0.4384 ||:  65%|██████▍   | 545/840 [00:33<00:21, 13.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8513, loss: 0.4377 ||:  65%|██████▌   | 547/840 [00:33<00:20, 14.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8514, loss: 0.4373 ||:  65%|██████▌   | 549/840 [00:33<00:20, 14.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8513, loss: 0.4376 ||:  66%|██████▌   | 551/840 [00:33<00:20, 14.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8513, loss: 0.4376 ||:  66%|██████▌   | 553/840 [00:34<00:20, 14.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8513, loss: 0.4379 ||:  66%|██████▌   | 555/840 [00:34<00:19, 14.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8513, loss: 0.4375 ||:  66%|██████▋   | 557/840 [00:34<00:19, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8513, loss: 0.4373 ||:  67%|██████▋   | 559/840 [00:34<00:19, 14.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8512, loss: 0.4376 ||:  67%|██████▋   | 561/840 [00:34<00:20, 13.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8512, loss: 0.4375 ||:  67%|██████▋   | 563/840 [00:34<00:20, 13.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8509, loss: 0.4383 ||:  67%|██████▋   | 565/840 [00:34<00:19, 13.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8509, loss: 0.4379 ||:  68%|██████▊   | 567/840 [00:35<00:19, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8511, loss: 0.4380 ||:  68%|██████▊   | 569/840 [00:35<00:18, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8512, loss: 0.4375 ||:  68%|██████▊   | 571/840 [00:35<00:18, 14.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8514, loss: 0.4371 ||:  68%|██████▊   | 573/840 [00:35<00:18, 14.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8516, loss: 0.4366 ||:  68%|██████▊   | 575/840 [00:35<00:18, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8516, loss: 0.4365 ||:  69%|██████▊   | 577/840 [00:35<00:18, 14.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8517, loss: 0.4364 ||:  69%|██████▉   | 579/840 [00:35<00:17, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8518, loss: 0.4363 ||:  69%|██████▉   | 581/840 [00:35<00:17, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8517, loss: 0.4367 ||:  69%|██████▉   | 583/840 [00:36<00:17, 14.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8521, loss: 0.4360 ||:  70%|██████▉   | 585/840 [00:36<00:17, 14.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8522, loss: 0.4356 ||:  70%|██████▉   | 587/840 [00:36<00:17, 14.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8525, loss: 0.4356 ||:  70%|███████   | 589/840 [00:36<00:17, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8525, loss: 0.4353 ||:  70%|███████   | 591/840 [00:36<00:17, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8525, loss: 0.4351 ||:  71%|███████   | 593/840 [00:36<00:17, 14.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8526, loss: 0.4350 ||:  71%|███████   | 595/840 [00:36<00:16, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8529, loss: 0.4346 ||:  71%|███████   | 597/840 [00:37<00:17, 13.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8530, loss: 0.4342 ||:  71%|███████▏  | 599/840 [00:37<00:16, 14.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8532, loss: 0.4338 ||:  72%|███████▏  | 601/840 [00:37<00:17, 14.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8533, loss: 0.4335 ||:  72%|███████▏  | 603/840 [00:37<00:16, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8536, loss: 0.4330 ||:  72%|███████▏  | 605/840 [00:37<00:17, 13.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8533, loss: 0.4335 ||:  72%|███████▏  | 607/840 [00:37<00:16, 13.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8538, loss: 0.4327 ||:  72%|███████▎  | 609/840 [00:37<00:16, 13.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8539, loss: 0.4323 ||:  73%|███████▎  | 611/840 [00:38<00:15, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8542, loss: 0.4318 ||:  73%|███████▎  | 613/840 [00:38<00:15, 14.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8543, loss: 0.4316 ||:  73%|███████▎  | 615/840 [00:38<00:15, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8547, loss: 0.4309 ||:  73%|███████▎  | 617/840 [00:38<00:15, 14.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8549, loss: 0.4305 ||:  74%|███████▎  | 619/840 [00:38<00:15, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8548, loss: 0.4311 ||:  74%|███████▍  | 621/840 [00:38<00:15, 14.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8551, loss: 0.4305 ||:  74%|███████▍  | 623/840 [00:38<00:15, 14.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8552, loss: 0.4301 ||:  74%|███████▍  | 625/840 [00:39<00:15, 14.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8552, loss: 0.4301 ||:  75%|███████▍  | 627/840 [00:39<00:15, 14.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8554, loss: 0.4300 ||:  75%|███████▍  | 629/840 [00:39<00:14, 14.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8555, loss: 0.4297 ||:  75%|███████▌  | 631/840 [00:39<00:14, 14.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8555, loss: 0.4293 ||:  75%|███████▌  | 633/840 [00:39<00:14, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8556, loss: 0.4291 ||:  76%|███████▌  | 635/840 [00:39<00:15, 13.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8557, loss: 0.4291 ||:  76%|███████▌  | 637/840 [00:39<00:14, 13.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8559, loss: 0.4287 ||:  76%|███████▌  | 639/840 [00:40<00:14, 13.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8560, loss: 0.4285 ||:  76%|███████▋  | 641/840 [00:40<00:14, 14.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8562, loss: 0.4282 ||:  77%|███████▋  | 643/840 [00:40<00:14, 13.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8564, loss: 0.4278 ||:  77%|███████▋  | 645/840 [00:40<00:14, 13.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8563, loss: 0.4283 ||:  77%|███████▋  | 647/840 [00:40<00:14, 13.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8566, loss: 0.4278 ||:  77%|███████▋  | 649/840 [00:40<00:14, 13.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8568, loss: 0.4276 ||:  78%|███████▊  | 651/840 [00:40<00:13, 13.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8568, loss: 0.4274 ||:  78%|███████▊  | 653/840 [00:41<00:13, 13.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8570, loss: 0.4268 ||:  78%|███████▊  | 655/840 [00:41<00:13, 13.70it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8572, loss: 0.4261 ||:  78%|███████▊  | 657/840 [00:41<00:12, 14.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8574, loss: 0.4258 ||:  78%|███████▊  | 659/840 [00:41<00:12, 14.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8573, loss: 0.4259 ||:  79%|███████▊  | 661/840 [00:41<00:14, 12.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8576, loss: 0.4253 ||:  79%|███████▉  | 663/840 [00:41<00:13, 12.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8577, loss: 0.4250 ||:  79%|███████▉  | 665/840 [00:42<00:13, 13.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8581, loss: 0.4245 ||:  79%|███████▉  | 667/840 [00:42<00:12, 13.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8581, loss: 0.4246 ||:  80%|███████▉  | 669/840 [00:42<00:12, 13.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8583, loss: 0.4241 ||:  80%|███████▉  | 671/840 [00:42<00:12, 13.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8582, loss: 0.4239 ||:  80%|████████  | 673/840 [00:42<00:11, 13.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8585, loss: 0.4233 ||:  80%|████████  | 675/840 [00:42<00:11, 14.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8585, loss: 0.4230 ||:  81%|████████  | 677/840 [00:42<00:11, 14.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8588, loss: 0.4224 ||:  81%|████████  | 679/840 [00:43<00:11, 13.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8591, loss: 0.4218 ||:  81%|████████  | 681/840 [00:43<00:11, 13.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8591, loss: 0.4215 ||:  81%|████████▏ | 683/840 [00:43<00:11, 13.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8591, loss: 0.4214 ||:  82%|████████▏ | 685/840 [00:43<00:10, 14.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8590, loss: 0.4213 ||:  82%|████████▏ | 687/840 [00:43<00:10, 14.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8591, loss: 0.4212 ||:  82%|████████▏ | 689/840 [00:43<00:10, 14.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8590, loss: 0.4210 ||:  82%|████████▏ | 691/840 [00:43<00:10, 13.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8592, loss: 0.4203 ||:  82%|████████▎ | 693/840 [00:44<00:10, 13.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8591, loss: 0.4205 ||:  83%|████████▎ | 695/840 [00:44<00:10, 13.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8592, loss: 0.4204 ||:  83%|████████▎ | 697/840 [00:44<00:10, 14.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8592, loss: 0.4201 ||:  83%|████████▎ | 699/840 [00:44<00:09, 14.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8593, loss: 0.4203 ||:  83%|████████▎ | 701/840 [00:44<00:09, 14.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8595, loss: 0.4205 ||:  84%|████████▎ | 703/840 [00:44<00:09, 14.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8594, loss: 0.4208 ||:  84%|████████▍ | 705/840 [00:44<00:09, 14.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8595, loss: 0.4205 ||:  84%|████████▍ | 707/840 [00:45<00:09, 13.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8596, loss: 0.4203 ||:  84%|████████▍ | 709/840 [00:45<00:09, 13.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8598, loss: 0.4197 ||:  85%|████████▍ | 711/840 [00:45<00:09, 13.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8601, loss: 0.4192 ||:  85%|████████▍ | 713/840 [00:45<00:09, 13.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8601, loss: 0.4189 ||:  85%|████████▌ | 715/840 [00:45<00:09, 13.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8603, loss: 0.4184 ||:  85%|████████▌ | 717/840 [00:45<00:08, 14.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8605, loss: 0.4182 ||:  86%|████████▌ | 719/840 [00:45<00:08, 14.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8607, loss: 0.4178 ||:  86%|████████▌ | 721/840 [00:46<00:08, 14.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8607, loss: 0.4177 ||:  86%|████████▌ | 723/840 [00:46<00:08, 14.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8606, loss: 0.4181 ||:  86%|████████▋ | 725/840 [00:46<00:07, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8606, loss: 0.4178 ||:  87%|████████▋ | 727/840 [00:46<00:07, 14.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8607, loss: 0.4173 ||:  87%|████████▋ | 729/840 [00:46<00:07, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8607, loss: 0.4171 ||:  87%|████████▋ | 731/840 [00:46<00:07, 14.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8609, loss: 0.4165 ||:  87%|████████▋ | 733/840 [00:46<00:07, 14.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8612, loss: 0.4160 ||:  88%|████████▊ | 735/840 [00:46<00:07, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8611, loss: 0.4162 ||:  88%|████████▊ | 737/840 [00:47<00:07, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8614, loss: 0.4160 ||:  88%|████████▊ | 739/840 [00:47<00:07, 14.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8611, loss: 0.4165 ||:  88%|████████▊ | 741/840 [00:47<00:06, 14.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8613, loss: 0.4163 ||:  88%|████████▊ | 743/840 [00:47<00:06, 14.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8615, loss: 0.4156 ||:  89%|████████▊ | 745/840 [00:47<00:06, 14.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8616, loss: 0.4155 ||:  89%|████████▉ | 747/840 [00:47<00:06, 14.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8617, loss: 0.4150 ||:  89%|████████▉ | 749/840 [00:47<00:06, 13.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8617, loss: 0.4148 ||:  89%|████████▉ | 751/840 [00:48<00:06, 13.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8620, loss: 0.4143 ||:  90%|████████▉ | 753/840 [00:48<00:06, 13.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8620, loss: 0.4140 ||:  90%|████████▉ | 755/840 [00:48<00:05, 14.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8620, loss: 0.4141 ||:  90%|█████████ | 757/840 [00:48<00:05, 13.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8623, loss: 0.4135 ||:  90%|█████████ | 759/840 [00:48<00:05, 13.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8622, loss: 0.4136 ||:  91%|█████████ | 761/840 [00:48<00:06, 12.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8620, loss: 0.4138 ||:  91%|█████████ | 763/840 [00:49<00:05, 12.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8617, loss: 0.4141 ||:  91%|█████████ | 765/840 [00:49<00:05, 13.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8618, loss: 0.4139 ||:  91%|█████████▏| 767/840 [00:49<00:05, 13.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8617, loss: 0.4140 ||:  92%|█████████▏| 769/840 [00:49<00:05, 13.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8620, loss: 0.4135 ||:  92%|█████████▏| 771/840 [00:49<00:04, 14.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8618, loss: 0.4133 ||:  92%|█████████▏| 773/840 [00:49<00:04, 14.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8621, loss: 0.4129 ||:  92%|█████████▏| 775/840 [00:49<00:04, 14.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8624, loss: 0.4123 ||:  92%|█████████▎| 777/840 [00:49<00:04, 14.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8626, loss: 0.4121 ||:  93%|█████████▎| 779/840 [00:50<00:04, 14.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8626, loss: 0.4118 ||:  93%|█████████▎| 781/840 [00:50<00:04, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8627, loss: 0.4118 ||:  93%|█████████▎| 783/840 [00:50<00:03, 14.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8626, loss: 0.4117 ||:  93%|█████████▎| 785/840 [00:50<00:03, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8628, loss: 0.4114 ||:  94%|█████████▎| 787/840 [00:50<00:03, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8628, loss: 0.4117 ||:  94%|█████████▍| 789/840 [00:50<00:03, 14.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8628, loss: 0.4115 ||:  94%|█████████▍| 791/840 [00:50<00:03, 13.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8629, loss: 0.4115 ||:  94%|█████████▍| 793/840 [00:51<00:03, 14.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8626, loss: 0.4121 ||:  95%|█████████▍| 795/840 [00:51<00:03, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8627, loss: 0.4121 ||:  95%|█████████▍| 797/840 [00:51<00:03, 14.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8629, loss: 0.4117 ||:  95%|█████████▌| 799/840 [00:51<00:03, 13.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8629, loss: 0.4120 ||:  95%|█████████▌| 801/840 [00:51<00:02, 13.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8630, loss: 0.4122 ||:  96%|█████████▌| 803/840 [00:51<00:02, 13.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8632, loss: 0.4115 ||:  96%|█████████▌| 805/840 [00:52<00:02, 13.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8630, loss: 0.4118 ||:  96%|█████████▌| 807/840 [00:52<00:02, 13.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8632, loss: 0.4115 ||:  96%|█████████▋| 809/840 [00:52<00:02, 13.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8632, loss: 0.4114 ||:  97%|█████████▋| 811/840 [00:52<00:02, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8633, loss: 0.4109 ||:  97%|█████████▋| 813/840 [00:52<00:01, 14.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8634, loss: 0.4106 ||:  97%|█████████▋| 815/840 [00:52<00:01, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8634, loss: 0.4107 ||:  97%|█████████▋| 817/840 [00:52<00:01, 14.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8634, loss: 0.4107 ||:  98%|█████████▊| 819/840 [00:52<00:01, 14.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8633, loss: 0.4111 ||:  98%|█████████▊| 821/840 [00:53<00:01, 13.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8634, loss: 0.4108 ||:  98%|█████████▊| 823/840 [00:53<00:01, 14.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8634, loss: 0.4106 ||:  98%|█████████▊| 825/840 [00:53<00:01, 14.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8633, loss: 0.4109 ||:  98%|█████████▊| 827/840 [00:53<00:00, 14.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8635, loss: 0.4106 ||:  99%|█████████▊| 829/840 [00:53<00:00, 14.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8637, loss: 0.4102 ||:  99%|█████████▉| 831/840 [00:53<00:00, 14.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8636, loss: 0.4103 ||:  99%|█████████▉| 833/840 [00:53<00:00, 13.97it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8635, loss: 0.4104 ||:  99%|█████████▉| 835/840 [00:54<00:00, 14.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8636, loss: 0.4100 ||: 100%|█████████▉| 837/840 [00:54<00:00, 14.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8638, loss: 0.4095 ||: 100%|█████████▉| 839/840 [00:54<00:00, 14.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8639, loss: 0.4093 ||: 100%|██████████| 840/840 [00:54<00:00, 15.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/360 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8531, loss: 0.4290 ||:   4%|▍         | 16/360 [00:00<00:02, 153.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8629, loss: 0.4294 ||:  10%|▉         | 35/360 [00:00<00:02, 161.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8691, loss: 0.4214 ||:  15%|█▌        | 55/360 [00:00<00:01, 169.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8687, loss: 0.4217 ||:  21%|██        | 75/360 [00:00<00:01, 177.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8739, loss: 0.4118 ||:  26%|██▌       | 94/360 [00:00<00:01, 181.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8697, loss: 0.4174 ||:  32%|███▏      | 114/360 [00:00<00:01, 183.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8704, loss: 0.4149 ||:  38%|███▊      | 135/360 [00:00<00:01, 188.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8714, loss: 0.4104 ||:  43%|████▎     | 154/360 [00:00<00:01, 188.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8711, loss: 0.4139 ||:  48%|████▊     | 173/360 [00:00<00:00, 187.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8721, loss: 0.4134 ||:  53%|█████▎    | 192/360 [00:01<00:00, 173.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8723, loss: 0.4139 ||:  59%|█████▊    | 211/360 [00:01<00:00, 175.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8734, loss: 0.4106 ||:  64%|██████▍   | 231/360 [00:01<00:00, 181.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8736, loss: 0.4085 ||:  69%|██████▉   | 250/360 [00:01<00:00, 183.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8732, loss: 0.4130 ||:  75%|███████▍  | 269/360 [00:01<00:00, 184.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8745, loss: 0.4095 ||:  80%|████████  | 288/360 [00:01<00:00, 183.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8718, loss: 0.4101 ||:  86%|████████▌ | 309/360 [00:01<00:00, 187.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8730, loss: 0.4062 ||:  92%|█████████▏| 330/360 [00:01<00:00, 193.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8727, loss: 0.4048 ||:  97%|█████████▋| 350/360 [00:01<00:00, 192.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8732, loss: 0.4038 ||: 100%|██████████| 360/360 [00:01<00:00, 185.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/840 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.2764 ||:   0%|          | 1/840 [00:00<01:31,  9.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.2196 ||:   0%|          | 2/840 [00:00<01:29,  9.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9250, loss: 0.2405 ||:   0%|          | 4/840 [00:00<01:23, 10.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9250, loss: 0.2339 ||:   1%|          | 6/840 [00:00<01:16, 10.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9375, loss: 0.2235 ||:   1%|          | 8/840 [00:00<01:10, 11.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9400, loss: 0.2125 ||:   1%|          | 10/840 [00:00<01:07, 12.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9417, loss: 0.2097 ||:   1%|▏         | 12/840 [00:00<01:06, 12.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9429, loss: 0.2095 ||:   2%|▏         | 14/840 [00:01<01:04, 12.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9437, loss: 0.2075 ||:   2%|▏         | 16/840 [00:01<01:03, 12.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9417, loss: 0.2148 ||:   2%|▏         | 18/840 [00:01<01:02, 13.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9400, loss: 0.2187 ||:   2%|▏         | 20/840 [00:01<01:07, 12.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9386, loss: 0.2205 ||:   3%|▎         | 22/840 [00:01<01:04, 12.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9417, loss: 0.2145 ||:   3%|▎         | 24/840 [00:01<01:04, 12.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9365, loss: 0.2295 ||:   3%|▎         | 26/840 [00:02<01:01, 13.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9357, loss: 0.2331 ||:   3%|▎         | 28/840 [00:02<00:59, 13.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9350, loss: 0.2420 ||:   4%|▎         | 30/840 [00:02<00:57, 14.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9344, loss: 0.2400 ||:   4%|▍         | 32/840 [00:02<00:58, 13.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9353, loss: 0.2463 ||:   4%|▍         | 34/840 [00:02<00:58, 13.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9361, loss: 0.2406 ||:   4%|▍         | 36/840 [00:02<00:58, 13.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9368, loss: 0.2365 ||:   5%|▍         | 38/840 [00:02<00:57, 13.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9350, loss: 0.2412 ||:   5%|▍         | 40/840 [00:03<00:58, 13.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9381, loss: 0.2377 ||:   5%|▌         | 42/840 [00:03<00:58, 13.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9375, loss: 0.2389 ||:   5%|▌         | 44/840 [00:03<00:57, 13.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9391, loss: 0.2339 ||:   5%|▌         | 46/840 [00:03<00:56, 13.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9406, loss: 0.2309 ||:   6%|▌         | 48/840 [00:03<00:56, 14.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9420, loss: 0.2255 ||:   6%|▌         | 50/840 [00:03<00:57, 13.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9404, loss: 0.2273 ||:   6%|▌         | 52/840 [00:03<00:57, 13.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9389, loss: 0.2262 ||:   6%|▋         | 54/840 [00:04<00:58, 13.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9402, loss: 0.2230 ||:   7%|▋         | 56/840 [00:04<00:58, 13.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9397, loss: 0.2275 ||:   7%|▋         | 58/840 [00:04<00:58, 13.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9400, loss: 0.2268 ||:   7%|▋         | 60/840 [00:04<00:59, 13.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9387, loss: 0.2292 ||:   7%|▋         | 62/840 [00:04<00:58, 13.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9406, loss: 0.2245 ||:   8%|▊         | 64/840 [00:04<00:58, 13.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9379, loss: 0.2328 ||:   8%|▊         | 66/840 [00:04<00:57, 13.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9382, loss: 0.2298 ||:   8%|▊         | 68/840 [00:05<00:57, 13.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9393, loss: 0.2269 ||:   8%|▊         | 70/840 [00:05<00:56, 13.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9368, loss: 0.2349 ||:   9%|▊         | 72/840 [00:05<00:55, 13.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9385, loss: 0.2302 ||:   9%|▉         | 74/840 [00:05<00:56, 13.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9395, loss: 0.2270 ||:   9%|▉         | 76/840 [00:05<00:55, 13.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9378, loss: 0.2304 ||:   9%|▉         | 78/840 [00:05<00:54, 13.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9356, loss: 0.2354 ||:  10%|▉         | 80/840 [00:05<00:54, 13.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9329, loss: 0.2408 ||:  10%|▉         | 82/840 [00:06<00:55, 13.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9339, loss: 0.2385 ||:  10%|█         | 84/840 [00:06<00:54, 13.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9314, loss: 0.2421 ||:  10%|█         | 86/840 [00:06<00:53, 14.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9313, loss: 0.2403 ||:  10%|█         | 88/840 [00:06<00:54, 13.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9294, loss: 0.2437 ||:  11%|█         | 90/840 [00:06<00:58, 12.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9293, loss: 0.2445 ||:  11%|█         | 92/840 [00:06<00:56, 13.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9293, loss: 0.2430 ||:  11%|█         | 94/840 [00:07<00:55, 13.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9297, loss: 0.2439 ||:  11%|█▏        | 96/840 [00:07<00:54, 13.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9311, loss: 0.2410 ||:  12%|█▏        | 98/840 [00:07<00:52, 14.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9305, loss: 0.2426 ||:  12%|█▏        | 100/840 [00:07<00:52, 14.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9304, loss: 0.2435 ||:  12%|█▏        | 102/840 [00:07<00:51, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9303, loss: 0.2426 ||:  12%|█▏        | 104/840 [00:07<00:50, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9302, loss: 0.2429 ||:  13%|█▎        | 106/840 [00:07<00:51, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9292, loss: 0.2456 ||:  13%|█▎        | 108/840 [00:07<00:52, 13.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9286, loss: 0.2476 ||:  13%|█▎        | 110/840 [00:08<00:55, 13.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9286, loss: 0.2470 ||:  13%|█▎        | 112/840 [00:08<00:55, 13.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9294, loss: 0.2445 ||:  14%|█▎        | 114/840 [00:08<00:55, 13.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9284, loss: 0.2448 ||:  14%|█▍        | 116/840 [00:08<00:54, 13.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9288, loss: 0.2434 ||:  14%|█▍        | 118/840 [00:08<00:52, 13.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2458 ||:  14%|█▍        | 120/840 [00:08<00:59, 12.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9266, loss: 0.2456 ||:  15%|█▍        | 122/840 [00:09<00:56, 12.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2445 ||:  15%|█▍        | 124/840 [00:09<00:55, 12.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2436 ||:  15%|█▌        | 126/840 [00:09<00:53, 13.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9266, loss: 0.2446 ||:  15%|█▌        | 128/840 [00:09<00:52, 13.63it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9273, loss: 0.2430 ||:  15%|█▌        | 130/840 [00:09<00:50, 13.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2418 ||:  16%|█▌        | 132/840 [00:09<00:49, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2426 ||:  16%|█▌        | 134/840 [00:09<00:52, 13.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2418 ||:  16%|█▌        | 136/840 [00:10<00:50, 13.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9283, loss: 0.2397 ||:  16%|█▋        | 138/840 [00:10<00:50, 13.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9279, loss: 0.2413 ||:  17%|█▋        | 140/840 [00:10<00:49, 14.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9282, loss: 0.2409 ||:  17%|█▋        | 142/840 [00:10<00:49, 14.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9281, loss: 0.2428 ||:  17%|█▋        | 144/840 [00:10<00:49, 14.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9288, loss: 0.2424 ||:  17%|█▋        | 146/840 [00:10<00:48, 14.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2436 ||:  18%|█▊        | 148/840 [00:10<00:49, 14.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2442 ||:  18%|█▊        | 150/840 [00:11<00:48, 14.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9283, loss: 0.2439 ||:  18%|█▊        | 152/840 [00:11<00:48, 14.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9279, loss: 0.2467 ||:  18%|█▊        | 154/840 [00:11<00:48, 14.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9282, loss: 0.2463 ||:  19%|█▊        | 156/840 [00:11<00:47, 14.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9285, loss: 0.2452 ||:  19%|█▉        | 158/840 [00:11<00:47, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9284, loss: 0.2450 ||:  19%|█▉        | 160/840 [00:11<00:46, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9284, loss: 0.2439 ||:  19%|█▉        | 162/840 [00:11<00:46, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9284, loss: 0.2425 ||:  20%|█▉        | 164/840 [00:12<00:44, 15.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9289, loss: 0.2415 ||:  20%|█▉        | 166/840 [00:12<00:45, 14.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9295, loss: 0.2404 ||:  20%|██        | 168/840 [00:12<00:45, 14.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9294, loss: 0.2406 ||:  20%|██        | 170/840 [00:12<00:45, 14.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9291, loss: 0.2401 ||:  20%|██        | 172/840 [00:12<00:45, 14.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9296, loss: 0.2389 ||:  21%|██        | 174/840 [00:12<00:45, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9301, loss: 0.2373 ||:  21%|██        | 176/840 [00:12<00:44, 14.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9303, loss: 0.2367 ||:  21%|██        | 178/840 [00:12<00:45, 14.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9300, loss: 0.2369 ||:  21%|██▏       | 180/840 [00:13<00:45, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9305, loss: 0.2353 ||:  22%|██▏       | 182/840 [00:13<00:45, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9307, loss: 0.2349 ||:  22%|██▏       | 184/840 [00:13<00:44, 14.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9315, loss: 0.2333 ||:  22%|██▏       | 186/840 [00:13<00:45, 14.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9316, loss: 0.2323 ||:  22%|██▏       | 188/840 [00:13<00:45, 14.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9324, loss: 0.2310 ||:  23%|██▎       | 190/840 [00:13<00:44, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9328, loss: 0.2298 ||:  23%|██▎       | 192/840 [00:13<00:44, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9327, loss: 0.2294 ||:  23%|██▎       | 194/840 [00:14<00:44, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9329, loss: 0.2301 ||:  23%|██▎       | 196/840 [00:14<00:43, 14.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9333, loss: 0.2288 ||:  24%|██▎       | 198/840 [00:14<00:42, 14.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9337, loss: 0.2277 ||:  24%|██▍       | 200/840 [00:14<00:42, 15.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9337, loss: 0.2267 ||:  24%|██▍       | 202/840 [00:14<00:44, 14.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9336, loss: 0.2266 ||:  24%|██▍       | 204/840 [00:14<00:44, 14.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9333, loss: 0.2280 ||:  25%|██▍       | 206/840 [00:14<00:44, 14.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9334, loss: 0.2278 ||:  25%|██▍       | 208/840 [00:15<00:44, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9331, loss: 0.2278 ||:  25%|██▌       | 210/840 [00:15<00:44, 14.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9330, loss: 0.2292 ||:  25%|██▌       | 212/840 [00:15<00:41, 14.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9327, loss: 0.2290 ||:  25%|██▌       | 214/840 [00:15<00:41, 15.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9326, loss: 0.2296 ||:  26%|██▌       | 216/840 [00:15<00:41, 15.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9328, loss: 0.2291 ||:  26%|██▌       | 218/840 [00:15<00:40, 15.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9325, loss: 0.2318 ||:  26%|██▌       | 220/840 [00:15<00:46, 13.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9315, loss: 0.2343 ||:  26%|██▋       | 222/840 [00:16<00:44, 13.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9319, loss: 0.2335 ||:  27%|██▋       | 224/840 [00:16<00:42, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9319, loss: 0.2336 ||:  27%|██▋       | 226/840 [00:16<00:42, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9316, loss: 0.2346 ||:  27%|██▋       | 228/840 [00:16<00:40, 14.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9311, loss: 0.2365 ||:  27%|██▋       | 230/840 [00:16<00:41, 14.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9306, loss: 0.2374 ||:  28%|██▊       | 232/840 [00:16<00:41, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9303, loss: 0.2369 ||:  28%|██▊       | 234/840 [00:16<00:41, 14.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9297, loss: 0.2378 ||:  28%|██▊       | 236/840 [00:16<00:40, 14.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9296, loss: 0.2383 ||:  28%|██▊       | 238/840 [00:17<00:39, 15.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9292, loss: 0.2387 ||:  29%|██▊       | 240/840 [00:17<00:39, 15.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9295, loss: 0.2376 ||:  29%|██▉       | 242/840 [00:17<00:38, 15.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9295, loss: 0.2372 ||:  29%|██▉       | 244/840 [00:17<00:38, 15.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9295, loss: 0.2375 ||:  29%|██▉       | 246/840 [00:17<00:42, 14.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9294, loss: 0.2373 ||:  30%|██▉       | 248/840 [00:17<00:41, 14.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9290, loss: 0.2375 ||:  30%|██▉       | 250/840 [00:17<00:40, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9292, loss: 0.2370 ||:  30%|███       | 252/840 [00:18<00:40, 14.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9297, loss: 0.2361 ||:  30%|███       | 254/840 [00:18<00:40, 14.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9291, loss: 0.2380 ||:  30%|███       | 256/840 [00:18<00:40, 14.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9293, loss: 0.2378 ||:  31%|███       | 258/840 [00:18<00:39, 14.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9296, loss: 0.2369 ||:  31%|███       | 260/840 [00:18<00:39, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9292, loss: 0.2380 ||:  31%|███       | 262/840 [00:18<00:40, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9292, loss: 0.2379 ||:  31%|███▏      | 264/840 [00:18<00:39, 14.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9295, loss: 0.2373 ||:  32%|███▏      | 266/840 [00:19<00:38, 14.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9300, loss: 0.2365 ||:  32%|███▏      | 268/840 [00:19<00:38, 14.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9302, loss: 0.2362 ||:  32%|███▏      | 270/840 [00:19<00:37, 15.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9305, loss: 0.2354 ||:  32%|███▏      | 272/840 [00:19<00:37, 15.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9310, loss: 0.2342 ||:  33%|███▎      | 274/840 [00:19<00:37, 14.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9308, loss: 0.2342 ||:  33%|███▎      | 276/840 [00:19<00:37, 15.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9302, loss: 0.2357 ||:  33%|███▎      | 278/840 [00:19<00:36, 15.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9305, loss: 0.2359 ||:  33%|███▎      | 280/840 [00:19<00:36, 15.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9310, loss: 0.2348 ||:  34%|███▎      | 282/840 [00:20<00:36, 15.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9306, loss: 0.2352 ||:  34%|███▍      | 284/840 [00:20<00:38, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9311, loss: 0.2344 ||:  34%|███▍      | 286/840 [00:20<00:37, 14.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9307, loss: 0.2346 ||:  34%|███▍      | 288/840 [00:20<00:37, 14.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9307, loss: 0.2349 ||:  35%|███▍      | 290/840 [00:20<00:36, 14.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9308, loss: 0.2346 ||:  35%|███▍      | 292/840 [00:20<00:36, 14.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9311, loss: 0.2343 ||:  35%|███▌      | 294/840 [00:20<00:36, 15.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9311, loss: 0.2347 ||:  35%|███▌      | 296/840 [00:20<00:36, 15.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9315, loss: 0.2335 ||:  35%|███▌      | 298/840 [00:21<00:36, 15.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9318, loss: 0.2329 ||:  36%|███▌      | 300/840 [00:21<00:36, 14.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9321, loss: 0.2325 ||:  36%|███▌      | 302/840 [00:21<00:36, 14.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9322, loss: 0.2323 ||:  36%|███▌      | 304/840 [00:21<00:36, 14.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9325, loss: 0.2315 ||:  36%|███▋      | 306/840 [00:21<00:37, 14.35it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9325, loss: 0.2324 ||:  37%|███▋      | 308/840 [00:21<00:37, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9323, loss: 0.2331 ||:  37%|███▋      | 310/840 [00:21<00:37, 14.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9321, loss: 0.2328 ||:  37%|███▋      | 312/840 [00:22<00:37, 14.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9322, loss: 0.2325 ||:  37%|███▋      | 314/840 [00:22<00:37, 14.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9326, loss: 0.2313 ||:  38%|███▊      | 316/840 [00:22<00:37, 13.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9327, loss: 0.2308 ||:  38%|███▊      | 318/840 [00:22<00:37, 13.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9330, loss: 0.2303 ||:  38%|███▊      | 320/840 [00:22<00:40, 12.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9328, loss: 0.2306 ||:  38%|███▊      | 322/840 [00:22<00:38, 13.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9326, loss: 0.2311 ||:  39%|███▊      | 324/840 [00:23<00:36, 14.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9327, loss: 0.2304 ||:  39%|███▉      | 326/840 [00:23<00:36, 14.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9328, loss: 0.2301 ||:  39%|███▉      | 328/840 [00:23<00:35, 14.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9329, loss: 0.2298 ||:  39%|███▉      | 330/840 [00:23<00:35, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9327, loss: 0.2305 ||:  40%|███▉      | 332/840 [00:23<00:35, 14.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9325, loss: 0.2302 ||:  40%|███▉      | 334/840 [00:23<00:36, 14.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9327, loss: 0.2297 ||:  40%|████      | 336/840 [00:23<00:36, 13.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9327, loss: 0.2297 ||:  40%|████      | 338/840 [00:23<00:35, 14.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9326, loss: 0.2305 ||:  40%|████      | 340/840 [00:24<00:36, 13.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9325, loss: 0.2311 ||:  41%|████      | 342/840 [00:24<00:34, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9321, loss: 0.2323 ||:  41%|████      | 344/840 [00:24<00:34, 14.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9316, loss: 0.2334 ||:  41%|████      | 346/840 [00:24<00:33, 14.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9319, loss: 0.2329 ||:  41%|████▏     | 348/840 [00:24<00:33, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9316, loss: 0.2343 ||:  42%|████▏     | 350/840 [00:24<00:32, 14.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9314, loss: 0.2359 ||:  42%|████▏     | 352/840 [00:24<00:33, 14.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9314, loss: 0.2355 ||:  42%|████▏     | 354/840 [00:25<00:33, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9315, loss: 0.2355 ||:  42%|████▏     | 356/840 [00:25<00:33, 14.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9309, loss: 0.2371 ||:  43%|████▎     | 358/840 [00:25<00:33, 14.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9307, loss: 0.2377 ||:  43%|████▎     | 360/840 [00:25<00:33, 14.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9307, loss: 0.2377 ||:  43%|████▎     | 362/840 [00:25<00:33, 14.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9308, loss: 0.2375 ||:  43%|████▎     | 364/840 [00:25<00:33, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9305, loss: 0.2384 ||:  44%|████▎     | 366/840 [00:25<00:32, 14.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9300, loss: 0.2394 ||:  44%|████▍     | 368/840 [00:26<00:32, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9303, loss: 0.2390 ||:  44%|████▍     | 370/840 [00:26<00:30, 15.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9302, loss: 0.2388 ||:  44%|████▍     | 372/840 [00:26<00:30, 15.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9303, loss: 0.2387 ||:  45%|████▍     | 374/840 [00:26<00:31, 14.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9298, loss: 0.2403 ||:  45%|████▍     | 376/840 [00:26<00:30, 15.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9299, loss: 0.2397 ||:  45%|████▌     | 378/840 [00:26<00:30, 14.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9300, loss: 0.2395 ||:  45%|████▌     | 380/840 [00:26<00:30, 15.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9301, loss: 0.2390 ||:  45%|████▌     | 382/840 [00:26<00:29, 15.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9302, loss: 0.2384 ||:  46%|████▌     | 384/840 [00:27<00:29, 15.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9299, loss: 0.2390 ||:  46%|████▌     | 386/840 [00:27<00:29, 15.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9299, loss: 0.2393 ||:  46%|████▌     | 388/840 [00:27<00:29, 15.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9295, loss: 0.2409 ||:  46%|████▋     | 390/840 [00:27<00:29, 15.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9295, loss: 0.2405 ||:  47%|████▋     | 392/840 [00:27<00:30, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9294, loss: 0.2406 ||:  47%|████▋     | 394/840 [00:27<00:29, 15.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9295, loss: 0.2401 ||:  47%|████▋     | 396/840 [00:27<00:29, 15.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9293, loss: 0.2409 ||:  47%|████▋     | 398/840 [00:28<00:29, 15.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9293, loss: 0.2409 ||:  48%|████▊     | 400/840 [00:28<00:28, 15.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9291, loss: 0.2408 ||:  48%|████▊     | 402/840 [00:28<00:28, 15.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9291, loss: 0.2407 ||:  48%|████▊     | 404/840 [00:28<00:29, 14.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9291, loss: 0.2405 ||:  48%|████▊     | 406/840 [00:28<00:29, 14.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9290, loss: 0.2401 ||:  49%|████▊     | 408/840 [00:28<00:29, 14.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9290, loss: 0.2403 ||:  49%|████▉     | 410/840 [00:28<00:30, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9288, loss: 0.2416 ||:  49%|████▉     | 412/840 [00:28<00:30, 14.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9287, loss: 0.2419 ||:  49%|████▉     | 414/840 [00:29<00:28, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9288, loss: 0.2415 ||:  50%|████▉     | 416/840 [00:29<00:29, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9288, loss: 0.2414 ||:  50%|████▉     | 418/840 [00:29<00:28, 14.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9286, loss: 0.2419 ||:  50%|█████     | 420/840 [00:29<00:32, 12.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9284, loss: 0.2419 ||:  50%|█████     | 422/840 [00:29<00:31, 13.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9283, loss: 0.2423 ||:  50%|█████     | 424/840 [00:29<00:30, 13.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9284, loss: 0.2423 ||:  51%|█████     | 426/840 [00:30<00:29, 14.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9284, loss: 0.2423 ||:  51%|█████     | 428/840 [00:30<00:29, 13.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9286, loss: 0.2419 ||:  51%|█████     | 430/840 [00:30<00:29, 14.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9288, loss: 0.2415 ||:  51%|█████▏    | 432/840 [00:30<00:28, 14.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9287, loss: 0.2415 ||:  52%|█████▏    | 434/840 [00:30<00:27, 14.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9284, loss: 0.2419 ||:  52%|█████▏    | 436/840 [00:30<00:27, 14.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9283, loss: 0.2421 ||:  52%|█████▏    | 438/840 [00:30<00:27, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2430 ||:  52%|█████▏    | 440/840 [00:30<00:26, 15.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2435 ||:  53%|█████▎    | 442/840 [00:31<00:26, 15.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2436 ||:  53%|█████▎    | 444/840 [00:31<00:26, 15.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2432 ||:  53%|█████▎    | 446/840 [00:31<00:25, 15.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9282, loss: 0.2429 ||:  53%|█████▎    | 448/840 [00:31<00:25, 15.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2432 ||:  54%|█████▎    | 450/840 [00:31<00:25, 15.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2437 ||:  54%|█████▍    | 452/840 [00:31<00:25, 15.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2433 ||:  54%|█████▍    | 454/840 [00:31<00:25, 15.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9279, loss: 0.2429 ||:  54%|█████▍    | 456/840 [00:32<00:25, 15.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2433 ||:  55%|█████▍    | 458/840 [00:32<00:24, 15.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2433 ||:  55%|█████▍    | 460/840 [00:32<00:25, 14.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2432 ||:  55%|█████▌    | 462/840 [00:32<00:25, 14.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2426 ||:  55%|█████▌    | 464/840 [00:32<00:25, 14.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2429 ||:  55%|█████▌    | 466/840 [00:32<00:26, 14.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2428 ||:  56%|█████▌    | 468/840 [00:32<00:26, 13.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2430 ||:  56%|█████▌    | 470/840 [00:32<00:26, 14.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2426 ||:  56%|█████▌    | 472/840 [00:33<00:25, 14.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2422 ||:  56%|█████▋    | 474/840 [00:33<00:25, 14.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2422 ||:  57%|█████▋    | 476/840 [00:33<00:25, 14.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9279, loss: 0.2415 ||:  57%|█████▋    | 478/840 [00:33<00:25, 14.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2414 ||:  57%|█████▋    | 480/840 [00:33<00:24, 14.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2414 ||:  57%|█████▋    | 482/840 [00:33<00:23, 15.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2413 ||:  58%|█████▊    | 484/840 [00:33<00:23, 15.34it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9277, loss: 0.2423 ||:  58%|█████▊    | 486/840 [00:34<00:23, 14.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2425 ||:  58%|█████▊    | 488/840 [00:34<00:24, 14.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2422 ||:  58%|█████▊    | 490/840 [00:34<00:24, 14.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2415 ||:  59%|█████▊    | 492/840 [00:34<00:23, 14.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2412 ||:  59%|█████▉    | 494/840 [00:34<00:23, 14.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2416 ||:  59%|█████▉    | 496/840 [00:34<00:23, 14.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2421 ||:  59%|█████▉    | 498/840 [00:34<00:23, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2414 ||:  60%|█████▉    | 500/840 [00:35<00:24, 13.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2410 ||:  60%|█████▉    | 502/840 [00:35<00:23, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2412 ||:  60%|██████    | 504/840 [00:35<00:23, 14.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2412 ||:  60%|██████    | 506/840 [00:35<00:23, 14.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2414 ||:  60%|██████    | 508/840 [00:35<00:23, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2409 ||:  61%|██████    | 510/840 [00:35<00:22, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2411 ||:  61%|██████    | 512/840 [00:35<00:22, 14.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2410 ||:  61%|██████    | 514/840 [00:35<00:21, 14.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2412 ||:  61%|██████▏   | 516/840 [00:36<00:22, 14.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2407 ||:  62%|██████▏   | 518/840 [00:36<00:21, 14.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2409 ||:  62%|██████▏   | 520/840 [00:36<00:24, 13.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2409 ||:  62%|██████▏   | 522/840 [00:36<00:22, 14.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2405 ||:  62%|██████▏   | 524/840 [00:36<00:21, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9279, loss: 0.2404 ||:  63%|██████▎   | 526/840 [00:36<00:21, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2413 ||:  63%|██████▎   | 528/840 [00:36<00:21, 14.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2413 ||:  63%|██████▎   | 530/840 [00:37<00:20, 14.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2412 ||:  63%|██████▎   | 532/840 [00:37<00:20, 15.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2416 ||:  64%|██████▎   | 534/840 [00:37<00:20, 14.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2413 ||:  64%|██████▍   | 536/840 [00:37<00:20, 14.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2422 ||:  64%|██████▍   | 538/840 [00:37<00:20, 14.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2420 ||:  64%|██████▍   | 540/840 [00:37<00:19, 15.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2422 ||:  65%|██████▍   | 542/840 [00:37<00:20, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2426 ||:  65%|██████▍   | 544/840 [00:38<00:20, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9266, loss: 0.2433 ||:  65%|██████▌   | 546/840 [00:38<00:20, 14.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2431 ||:  65%|██████▌   | 548/840 [00:38<00:20, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2428 ||:  65%|██████▌   | 550/840 [00:38<00:20, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2421 ||:  66%|██████▌   | 552/840 [00:38<00:20, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2418 ||:  66%|██████▌   | 554/840 [00:38<00:20, 14.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2419 ||:  66%|██████▌   | 556/840 [00:38<00:20, 13.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2420 ||:  66%|██████▋   | 558/840 [00:39<00:19, 14.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2420 ||:  67%|██████▋   | 560/840 [00:39<00:19, 14.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2419 ||:  67%|██████▋   | 562/840 [00:39<00:19, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2417 ||:  67%|██████▋   | 564/840 [00:39<00:18, 14.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2415 ||:  67%|██████▋   | 566/840 [00:39<00:18, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2411 ||:  68%|██████▊   | 568/840 [00:39<00:18, 14.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2412 ||:  68%|██████▊   | 570/840 [00:39<00:19, 14.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2408 ||:  68%|██████▊   | 572/840 [00:40<00:19, 14.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2403 ||:  68%|██████▊   | 574/840 [00:40<00:18, 14.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2405 ||:  69%|██████▊   | 576/840 [00:40<00:17, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9279, loss: 0.2408 ||:  69%|██████▉   | 578/840 [00:40<00:17, 14.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9281, loss: 0.2403 ||:  69%|██████▉   | 580/840 [00:40<00:17, 14.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9283, loss: 0.2399 ||:  69%|██████▉   | 582/840 [00:40<00:17, 15.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9282, loss: 0.2397 ||:  70%|██████▉   | 584/840 [00:40<00:16, 15.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9282, loss: 0.2398 ||:  70%|██████▉   | 586/840 [00:40<00:16, 14.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9281, loss: 0.2396 ||:  70%|███████   | 588/840 [00:41<00:16, 15.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2401 ||:  70%|███████   | 590/840 [00:41<00:15, 15.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9281, loss: 0.2398 ||:  70%|███████   | 592/840 [00:41<00:15, 16.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9282, loss: 0.2396 ||:  71%|███████   | 594/840 [00:41<00:15, 16.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9283, loss: 0.2396 ||:  71%|███████   | 596/840 [00:41<00:15, 16.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9283, loss: 0.2399 ||:  71%|███████   | 598/840 [00:41<00:15, 15.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9284, loss: 0.2403 ||:  71%|███████▏  | 600/840 [00:41<00:15, 15.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9282, loss: 0.2405 ||:  72%|███████▏  | 602/840 [00:41<00:15, 15.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2406 ||:  72%|███████▏  | 604/840 [00:42<00:15, 15.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9281, loss: 0.2405 ||:  72%|███████▏  | 606/840 [00:42<00:15, 14.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9280, loss: 0.2406 ||:  72%|███████▏  | 608/840 [00:42<00:15, 14.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2410 ||:  73%|███████▎  | 610/840 [00:42<00:15, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9279, loss: 0.2407 ||:  73%|███████▎  | 612/840 [00:42<00:15, 14.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2416 ||:  73%|███████▎  | 614/840 [00:42<00:15, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2414 ||:  73%|███████▎  | 616/840 [00:42<00:15, 14.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2413 ||:  74%|███████▎  | 618/840 [00:43<00:15, 14.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2416 ||:  74%|███████▍  | 620/840 [00:43<00:16, 13.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2414 ||:  74%|███████▍  | 622/840 [00:43<00:15, 13.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2415 ||:  74%|███████▍  | 624/840 [00:43<00:15, 14.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2411 ||:  75%|███████▍  | 626/840 [00:43<00:15, 13.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2410 ||:  75%|███████▍  | 628/840 [00:43<00:14, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2405 ||:  75%|███████▌  | 630/840 [00:43<00:14, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2403 ||:  75%|███████▌  | 632/840 [00:44<00:14, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2403 ||:  75%|███████▌  | 634/840 [00:44<00:14, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2399 ||:  76%|███████▌  | 636/840 [00:44<00:13, 14.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2398 ||:  76%|███████▌  | 638/840 [00:44<00:13, 15.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9278, loss: 0.2403 ||:  76%|███████▌  | 640/840 [00:44<00:13, 15.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2410 ||:  76%|███████▋  | 642/840 [00:44<00:13, 15.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2410 ||:  77%|███████▋  | 644/840 [00:44<00:12, 15.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2410 ||:  77%|███████▋  | 646/840 [00:44<00:12, 14.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2407 ||:  77%|███████▋  | 648/840 [00:45<00:12, 15.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2411 ||:  77%|███████▋  | 650/840 [00:45<00:12, 15.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9277, loss: 0.2412 ||:  78%|███████▊  | 652/840 [00:45<00:12, 15.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9276, loss: 0.2414 ||:  78%|███████▊  | 654/840 [00:45<00:12, 15.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2414 ||:  78%|███████▊  | 656/840 [00:45<00:12, 15.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2413 ||:  78%|███████▊  | 658/840 [00:45<00:11, 15.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2416 ||:  79%|███████▊  | 660/840 [00:45<00:11, 15.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2415 ||:  79%|███████▉  | 662/840 [00:46<00:11, 15.20it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9273, loss: 0.2415 ||:  79%|███████▉  | 664/840 [00:46<00:11, 14.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2414 ||:  79%|███████▉  | 666/840 [00:46<00:11, 14.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2420 ||:  80%|███████▉  | 668/840 [00:46<00:11, 14.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2419 ||:  80%|███████▉  | 670/840 [00:46<00:11, 15.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2421 ||:  80%|████████  | 672/840 [00:46<00:11, 15.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2416 ||:  80%|████████  | 674/840 [00:46<00:10, 15.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2416 ||:  80%|████████  | 676/840 [00:46<00:10, 15.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2414 ||:  81%|████████  | 678/840 [00:47<00:10, 15.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2413 ||:  81%|████████  | 680/840 [00:47<00:11, 14.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2412 ||:  81%|████████  | 682/840 [00:47<00:11, 14.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2413 ||:  81%|████████▏ | 684/840 [00:47<00:10, 14.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2410 ||:  82%|████████▏ | 686/840 [00:47<00:10, 14.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2410 ||:  82%|████████▏ | 688/840 [00:47<00:10, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2414 ||:  82%|████████▏ | 690/840 [00:47<00:10, 14.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2411 ||:  82%|████████▏ | 692/840 [00:48<00:09, 14.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2408 ||:  83%|████████▎ | 694/840 [00:48<00:09, 14.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2408 ||:  83%|████████▎ | 696/840 [00:48<00:09, 14.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2409 ||:  83%|████████▎ | 698/840 [00:48<00:10, 14.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2418 ||:  83%|████████▎ | 700/840 [00:48<00:09, 14.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2417 ||:  84%|████████▎ | 702/840 [00:48<00:09, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2420 ||:  84%|████████▍ | 704/840 [00:48<00:09, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9269, loss: 0.2423 ||:  84%|████████▍ | 706/840 [00:49<00:09, 14.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2427 ||:  84%|████████▍ | 708/840 [00:49<00:09, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2428 ||:  85%|████████▍ | 710/840 [00:49<00:09, 14.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2428 ||:  85%|████████▍ | 712/840 [00:49<00:08, 14.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9266, loss: 0.2434 ||:  85%|████████▌ | 714/840 [00:49<00:08, 14.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9267, loss: 0.2429 ||:  85%|████████▌ | 716/840 [00:49<00:08, 14.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9266, loss: 0.2432 ||:  85%|████████▌ | 718/840 [00:49<00:08, 14.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2428 ||:  86%|████████▌ | 720/840 [00:50<00:09, 12.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2431 ||:  86%|████████▌ | 722/840 [00:50<00:09, 13.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2432 ||:  86%|████████▌ | 724/840 [00:50<00:08, 13.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9269, loss: 0.2429 ||:  86%|████████▋ | 726/840 [00:50<00:08, 13.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2425 ||:  87%|████████▋ | 728/840 [00:50<00:08, 13.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9269, loss: 0.2425 ||:  87%|████████▋ | 730/840 [00:50<00:07, 13.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9269, loss: 0.2423 ||:  87%|████████▋ | 732/840 [00:50<00:07, 14.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2421 ||:  87%|████████▋ | 734/840 [00:51<00:07, 13.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2418 ||:  88%|████████▊ | 736/840 [00:51<00:07, 14.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2415 ||:  88%|████████▊ | 738/840 [00:51<00:07, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2413 ||:  88%|████████▊ | 740/840 [00:51<00:06, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2410 ||:  88%|████████▊ | 742/840 [00:51<00:06, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2410 ||:  89%|████████▊ | 744/840 [00:51<00:06, 14.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2408 ||:  89%|████████▉ | 746/840 [00:51<00:06, 14.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2405 ||:  89%|████████▉ | 748/840 [00:52<00:06, 14.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2404 ||:  89%|████████▉ | 750/840 [00:52<00:06, 14.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2403 ||:  90%|████████▉ | 752/840 [00:52<00:06, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2404 ||:  90%|████████▉ | 754/840 [00:52<00:06, 14.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2405 ||:  90%|█████████ | 756/840 [00:52<00:05, 14.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2406 ||:  90%|█████████ | 758/840 [00:52<00:05, 14.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2402 ||:  90%|█████████ | 760/840 [00:52<00:05, 14.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2403 ||:  91%|█████████ | 762/840 [00:52<00:05, 14.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2404 ||:  91%|█████████ | 764/840 [00:53<00:05, 14.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2403 ||:  91%|█████████ | 766/840 [00:53<00:05, 14.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2401 ||:  91%|█████████▏| 768/840 [00:53<00:05, 14.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2401 ||:  92%|█████████▏| 770/840 [00:53<00:04, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9275, loss: 0.2399 ||:  92%|█████████▏| 772/840 [00:53<00:04, 14.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2407 ||:  92%|█████████▏| 774/840 [00:53<00:04, 14.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2408 ||:  92%|█████████▏| 776/840 [00:53<00:04, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2407 ||:  93%|█████████▎| 778/840 [00:54<00:04, 14.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9274, loss: 0.2408 ||:  93%|█████████▎| 780/840 [00:54<00:04, 14.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2409 ||:  93%|█████████▎| 782/840 [00:54<00:03, 14.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2411 ||:  93%|█████████▎| 784/840 [00:54<00:03, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2420 ||:  94%|█████████▎| 786/840 [00:54<00:03, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2417 ||:  94%|█████████▍| 788/840 [00:54<00:03, 14.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2420 ||:  94%|█████████▍| 790/840 [00:54<00:03, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2418 ||:  94%|█████████▍| 792/840 [00:55<00:03, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2416 ||:  95%|█████████▍| 794/840 [00:55<00:03, 14.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9269, loss: 0.2417 ||:  95%|█████████▍| 796/840 [00:55<00:03, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9269, loss: 0.2414 ||:  95%|█████████▌| 798/840 [00:55<00:02, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9269, loss: 0.2413 ||:  95%|█████████▌| 800/840 [00:55<00:02, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2412 ||:  95%|█████████▌| 802/840 [00:55<00:02, 14.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9269, loss: 0.2413 ||:  96%|█████████▌| 804/840 [00:55<00:02, 14.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2416 ||:  96%|█████████▌| 806/840 [00:55<00:02, 14.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9267, loss: 0.2416 ||:  96%|█████████▌| 808/840 [00:56<00:02, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9269, loss: 0.2412 ||:  96%|█████████▋| 810/840 [00:56<00:02, 14.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2410 ||:  97%|█████████▋| 812/840 [00:56<00:01, 14.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2413 ||:  97%|█████████▋| 814/840 [00:56<00:01, 15.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9268, loss: 0.2411 ||:  97%|█████████▋| 816/840 [00:56<00:01, 15.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9270, loss: 0.2407 ||:  97%|█████████▋| 818/840 [00:56<00:01, 15.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2403 ||:  98%|█████████▊| 820/840 [00:56<00:01, 13.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2403 ||:  98%|█████████▊| 822/840 [00:57<00:01, 13.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2404 ||:  98%|█████████▊| 824/840 [00:57<00:01, 14.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2408 ||:  98%|█████████▊| 826/840 [00:57<00:01, 13.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2405 ||:  99%|█████████▊| 828/840 [00:57<00:00, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2409 ||:  99%|█████████▉| 830/840 [00:57<00:00, 14.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9273, loss: 0.2405 ||:  99%|█████████▉| 832/840 [00:57<00:00, 14.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2411 ||:  99%|█████████▉| 834/840 [00:57<00:00, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2411 ||: 100%|█████████▉| 836/840 [00:58<00:00, 14.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9271, loss: 0.2412 ||: 100%|█████████▉| 838/840 [00:58<00:00, 14.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9272, loss: 0.2410 ||: 100%|██████████| 840/840 [00:58<00:00, 14.75it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/360 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8536, loss: 0.4416 ||:   4%|▍         | 14/360 [00:00<00:02, 131.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8603, loss: 0.4250 ||:   8%|▊         | 29/360 [00:00<00:02, 136.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8663, loss: 0.4317 ||:  13%|█▎        | 46/360 [00:00<00:02, 144.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8672, loss: 0.4419 ||:  17%|█▋        | 61/360 [00:00<00:02, 145.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8650, loss: 0.4362 ||:  22%|██▏       | 80/360 [00:00<00:01, 154.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8687, loss: 0.4357 ||:  28%|██▊       | 99/360 [00:00<00:01, 162.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8705, loss: 0.4344 ||:  32%|███▎      | 117/360 [00:00<00:01, 165.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8694, loss: 0.4330 ||:  37%|███▋      | 134/360 [00:00<00:01, 165.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8709, loss: 0.4284 ||:  42%|████▏     | 151/360 [00:00<00:01, 166.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8694, loss: 0.4344 ||:  47%|████▋     | 170/360 [00:01<00:01, 170.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8698, loss: 0.4365 ||:  52%|█████▏    | 187/360 [00:01<00:01, 167.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8684, loss: 0.4398 ||:  57%|█████▋    | 204/360 [00:01<00:00, 165.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8695, loss: 0.4337 ||:  61%|██████▏   | 221/360 [00:01<00:00, 166.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8692, loss: 0.4308 ||:  66%|██████▋   | 239/360 [00:01<00:00, 170.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8703, loss: 0.4296 ||:  72%|███████▏  | 258/360 [00:01<00:00, 174.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8707, loss: 0.4285 ||:  77%|███████▋  | 276/360 [00:01<00:00, 171.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8711, loss: 0.4277 ||:  82%|████████▏ | 294/360 [00:01<00:00, 170.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8705, loss: 0.4261 ||:  87%|████████▋ | 312/360 [00:01<00:00, 171.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8711, loss: 0.4217 ||:  92%|█████████▏| 331/360 [00:01<00:00, 174.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8709, loss: 0.4196 ||:  97%|█████████▋| 349/360 [00:02<00:00, 175.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8717, loss: 0.4187 ||: 100%|██████████| 360/360 [00:02<00:00, 168.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/840 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9750, loss: 0.1480 ||:   0%|          | 2/840 [00:00<01:08, 12.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9375, loss: 0.2058 ||:   0%|          | 4/840 [00:00<01:04, 12.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9333, loss: 0.2367 ||:   1%|          | 6/840 [00:00<01:02, 13.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9437, loss: 0.2040 ||:   1%|          | 8/840 [00:00<01:01, 13.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.1778 ||:   1%|          | 10/840 [00:00<01:00, 13.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.1859 ||:   1%|▏         | 12/840 [00:00<01:00, 13.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.1810 ||:   2%|▏         | 14/840 [00:01<00:59, 13.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1714 ||:   2%|▏         | 16/840 [00:01<00:58, 14.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.1690 ||:   2%|▏         | 18/840 [00:01<00:59, 13.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1638 ||:   2%|▏         | 20/840 [00:01<00:58, 13.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1602 ||:   3%|▎         | 22/840 [00:01<00:59, 13.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.1587 ||:   3%|▎         | 24/840 [00:01<01:00, 13.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9481, loss: 0.1589 ||:   3%|▎         | 26/840 [00:01<01:01, 13.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.1570 ||:   3%|▎         | 28/840 [00:02<01:00, 13.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9483, loss: 0.1643 ||:   4%|▎         | 30/840 [00:02<00:58, 13.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.1592 ||:   4%|▍         | 32/840 [00:02<00:57, 13.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.1581 ||:   4%|▍         | 34/840 [00:02<00:56, 14.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.1564 ||:   4%|▍         | 36/840 [00:02<00:54, 14.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1519 ||:   5%|▍         | 38/840 [00:02<00:58, 13.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1513 ||:   5%|▍         | 40/840 [00:02<00:57, 13.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1532 ||:   5%|▌         | 42/840 [00:03<00:56, 14.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1513 ||:   5%|▌         | 44/840 [00:03<00:55, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9543, loss: 0.1476 ||:   5%|▌         | 46/840 [00:03<00:55, 14.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1494 ||:   6%|▌         | 48/840 [00:03<00:54, 14.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1475 ||:   6%|▌         | 50/840 [00:03<00:54, 14.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1463 ||:   6%|▌         | 52/840 [00:03<00:57, 13.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9500, loss: 0.1541 ||:   6%|▋         | 54/840 [00:03<00:57, 13.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9509, loss: 0.1553 ||:   7%|▋         | 56/840 [00:04<00:55, 14.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1512 ||:   7%|▋         | 58/840 [00:04<00:55, 14.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9533, loss: 0.1479 ||:   7%|▋         | 60/840 [00:04<00:57, 13.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9540, loss: 0.1465 ||:   7%|▋         | 62/840 [00:04<00:56, 13.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9547, loss: 0.1467 ||:   8%|▊         | 64/840 [00:04<00:55, 14.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9545, loss: 0.1500 ||:   8%|▊         | 66/840 [00:04<00:54, 14.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9559, loss: 0.1467 ||:   8%|▊         | 68/840 [00:04<00:53, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9564, loss: 0.1457 ||:   8%|▊         | 70/840 [00:04<00:53, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9569, loss: 0.1438 ||:   9%|▊         | 72/840 [00:05<00:52, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9561, loss: 0.1451 ||:   9%|▉         | 74/840 [00:05<00:51, 14.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9553, loss: 0.1498 ||:   9%|▉         | 76/840 [00:05<00:51, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9564, loss: 0.1473 ||:   9%|▉         | 78/840 [00:05<00:50, 14.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9569, loss: 0.1454 ||:  10%|▉         | 80/840 [00:05<00:58, 13.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9567, loss: 0.1463 ||:  10%|▉         | 82/840 [00:05<00:57, 13.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9565, loss: 0.1455 ||:  10%|█         | 84/840 [00:05<00:54, 13.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9570, loss: 0.1444 ||:  10%|█         | 86/840 [00:06<00:53, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9557, loss: 0.1479 ||:  10%|█         | 88/840 [00:06<00:53, 14.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9561, loss: 0.1477 ||:  11%|█         | 90/840 [00:06<00:52, 14.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9560, loss: 0.1482 ||:  11%|█         | 92/840 [00:06<00:51, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9564, loss: 0.1471 ||:  11%|█         | 94/840 [00:06<00:50, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9568, loss: 0.1460 ||:  11%|█▏        | 96/840 [00:06<00:50, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9566, loss: 0.1463 ||:  12%|█▏        | 98/840 [00:06<00:50, 14.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9560, loss: 0.1473 ||:  12%|█▏        | 100/840 [00:07<00:50, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9564, loss: 0.1471 ||:  12%|█▏        | 102/840 [00:07<00:50, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9567, loss: 0.1456 ||:  12%|█▏        | 104/840 [00:07<00:50, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9571, loss: 0.1455 ||:  13%|█▎        | 106/840 [00:07<00:50, 14.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9565, loss: 0.1468 ||:  13%|█▎        | 108/840 [00:07<00:49, 14.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9564, loss: 0.1471 ||:  13%|█▎        | 110/840 [00:07<00:50, 14.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9545, loss: 0.1533 ||:  13%|█▎        | 112/840 [00:07<00:48, 14.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9544, loss: 0.1531 ||:  14%|█▎        | 114/840 [00:08<00:48, 14.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9547, loss: 0.1530 ||:  14%|█▍        | 116/840 [00:08<00:48, 14.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9551, loss: 0.1531 ||:  14%|█▍        | 118/840 [00:08<00:49, 14.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9546, loss: 0.1558 ||:  14%|█▍        | 120/840 [00:08<00:49, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9549, loss: 0.1552 ||:  15%|█▍        | 122/840 [00:08<00:49, 14.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9544, loss: 0.1554 ||:  15%|█▍        | 124/840 [00:08<00:49, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9552, loss: 0.1538 ||:  15%|█▌        | 126/840 [00:08<00:50, 14.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9547, loss: 0.1535 ||:  15%|█▌        | 128/840 [00:09<00:51, 13.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9554, loss: 0.1520 ||:  15%|█▌        | 130/840 [00:09<00:50, 14.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9557, loss: 0.1510 ||:  16%|█▌        | 132/840 [00:09<00:50, 14.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9563, loss: 0.1499 ||:  16%|█▌        | 134/840 [00:09<00:49, 14.31it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9559, loss: 0.1525 ||:  16%|█▌        | 136/840 [00:09<00:51, 13.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9562, loss: 0.1532 ||:  16%|█▋        | 138/840 [00:09<00:51, 13.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9550, loss: 0.1541 ||:  17%|█▋        | 140/840 [00:09<00:50, 13.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9553, loss: 0.1538 ||:  17%|█▋        | 142/840 [00:10<00:48, 14.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9545, loss: 0.1570 ||:  17%|█▋        | 144/840 [00:10<00:47, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9545, loss: 0.1577 ||:  17%|█▋        | 146/840 [00:10<00:46, 14.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9537, loss: 0.1585 ||:  18%|█▊        | 148/840 [00:10<00:46, 15.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9543, loss: 0.1569 ||:  18%|█▊        | 150/840 [00:10<00:47, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9543, loss: 0.1562 ||:  18%|█▊        | 152/840 [00:10<00:46, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9542, loss: 0.1557 ||:  18%|█▊        | 154/840 [00:10<00:46, 14.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9542, loss: 0.1570 ||:  19%|█▊        | 156/840 [00:10<00:46, 14.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1586 ||:  19%|█▉        | 158/840 [00:11<00:47, 14.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1591 ||:  19%|█▉        | 160/840 [00:11<00:47, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1586 ||:  19%|█▉        | 162/840 [00:11<00:48, 13.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1579 ||:  20%|█▉        | 164/840 [00:11<00:47, 14.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1576 ||:  20%|█▉        | 166/840 [00:11<00:46, 14.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9533, loss: 0.1563 ||:  20%|██        | 168/840 [00:11<00:45, 14.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9538, loss: 0.1548 ||:  20%|██        | 170/840 [00:11<00:44, 15.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9541, loss: 0.1545 ||:  20%|██        | 172/840 [00:12<00:43, 15.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9543, loss: 0.1535 ||:  21%|██        | 174/840 [00:12<00:44, 15.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9540, loss: 0.1537 ||:  21%|██        | 176/840 [00:12<00:43, 15.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9534, loss: 0.1551 ||:  21%|██        | 178/840 [00:12<00:43, 15.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9539, loss: 0.1541 ||:  21%|██▏       | 180/840 [00:12<00:49, 13.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9536, loss: 0.1537 ||:  22%|██▏       | 182/840 [00:12<00:47, 13.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1540 ||:  22%|██▏       | 184/840 [00:12<00:45, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1535 ||:  22%|██▏       | 186/840 [00:13<00:45, 14.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1533 ||:  22%|██▏       | 188/840 [00:13<00:45, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9534, loss: 0.1525 ||:  23%|██▎       | 190/840 [00:13<00:45, 14.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9536, loss: 0.1520 ||:  23%|██▎       | 192/840 [00:13<00:44, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9539, loss: 0.1513 ||:  23%|██▎       | 194/840 [00:13<00:44, 14.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9536, loss: 0.1518 ||:  23%|██▎       | 196/840 [00:13<00:44, 14.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9538, loss: 0.1519 ||:  24%|██▎       | 198/840 [00:13<00:43, 14.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9537, loss: 0.1515 ||:  24%|██▍       | 200/840 [00:13<00:42, 15.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9537, loss: 0.1512 ||:  24%|██▍       | 202/840 [00:14<00:41, 15.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9542, loss: 0.1501 ||:  24%|██▍       | 204/840 [00:14<00:42, 15.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9539, loss: 0.1515 ||:  25%|██▍       | 206/840 [00:14<00:44, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9536, loss: 0.1528 ||:  25%|██▍       | 208/840 [00:14<00:44, 14.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9538, loss: 0.1525 ||:  25%|██▌       | 210/840 [00:14<00:43, 14.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9538, loss: 0.1519 ||:  25%|██▌       | 212/840 [00:14<00:43, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9542, loss: 0.1509 ||:  25%|██▌       | 214/840 [00:14<00:44, 13.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9542, loss: 0.1511 ||:  26%|██▌       | 216/840 [00:15<00:45, 13.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9544, loss: 0.1509 ||:  26%|██▌       | 218/840 [00:15<00:43, 14.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9548, loss: 0.1498 ||:  26%|██▌       | 220/840 [00:15<00:43, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9550, loss: 0.1495 ||:  26%|██▋       | 222/840 [00:15<00:43, 14.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9542, loss: 0.1522 ||:  27%|██▋       | 224/840 [00:15<00:42, 14.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9542, loss: 0.1536 ||:  27%|██▋       | 226/840 [00:15<00:41, 14.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9544, loss: 0.1535 ||:  27%|██▋       | 228/840 [00:15<00:41, 14.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9535, loss: 0.1555 ||:  27%|██▋       | 230/840 [00:16<00:40, 14.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9534, loss: 0.1554 ||:  28%|██▊       | 232/840 [00:16<00:41, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9538, loss: 0.1545 ||:  28%|██▊       | 234/840 [00:16<00:41, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9538, loss: 0.1547 ||:  28%|██▊       | 236/840 [00:16<00:40, 14.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9536, loss: 0.1547 ||:  28%|██▊       | 238/840 [00:16<00:40, 15.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9537, loss: 0.1542 ||:  29%|██▊       | 240/840 [00:16<00:40, 14.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9535, loss: 0.1545 ||:  29%|██▉       | 242/840 [00:16<00:39, 15.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9535, loss: 0.1538 ||:  29%|██▉       | 244/840 [00:16<00:39, 14.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9539, loss: 0.1531 ||:  29%|██▉       | 246/840 [00:17<00:40, 14.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1566 ||:  30%|██▉       | 248/840 [00:17<00:39, 14.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1590 ||:  30%|██▉       | 250/840 [00:17<00:39, 14.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1591 ||:  30%|███       | 252/840 [00:17<00:39, 15.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1585 ||:  30%|███       | 254/840 [00:17<00:39, 14.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1582 ||:  30%|███       | 256/840 [00:17<00:38, 15.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1585 ||:  31%|███       | 258/840 [00:17<00:38, 14.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9533, loss: 0.1576 ||:  31%|███       | 260/840 [00:18<00:39, 14.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1588 ||:  31%|███       | 262/840 [00:18<00:40, 14.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9534, loss: 0.1579 ||:  31%|███▏      | 264/840 [00:18<00:41, 13.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9534, loss: 0.1577 ||:  32%|███▏      | 266/840 [00:18<00:40, 14.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1590 ||:  32%|███▏      | 268/840 [00:18<00:41, 13.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1593 ||:  32%|███▏      | 270/840 [00:18<00:41, 13.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1591 ||:  32%|███▏      | 272/840 [00:18<00:39, 14.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1587 ||:  33%|███▎      | 274/840 [00:19<00:38, 14.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1586 ||:  33%|███▎      | 276/840 [00:19<00:39, 14.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1580 ||:  33%|███▎      | 278/840 [00:19<00:38, 14.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1596 ||:  33%|███▎      | 280/840 [00:19<00:43, 12.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1615 ||:  34%|███▎      | 282/840 [00:19<00:41, 13.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1609 ||:  34%|███▍      | 284/840 [00:19<00:39, 13.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1611 ||:  34%|███▍      | 286/840 [00:19<00:39, 14.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1620 ||:  34%|███▍      | 288/840 [00:20<00:38, 14.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1623 ||:  35%|███▍      | 290/840 [00:20<00:39, 14.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1620 ||:  35%|███▍      | 292/840 [00:20<00:38, 14.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9520, loss: 0.1637 ||:  35%|███▌      | 294/840 [00:20<00:41, 13.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1640 ||:  35%|███▌      | 296/840 [00:20<00:39, 13.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1637 ||:  35%|███▌      | 298/840 [00:20<00:38, 13.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1629 ||:  36%|███▌      | 300/840 [00:20<00:37, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1623 ||:  36%|███▌      | 302/840 [00:21<00:37, 14.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1626 ||:  36%|███▌      | 304/840 [00:21<00:38, 14.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1636 ||:  36%|███▋      | 306/840 [00:21<00:38, 13.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1632 ||:  37%|███▋      | 308/840 [00:21<00:38, 13.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1628 ||:  37%|███▋      | 310/840 [00:21<00:39, 13.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1629 ||:  37%|███▋      | 312/840 [00:21<00:39, 13.42it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9522, loss: 0.1630 ||:  37%|███▋      | 314/840 [00:21<00:39, 13.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1627 ||:  38%|███▊      | 316/840 [00:22<00:38, 13.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1627 ||:  38%|███▊      | 318/840 [00:22<00:37, 13.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1626 ||:  38%|███▊      | 320/840 [00:22<00:36, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1626 ||:  38%|███▊      | 322/840 [00:22<00:36, 14.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1625 ||:  39%|███▊      | 324/840 [00:22<00:35, 14.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1625 ||:  39%|███▉      | 326/840 [00:22<00:35, 14.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1618 ||:  39%|███▉      | 328/840 [00:22<00:34, 14.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1616 ||:  39%|███▉      | 330/840 [00:23<00:36, 14.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1621 ||:  40%|███▉      | 332/840 [00:23<00:36, 13.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1617 ||:  40%|███▉      | 334/840 [00:23<00:37, 13.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1627 ||:  40%|████      | 336/840 [00:23<00:35, 14.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1619 ||:  40%|████      | 338/840 [00:23<00:34, 14.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1624 ||:  40%|████      | 340/840 [00:23<00:34, 14.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1622 ||:  41%|████      | 342/840 [00:23<00:33, 14.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1619 ||:  41%|████      | 344/840 [00:24<00:32, 15.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1615 ||:  41%|████      | 346/840 [00:24<00:33, 14.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1609 ||:  41%|████▏     | 348/840 [00:24<00:34, 14.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1612 ||:  42%|████▏     | 350/840 [00:24<00:34, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1612 ||:  42%|████▏     | 352/840 [00:24<00:33, 14.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1617 ||:  42%|████▏     | 354/840 [00:24<00:33, 14.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1612 ||:  42%|████▏     | 356/840 [00:24<00:33, 14.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1611 ||:  43%|████▎     | 358/840 [00:25<00:33, 14.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1610 ||:  43%|████▎     | 360/840 [00:25<00:32, 14.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1609 ||:  43%|████▎     | 362/840 [00:25<00:32, 14.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1609 ||:  43%|████▎     | 364/840 [00:25<00:32, 14.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1605 ||:  44%|████▎     | 366/840 [00:25<00:31, 14.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1601 ||:  44%|████▍     | 368/840 [00:25<00:31, 15.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1603 ||:  44%|████▍     | 370/840 [00:25<00:31, 14.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1604 ||:  44%|████▍     | 372/840 [00:25<00:31, 14.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1614 ||:  45%|████▍     | 374/840 [00:26<00:31, 14.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1617 ||:  45%|████▍     | 376/840 [00:26<00:32, 14.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1622 ||:  45%|████▌     | 378/840 [00:26<00:32, 14.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1625 ||:  45%|████▌     | 380/840 [00:26<00:36, 12.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1621 ||:  45%|████▌     | 382/840 [00:26<00:35, 12.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1622 ||:  46%|████▌     | 384/840 [00:26<00:34, 13.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1615 ||:  46%|████▌     | 386/840 [00:27<00:33, 13.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1612 ||:  46%|████▌     | 388/840 [00:27<00:33, 13.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1608 ||:  46%|████▋     | 390/840 [00:27<00:33, 13.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1607 ||:  47%|████▋     | 392/840 [00:27<00:32, 13.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1603 ||:  47%|████▋     | 394/840 [00:27<00:33, 13.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1607 ||:  47%|████▋     | 396/840 [00:27<00:32, 13.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1605 ||:  47%|████▋     | 398/840 [00:27<00:32, 13.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1606 ||:  48%|████▊     | 400/840 [00:28<00:31, 14.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1605 ||:  48%|████▊     | 402/840 [00:28<00:31, 13.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9533, loss: 0.1600 ||:  48%|████▊     | 404/840 [00:28<00:31, 13.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9534, loss: 0.1600 ||:  48%|████▊     | 406/840 [00:28<00:31, 13.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9533, loss: 0.1606 ||:  49%|████▊     | 408/840 [00:28<00:31, 13.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1608 ||:  49%|████▉     | 410/840 [00:28<00:31, 13.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1606 ||:  49%|████▉     | 412/840 [00:28<00:29, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1612 ||:  49%|████▉     | 414/840 [00:29<00:29, 14.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1609 ||:  50%|████▉     | 416/840 [00:29<00:29, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1613 ||:  50%|████▉     | 418/840 [00:29<00:30, 13.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1610 ||:  50%|█████     | 420/840 [00:29<00:31, 13.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1606 ||:  50%|█████     | 422/840 [00:29<00:30, 13.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1603 ||:  50%|█████     | 424/840 [00:29<00:29, 13.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1608 ||:  51%|█████     | 426/840 [00:29<00:29, 14.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1603 ||:  51%|█████     | 428/840 [00:30<00:28, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1598 ||:  51%|█████     | 430/840 [00:30<00:28, 14.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1607 ||:  51%|█████▏    | 432/840 [00:30<00:28, 14.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1611 ||:  52%|█████▏    | 434/840 [00:30<00:29, 13.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1614 ||:  52%|█████▏    | 436/840 [00:30<00:29, 13.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1623 ||:  52%|█████▏    | 438/840 [00:30<00:28, 14.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1621 ||:  52%|█████▏    | 440/840 [00:30<00:27, 14.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1620 ||:  53%|█████▎    | 442/840 [00:31<00:27, 14.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1620 ||:  53%|█████▎    | 444/840 [00:31<00:28, 13.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1616 ||:  53%|█████▎    | 446/840 [00:31<00:27, 14.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1617 ||:  53%|█████▎    | 448/840 [00:31<00:27, 14.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1617 ||:  54%|█████▎    | 450/840 [00:31<00:27, 14.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1623 ||:  54%|█████▍    | 452/840 [00:31<00:27, 14.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1625 ||:  54%|█████▍    | 454/840 [00:31<00:26, 14.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1627 ||:  54%|█████▍    | 456/840 [00:32<00:26, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1627 ||:  55%|█████▍    | 458/840 [00:32<00:27, 14.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1626 ||:  55%|█████▍    | 460/840 [00:32<00:26, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9516, loss: 0.1632 ||:  55%|█████▌    | 462/840 [00:32<00:27, 13.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9516, loss: 0.1629 ||:  55%|█████▌    | 464/840 [00:32<00:26, 14.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1625 ||:  55%|█████▌    | 466/840 [00:32<00:26, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9515, loss: 0.1630 ||:  56%|█████▌    | 468/840 [00:32<00:26, 14.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9512, loss: 0.1637 ||:  56%|█████▌    | 470/840 [00:33<00:27, 13.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9513, loss: 0.1637 ||:  56%|█████▌    | 472/840 [00:33<00:26, 13.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9513, loss: 0.1636 ||:  56%|█████▋    | 474/840 [00:33<00:26, 13.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9515, loss: 0.1631 ||:  57%|█████▋    | 476/840 [00:33<00:25, 14.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9516, loss: 0.1630 ||:  57%|█████▋    | 478/840 [00:33<00:25, 14.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1626 ||:  57%|█████▋    | 480/840 [00:33<00:28, 12.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1623 ||:  57%|█████▋    | 482/840 [00:33<00:27, 12.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1621 ||:  58%|█████▊    | 484/840 [00:34<00:26, 13.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1623 ||:  58%|█████▊    | 486/840 [00:34<00:25, 13.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9516, loss: 0.1624 ||:  58%|█████▊    | 488/840 [00:34<00:24, 14.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1621 ||:  58%|█████▊    | 490/840 [00:34<00:24, 14.11it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9519, loss: 0.1616 ||:  59%|█████▊    | 492/840 [00:34<00:24, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1618 ||:  59%|█████▉    | 494/840 [00:34<00:24, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1620 ||:  59%|█████▉    | 496/840 [00:34<00:23, 14.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1622 ||:  59%|█████▉    | 498/840 [00:35<00:23, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1628 ||:  60%|█████▉    | 500/840 [00:35<00:22, 14.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1630 ||:  60%|█████▉    | 502/840 [00:35<00:22, 15.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1634 ||:  60%|██████    | 504/840 [00:35<00:22, 14.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1633 ||:  60%|██████    | 506/840 [00:35<00:22, 14.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1631 ||:  60%|██████    | 508/840 [00:35<00:22, 14.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1628 ||:  61%|██████    | 510/840 [00:35<00:22, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9520, loss: 0.1623 ||:  61%|██████    | 512/840 [00:35<00:22, 14.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1626 ||:  61%|██████    | 514/840 [00:36<00:22, 14.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1621 ||:  61%|██████▏   | 516/840 [00:36<00:22, 14.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1618 ||:  62%|██████▏   | 518/840 [00:36<00:21, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1612 ||:  62%|██████▏   | 520/840 [00:36<00:21, 14.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1616 ||:  62%|██████▏   | 522/840 [00:36<00:22, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1613 ||:  62%|██████▏   | 524/840 [00:36<00:21, 14.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1617 ||:  63%|██████▎   | 526/840 [00:36<00:21, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1612 ||:  63%|██████▎   | 528/840 [00:37<00:21, 14.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1614 ||:  63%|██████▎   | 530/840 [00:37<00:21, 14.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1612 ||:  63%|██████▎   | 532/840 [00:37<00:20, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1615 ||:  64%|██████▎   | 534/840 [00:37<00:21, 14.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1616 ||:  64%|██████▍   | 536/840 [00:37<00:22, 13.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1620 ||:  64%|██████▍   | 538/840 [00:37<00:21, 13.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1625 ||:  64%|██████▍   | 540/840 [00:37<00:20, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1625 ||:  65%|██████▍   | 542/840 [00:38<00:20, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1621 ||:  65%|██████▍   | 544/840 [00:38<00:20, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1617 ||:  65%|██████▌   | 546/840 [00:38<00:19, 14.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1622 ||:  65%|██████▌   | 548/840 [00:38<00:19, 14.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1631 ||:  65%|██████▌   | 550/840 [00:38<00:19, 15.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1629 ||:  66%|██████▌   | 552/840 [00:38<00:18, 15.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1637 ||:  66%|██████▌   | 554/840 [00:38<00:18, 15.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9520, loss: 0.1637 ||:  66%|██████▌   | 556/840 [00:38<00:18, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1634 ||:  66%|██████▋   | 558/840 [00:39<00:19, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1634 ||:  67%|██████▋   | 560/840 [00:39<00:19, 14.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1631 ||:  67%|██████▋   | 562/840 [00:39<00:19, 14.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1633 ||:  67%|██████▋   | 564/840 [00:39<00:18, 14.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1630 ||:  67%|██████▋   | 566/840 [00:39<00:18, 14.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1636 ||:  68%|██████▊   | 568/840 [00:39<00:18, 14.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1636 ||:  68%|██████▊   | 570/840 [00:39<00:18, 14.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1640 ||:  68%|██████▊   | 572/840 [00:40<00:17, 15.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1640 ||:  68%|██████▊   | 574/840 [00:40<00:18, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1639 ||:  69%|██████▊   | 576/840 [00:40<00:18, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1639 ||:  69%|██████▉   | 578/840 [00:40<00:18, 14.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1642 ||:  69%|██████▉   | 580/840 [00:40<00:20, 12.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1642 ||:  69%|██████▉   | 582/840 [00:40<00:19, 13.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9518, loss: 0.1642 ||:  70%|██████▉   | 584/840 [00:40<00:18, 13.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1647 ||:  70%|██████▉   | 586/840 [00:41<00:17, 14.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9515, loss: 0.1652 ||:  70%|███████   | 588/840 [00:41<00:17, 14.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9514, loss: 0.1661 ||:  70%|███████   | 590/840 [00:41<00:17, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9514, loss: 0.1663 ||:  70%|███████   | 592/840 [00:41<00:16, 14.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9515, loss: 0.1660 ||:  71%|███████   | 594/840 [00:41<00:16, 15.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9516, loss: 0.1657 ||:  71%|███████   | 596/840 [00:41<00:15, 15.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1657 ||:  71%|███████   | 598/840 [00:41<00:15, 15.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9515, loss: 0.1663 ||:  71%|███████▏  | 600/840 [00:41<00:15, 15.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9516, loss: 0.1660 ||:  72%|███████▏  | 602/840 [00:42<00:15, 15.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1656 ||:  72%|███████▏  | 604/840 [00:42<00:15, 14.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1653 ||:  72%|███████▏  | 606/840 [00:42<00:15, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1653 ||:  72%|███████▏  | 608/840 [00:42<00:16, 14.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9520, loss: 0.1650 ||:  73%|███████▎  | 610/840 [00:42<00:16, 14.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1656 ||:  73%|███████▎  | 612/840 [00:42<00:15, 14.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9520, loss: 0.1653 ||:  73%|███████▎  | 614/840 [00:42<00:15, 14.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1649 ||:  73%|███████▎  | 616/840 [00:43<00:15, 14.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1648 ||:  74%|███████▎  | 618/840 [00:43<00:15, 14.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1646 ||:  74%|███████▍  | 620/840 [00:43<00:14, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1645 ||:  74%|███████▍  | 622/840 [00:43<00:14, 14.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1649 ||:  74%|███████▍  | 624/840 [00:43<00:14, 15.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1647 ||:  75%|███████▍  | 626/840 [00:43<00:14, 14.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9520, loss: 0.1652 ||:  75%|███████▍  | 628/840 [00:43<00:14, 14.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9517, loss: 0.1661 ||:  75%|███████▌  | 630/840 [00:44<00:14, 14.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9519, loss: 0.1658 ||:  75%|███████▌  | 632/840 [00:44<00:14, 14.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9520, loss: 0.1655 ||:  75%|███████▌  | 634/840 [00:44<00:14, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1652 ||:  76%|███████▌  | 636/840 [00:44<00:13, 14.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9520, loss: 0.1650 ||:  76%|███████▌  | 638/840 [00:44<00:13, 14.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1647 ||:  76%|███████▌  | 640/840 [00:44<00:13, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1643 ||:  76%|███████▋  | 642/840 [00:44<00:13, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1649 ||:  77%|███████▋  | 644/840 [00:45<00:13, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1649 ||:  77%|███████▋  | 646/840 [00:45<00:13, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1648 ||:  77%|███████▋  | 648/840 [00:45<00:13, 14.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1650 ||:  77%|███████▋  | 650/840 [00:45<00:13, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9521, loss: 0.1648 ||:  78%|███████▊  | 652/840 [00:45<00:13, 14.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9522, loss: 0.1645 ||:  78%|███████▊  | 654/840 [00:45<00:12, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1643 ||:  78%|███████▊  | 656/840 [00:45<00:12, 14.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1639 ||:  78%|███████▊  | 658/840 [00:45<00:12, 14.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1637 ||:  79%|███████▊  | 660/840 [00:46<00:11, 15.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1636 ||:  79%|███████▉  | 662/840 [00:46<00:11, 15.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1641 ||:  79%|███████▉  | 664/840 [00:46<00:11, 15.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9523, loss: 0.1639 ||:  79%|███████▉  | 666/840 [00:46<00:11, 15.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1637 ||:  80%|███████▉  | 668/840 [00:46<00:11, 15.48it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9525, loss: 0.1636 ||:  80%|███████▉  | 670/840 [00:46<00:11, 15.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1632 ||:  80%|████████  | 672/840 [00:46<00:11, 14.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1638 ||:  80%|████████  | 674/840 [00:47<00:11, 14.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1638 ||:  80%|████████  | 676/840 [00:47<00:10, 15.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1645 ||:  81%|████████  | 678/840 [00:47<00:10, 15.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1642 ||:  81%|████████  | 680/840 [00:47<00:11, 13.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1642 ||:  81%|████████  | 682/840 [00:47<00:11, 13.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1639 ||:  81%|████████▏ | 684/840 [00:47<00:11, 14.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1641 ||:  82%|████████▏ | 686/840 [00:47<00:11, 13.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1641 ||:  82%|████████▏ | 688/840 [00:48<00:10, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1641 ||:  82%|████████▏ | 690/840 [00:48<00:10, 14.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1641 ||:  82%|████████▏ | 692/840 [00:48<00:10, 14.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1638 ||:  83%|████████▎ | 694/840 [00:48<00:10, 14.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1640 ||:  83%|████████▎ | 696/840 [00:48<00:09, 14.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1638 ||:  83%|████████▎ | 698/840 [00:48<00:09, 14.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1635 ||:  83%|████████▎ | 700/840 [00:48<00:09, 15.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1638 ||:  84%|████████▎ | 702/840 [00:48<00:09, 14.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1636 ||:  84%|████████▍ | 704/840 [00:49<00:09, 14.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1637 ||:  84%|████████▍ | 706/840 [00:49<00:09, 14.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1636 ||:  84%|████████▍ | 708/840 [00:49<00:08, 15.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1639 ||:  85%|████████▍ | 710/840 [00:49<00:08, 15.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1637 ||:  85%|████████▍ | 712/840 [00:49<00:08, 15.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1633 ||:  85%|████████▌ | 714/840 [00:49<00:08, 14.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1633 ||:  85%|████████▌ | 716/840 [00:49<00:08, 14.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1631 ||:  85%|████████▌ | 718/840 [00:50<00:08, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1630 ||:  86%|████████▌ | 720/840 [00:50<00:08, 14.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1636 ||:  86%|████████▌ | 722/840 [00:50<00:07, 14.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1641 ||:  86%|████████▌ | 724/840 [00:50<00:07, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1638 ||:  86%|████████▋ | 726/840 [00:50<00:07, 14.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1636 ||:  87%|████████▋ | 728/840 [00:50<00:07, 14.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1642 ||:  87%|████████▋ | 730/840 [00:50<00:07, 14.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1639 ||:  87%|████████▋ | 732/840 [00:50<00:07, 14.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1637 ||:  87%|████████▋ | 734/840 [00:51<00:06, 15.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1638 ||:  88%|████████▊ | 736/840 [00:51<00:06, 15.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1641 ||:  88%|████████▊ | 738/840 [00:51<00:06, 15.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1638 ||:  88%|████████▊ | 740/840 [00:51<00:06, 15.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1636 ||:  88%|████████▊ | 742/840 [00:51<00:06, 15.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1635 ||:  89%|████████▊ | 744/840 [00:51<00:06, 15.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1636 ||:  89%|████████▉ | 746/840 [00:51<00:06, 14.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1636 ||:  89%|████████▉ | 748/840 [00:52<00:06, 14.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1639 ||:  89%|████████▉ | 750/840 [00:52<00:05, 15.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1639 ||:  90%|████████▉ | 752/840 [00:52<00:05, 15.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1641 ||:  90%|████████▉ | 754/840 [00:52<00:06, 14.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1640 ||:  90%|█████████ | 756/840 [00:52<00:05, 14.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1640 ||:  90%|█████████ | 758/840 [00:52<00:05, 14.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1642 ||:  90%|█████████ | 760/840 [00:52<00:05, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1642 ||:  91%|█████████ | 762/840 [00:53<00:05, 14.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1640 ||:  91%|█████████ | 764/840 [00:53<00:05, 14.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1642 ||:  91%|█████████ | 766/840 [00:53<00:04, 15.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1639 ||:  91%|█████████▏| 768/840 [00:53<00:04, 15.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1636 ||:  92%|█████████▏| 770/840 [00:53<00:04, 14.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1633 ||:  92%|█████████▏| 772/840 [00:53<00:04, 14.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1636 ||:  92%|█████████▏| 774/840 [00:53<00:04, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1634 ||:  92%|█████████▏| 776/840 [00:53<00:04, 15.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1631 ||:  93%|█████████▎| 778/840 [00:54<00:04, 15.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1633 ||:  93%|█████████▎| 780/840 [00:54<00:04, 13.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9532, loss: 0.1631 ||:  93%|█████████▎| 782/840 [00:54<00:04, 13.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1635 ||:  93%|█████████▎| 784/840 [00:54<00:04, 13.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1637 ||:  94%|█████████▎| 786/840 [00:54<00:04, 13.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1643 ||:  94%|█████████▍| 788/840 [00:54<00:03, 14.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1644 ||:  94%|█████████▍| 790/840 [00:54<00:03, 14.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1643 ||:  94%|█████████▍| 792/840 [00:55<00:03, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9530, loss: 0.1642 ||:  95%|█████████▍| 794/840 [00:55<00:03, 14.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1638 ||:  95%|█████████▍| 796/840 [00:55<00:02, 14.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9531, loss: 0.1636 ||:  95%|█████████▌| 798/840 [00:55<00:02, 14.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9529, loss: 0.1642 ||:  95%|█████████▌| 800/840 [00:55<00:02, 14.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1646 ||:  95%|█████████▌| 802/840 [00:55<00:02, 15.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1645 ||:  96%|█████████▌| 804/840 [00:55<00:02, 15.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1651 ||:  96%|█████████▌| 806/840 [00:56<00:02, 15.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1652 ||:  96%|█████████▌| 808/840 [00:56<00:02, 14.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1656 ||:  96%|█████████▋| 810/840 [00:56<00:02, 14.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9524, loss: 0.1656 ||:  97%|█████████▋| 812/840 [00:56<00:01, 14.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9525, loss: 0.1654 ||:  97%|█████████▋| 814/840 [00:56<00:01, 14.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1651 ||:  97%|█████████▋| 816/840 [00:56<00:01, 15.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1653 ||:  97%|█████████▋| 818/840 [00:56<00:01, 15.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1653 ||:  98%|█████████▊| 820/840 [00:56<00:01, 15.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1655 ||:  98%|█████████▊| 822/840 [00:57<00:01, 14.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1654 ||:  98%|█████████▊| 824/840 [00:57<00:01, 15.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1653 ||:  98%|█████████▊| 826/840 [00:57<00:00, 14.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1651 ||:  99%|█████████▊| 828/840 [00:57<00:00, 14.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1652 ||:  99%|█████████▉| 830/840 [00:57<00:00, 15.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1655 ||:  99%|█████████▉| 832/840 [00:57<00:00, 15.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1660 ||:  99%|█████████▉| 834/840 [00:57<00:00, 15.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9527, loss: 0.1659 ||: 100%|█████████▉| 836/840 [00:58<00:00, 15.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9528, loss: 0.1656 ||: 100%|█████████▉| 838/840 [00:58<00:00, 15.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9526, loss: 0.1663 ||: 100%|██████████| 840/840 [00:58<00:00, 15.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/360 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8577, loss: 0.4211 ||:   4%|▎         | 13/360 [00:00<00:02, 128.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8607, loss: 0.4343 ||:   8%|▊         | 28/360 [00:00<00:02, 133.21it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8670, loss: 0.4526 ||:  13%|█▎        | 47/360 [00:00<00:02, 145.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8615, loss: 0.4870 ||:  18%|█▊        | 65/360 [00:00<00:01, 153.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8627, loss: 0.4768 ||:  23%|██▎       | 83/360 [00:00<00:01, 159.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8652, loss: 0.4754 ||:  28%|██▊       | 102/360 [00:00<00:01, 165.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8643, loss: 0.4705 ||:  33%|███▎      | 119/360 [00:00<00:01, 166.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8627, loss: 0.4717 ||:  38%|███▊      | 138/360 [00:00<00:01, 171.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8605, loss: 0.4778 ||:  44%|████▎     | 157/360 [00:00<00:01, 173.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8609, loss: 0.4796 ||:  48%|████▊     | 174/360 [00:01<00:01, 172.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8602, loss: 0.4803 ||:  53%|█████▎    | 192/360 [00:01<00:00, 173.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8600, loss: 0.4815 ||:  58%|█████▊    | 210/360 [00:01<00:00, 171.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8620, loss: 0.4750 ||:  64%|██████▎   | 229/360 [00:01<00:00, 174.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8644, loss: 0.4665 ||:  69%|██████▊   | 247/360 [00:01<00:00, 172.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8641, loss: 0.4713 ||:  74%|███████▍  | 266/360 [00:01<00:00, 175.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8648, loss: 0.4663 ||:  79%|███████▉  | 284/360 [00:01<00:00, 174.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8647, loss: 0.4639 ||:  84%|████████▍ | 303/360 [00:01<00:00, 177.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8649, loss: 0.4618 ||:  90%|████████▉ | 323/360 [00:01<00:00, 181.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8645, loss: 0.4615 ||:  95%|█████████▌| 342/360 [00:01<00:00, 179.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.8651, loss: 0.4578 ||: 100%|██████████| 360/360 [00:02<00:00, 173.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/840 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 1.0000, loss: 0.0312 ||:   0%|          | 1/840 [00:00<01:40,  8.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9833, loss: 0.1549 ||:   0%|          | 3/840 [00:00<01:34,  8.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9900, loss: 0.1073 ||:   1%|          | 5/840 [00:00<01:24,  9.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9643, loss: 0.1492 ||:   1%|          | 7/840 [00:00<01:16, 10.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9667, loss: 0.1296 ||:   1%|          | 9/840 [00:00<01:11, 11.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9682, loss: 0.1199 ||:   1%|▏         | 11/840 [00:00<01:07, 12.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9654, loss: 0.1124 ||:   2%|▏         | 13/840 [00:01<01:05, 12.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9700, loss: 0.1017 ||:   2%|▏         | 15/840 [00:01<01:02, 13.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9706, loss: 0.1001 ||:   2%|▏         | 17/840 [00:01<00:59, 13.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9737, loss: 0.0933 ||:   2%|▏         | 19/840 [00:01<00:58, 14.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9762, loss: 0.0877 ||:   2%|▎         | 21/840 [00:01<00:57, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9761, loss: 0.0855 ||:   3%|▎         | 23/840 [00:01<00:56, 14.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9760, loss: 0.0865 ||:   3%|▎         | 25/840 [00:01<00:56, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9778, loss: 0.0837 ||:   3%|▎         | 27/840 [00:01<00:55, 14.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9793, loss: 0.0804 ||:   3%|▎         | 29/840 [00:02<00:55, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9806, loss: 0.0765 ||:   4%|▎         | 31/840 [00:02<00:56, 14.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9803, loss: 0.0807 ||:   4%|▍         | 33/840 [00:02<01:00, 13.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9814, loss: 0.0779 ||:   4%|▍         | 35/840 [00:02<00:58, 13.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9784, loss: 0.0870 ||:   4%|▍         | 37/840 [00:02<00:56, 14.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9769, loss: 0.0926 ||:   5%|▍         | 39/840 [00:02<00:56, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9768, loss: 0.0925 ||:   5%|▍         | 41/840 [00:03<01:01, 12.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9767, loss: 0.0935 ||:   5%|▌         | 43/840 [00:03<00:59, 13.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9778, loss: 0.0921 ||:   5%|▌         | 45/840 [00:03<00:56, 14.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9766, loss: 0.0985 ||:   6%|▌         | 47/840 [00:03<00:55, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9765, loss: 0.0979 ||:   6%|▌         | 49/840 [00:03<00:54, 14.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9775, loss: 0.0958 ||:   6%|▌         | 51/840 [00:03<00:54, 14.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9783, loss: 0.0942 ||:   6%|▋         | 53/840 [00:03<00:56, 13.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9782, loss: 0.0943 ||:   7%|▋         | 55/840 [00:03<00:54, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9772, loss: 0.0961 ||:   7%|▋         | 57/840 [00:04<00:54, 14.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9771, loss: 0.0973 ||:   7%|▋         | 59/840 [00:04<00:53, 14.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9779, loss: 0.0951 ||:   7%|▋         | 61/840 [00:04<00:52, 14.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9770, loss: 0.0980 ||:   8%|▊         | 63/840 [00:04<00:52, 14.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9769, loss: 0.0987 ||:   8%|▊         | 65/840 [00:04<00:51, 15.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9769, loss: 0.0979 ||:   8%|▊         | 67/840 [00:04<00:51, 14.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9754, loss: 0.1027 ||:   8%|▊         | 69/840 [00:04<00:52, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9761, loss: 0.1003 ||:   8%|▊         | 71/840 [00:05<00:51, 14.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9767, loss: 0.0984 ||:   9%|▊         | 73/840 [00:05<00:52, 14.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9773, loss: 0.0964 ||:   9%|▉         | 75/840 [00:05<00:56, 13.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9779, loss: 0.0957 ||:   9%|▉         | 77/840 [00:05<00:58, 13.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9785, loss: 0.0937 ||:   9%|▉         | 79/840 [00:05<00:55, 13.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9772, loss: 0.0947 ||:  10%|▉         | 81/840 [00:05<00:57, 13.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9777, loss: 0.0934 ||:  10%|▉         | 83/840 [00:05<00:58, 12.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9771, loss: 0.0934 ||:  10%|█         | 85/840 [00:06<00:57, 13.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9776, loss: 0.0920 ||:  10%|█         | 87/840 [00:06<00:56, 13.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9775, loss: 0.0928 ||:  11%|█         | 89/840 [00:06<00:57, 13.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9775, loss: 0.0920 ||:  11%|█         | 91/840 [00:06<00:55, 13.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9769, loss: 0.0933 ||:  11%|█         | 93/840 [00:06<00:55, 13.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9763, loss: 0.0936 ||:  11%|█▏        | 95/840 [00:06<00:54, 13.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9763, loss: 0.0945 ||:  12%|█▏        | 97/840 [00:07<00:54, 13.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9768, loss: 0.0930 ||:  12%|█▏        | 99/840 [00:07<00:53, 13.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9762, loss: 0.0938 ||:  12%|█▏        | 101/840 [00:07<00:52, 14.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9757, loss: 0.0952 ||:  12%|█▏        | 103/840 [00:07<00:51, 14.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9757, loss: 0.0970 ||:  12%|█▎        | 105/840 [00:07<00:52, 13.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9762, loss: 0.0960 ||:  13%|█▎        | 107/840 [00:07<00:51, 14.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9766, loss: 0.0949 ||:  13%|█▎        | 109/840 [00:07<00:50, 14.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9770, loss: 0.0935 ||:  13%|█▎        | 111/840 [00:07<00:50, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9774, loss: 0.0923 ||:  13%|█▎        | 113/840 [00:08<00:50, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9778, loss: 0.0910 ||:  14%|█▎        | 115/840 [00:08<00:49, 14.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9782, loss: 0.0898 ||:  14%|█▍        | 117/840 [00:08<00:50, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9773, loss: 0.0915 ||:  14%|█▍        | 119/840 [00:08<00:48, 14.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9773, loss: 0.0916 ||:  14%|█▍        | 121/840 [00:08<00:47, 15.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9772, loss: 0.0911 ||:  15%|█▍        | 123/840 [00:08<00:49, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9772, loss: 0.0932 ||:  15%|█▍        | 125/840 [00:08<00:47, 14.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9772, loss: 0.0933 ||:  15%|█▌        | 127/840 [00:09<00:47, 14.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9767, loss: 0.0948 ||:  15%|█▌        | 129/840 [00:09<00:47, 14.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9767, loss: 0.0967 ||:  16%|█▌        | 131/840 [00:09<00:49, 14.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9771, loss: 0.0954 ||:  16%|█▌        | 133/840 [00:09<00:49, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9770, loss: 0.0948 ||:  16%|█▌        | 135/840 [00:09<00:49, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9766, loss: 0.0958 ||:  16%|█▋        | 137/840 [00:09<00:50, 14.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9766, loss: 0.0955 ||:  17%|█▋        | 139/840 [00:09<00:48, 14.50it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9755, loss: 0.0972 ||:  17%|█▋        | 141/840 [00:10<00:54, 12.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9755, loss: 0.0972 ||:  17%|█▋        | 143/840 [00:10<00:52, 13.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9752, loss: 0.0985 ||:  17%|█▋        | 145/840 [00:10<00:51, 13.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9741, loss: 0.1017 ||:  18%|█▊        | 147/840 [00:10<00:52, 13.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9738, loss: 0.1026 ||:  18%|█▊        | 149/840 [00:10<00:50, 13.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9738, loss: 0.1036 ||:  18%|█▊        | 151/840 [00:10<00:49, 14.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9739, loss: 0.1042 ||:  18%|█▊        | 153/840 [00:10<00:47, 14.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9735, loss: 0.1045 ||:  18%|█▊        | 155/840 [00:11<00:46, 14.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9729, loss: 0.1051 ||:  19%|█▊        | 157/840 [00:11<00:45, 14.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9730, loss: 0.1050 ||:  19%|█▉        | 159/840 [00:11<00:44, 15.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1040 ||:  19%|█▉        | 161/840 [00:11<00:45, 15.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1033 ||:  19%|█▉        | 163/840 [00:11<00:45, 14.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9730, loss: 0.1053 ||:  20%|█▉        | 165/840 [00:11<00:45, 14.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9734, loss: 0.1045 ||:  20%|█▉        | 167/840 [00:11<00:46, 14.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9734, loss: 0.1039 ||:  20%|██        | 169/840 [00:12<00:46, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9737, loss: 0.1036 ||:  20%|██        | 171/840 [00:12<00:45, 14.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9734, loss: 0.1046 ||:  21%|██        | 173/840 [00:12<00:44, 14.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9729, loss: 0.1053 ||:  21%|██        | 175/840 [00:12<00:44, 15.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9729, loss: 0.1054 ||:  21%|██        | 177/840 [00:12<00:44, 15.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9732, loss: 0.1045 ||:  21%|██▏       | 179/840 [00:12<00:44, 14.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9735, loss: 0.1039 ||:  22%|██▏       | 181/840 [00:12<00:43, 15.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9735, loss: 0.1046 ||:  22%|██▏       | 183/840 [00:12<00:43, 15.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9735, loss: 0.1050 ||:  22%|██▏       | 185/840 [00:13<00:43, 15.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1053 ||:  22%|██▏       | 187/840 [00:13<00:44, 14.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9728, loss: 0.1066 ||:  22%|██▎       | 189/840 [00:13<00:43, 15.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9728, loss: 0.1063 ||:  23%|██▎       | 191/840 [00:13<00:44, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9725, loss: 0.1081 ||:  23%|██▎       | 193/840 [00:13<00:43, 14.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9728, loss: 0.1075 ||:  23%|██▎       | 195/840 [00:13<00:42, 15.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9728, loss: 0.1082 ||:  23%|██▎       | 197/840 [00:13<00:43, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9731, loss: 0.1074 ||:  24%|██▎       | 199/840 [00:14<00:43, 14.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9731, loss: 0.1067 ||:  24%|██▍       | 201/840 [00:14<00:43, 14.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9732, loss: 0.1064 ||:  24%|██▍       | 203/840 [00:14<00:43, 14.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9734, loss: 0.1056 ||:  24%|██▍       | 205/840 [00:14<00:45, 14.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9734, loss: 0.1052 ||:  25%|██▍       | 207/840 [00:14<00:45, 13.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9732, loss: 0.1054 ||:  25%|██▍       | 209/840 [00:14<00:43, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9730, loss: 0.1056 ||:  25%|██▌       | 211/840 [00:14<00:43, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9732, loss: 0.1050 ||:  25%|██▌       | 213/840 [00:14<00:42, 14.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1047 ||:  26%|██▌       | 215/840 [00:15<00:43, 14.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1041 ||:  26%|██▌       | 217/840 [00:15<00:43, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1044 ||:  26%|██▌       | 219/840 [00:15<00:44, 14.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9731, loss: 0.1044 ||:  26%|██▋       | 221/840 [00:15<00:43, 14.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1037 ||:  27%|██▋       | 223/840 [00:15<00:42, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1040 ||:  27%|██▋       | 225/840 [00:15<00:41, 14.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1038 ||:  27%|██▋       | 227/840 [00:15<00:42, 14.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9734, loss: 0.1042 ||:  27%|██▋       | 229/840 [00:16<00:42, 14.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9734, loss: 0.1038 ||:  28%|██▊       | 231/840 [00:16<00:42, 14.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9736, loss: 0.1032 ||:  28%|██▊       | 233/840 [00:16<00:43, 13.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9738, loss: 0.1025 ||:  28%|██▊       | 235/840 [00:16<00:41, 14.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9738, loss: 0.1029 ||:  28%|██▊       | 237/840 [00:16<00:41, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9738, loss: 0.1024 ||:  28%|██▊       | 239/840 [00:16<00:41, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9739, loss: 0.1024 ||:  29%|██▊       | 241/840 [00:16<00:46, 13.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9741, loss: 0.1019 ||:  29%|██▉       | 243/840 [00:17<00:43, 13.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9743, loss: 0.1013 ||:  29%|██▉       | 245/840 [00:17<00:41, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9743, loss: 0.1018 ||:  29%|██▉       | 247/840 [00:17<00:41, 14.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9745, loss: 0.1013 ||:  30%|██▉       | 249/840 [00:17<00:40, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9743, loss: 0.1031 ||:  30%|██▉       | 251/840 [00:17<00:40, 14.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9741, loss: 0.1029 ||:  30%|███       | 253/840 [00:17<00:41, 14.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9741, loss: 0.1025 ||:  30%|███       | 255/840 [00:17<00:41, 14.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9739, loss: 0.1041 ||:  31%|███       | 257/840 [00:18<00:40, 14.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9741, loss: 0.1034 ||:  31%|███       | 259/840 [00:18<00:39, 14.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9739, loss: 0.1035 ||:  31%|███       | 261/840 [00:18<00:38, 15.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9740, loss: 0.1040 ||:  31%|███▏      | 263/840 [00:18<00:38, 15.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9734, loss: 0.1050 ||:  32%|███▏      | 265/840 [00:18<00:39, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9728, loss: 0.1063 ||:  32%|███▏      | 267/840 [00:18<00:40, 14.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9727, loss: 0.1061 ||:  32%|███▏      | 269/840 [00:18<00:40, 14.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9729, loss: 0.1055 ||:  32%|███▏      | 271/840 [00:19<00:40, 14.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9727, loss: 0.1064 ||:  32%|███▎      | 273/840 [00:19<00:38, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9727, loss: 0.1062 ||:  33%|███▎      | 275/840 [00:19<00:38, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9727, loss: 0.1061 ||:  33%|███▎      | 277/840 [00:19<00:39, 14.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9726, loss: 0.1063 ||:  33%|███▎      | 279/840 [00:19<00:37, 14.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9728, loss: 0.1061 ||:  33%|███▎      | 281/840 [00:19<00:37, 15.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9730, loss: 0.1057 ||:  34%|███▎      | 283/840 [00:19<00:38, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9732, loss: 0.1051 ||:  34%|███▍      | 285/840 [00:19<00:38, 14.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9730, loss: 0.1057 ||:  34%|███▍      | 287/840 [00:20<00:38, 14.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9732, loss: 0.1050 ||:  34%|███▍      | 289/840 [00:20<00:37, 14.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9730, loss: 0.1055 ||:  35%|███▍      | 291/840 [00:20<00:36, 14.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9730, loss: 0.1055 ||:  35%|███▍      | 293/840 [00:20<00:37, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9732, loss: 0.1049 ||:  35%|███▌      | 295/840 [00:20<00:38, 14.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9734, loss: 0.1044 ||:  35%|███▌      | 297/840 [00:20<00:38, 14.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9732, loss: 0.1050 ||:  36%|███▌      | 299/840 [00:20<00:36, 14.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1048 ||:  36%|███▌      | 301/840 [00:21<00:36, 14.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9731, loss: 0.1050 ||:  36%|███▌      | 303/840 [00:21<00:36, 14.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9731, loss: 0.1048 ||:  36%|███▋      | 305/840 [00:21<00:37, 14.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9731, loss: 0.1053 ||:  37%|███▋      | 307/840 [00:21<00:38, 13.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9731, loss: 0.1050 ||:  37%|███▋      | 309/840 [00:21<00:37, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9732, loss: 0.1052 ||:  37%|███▋      | 311/840 [00:21<00:37, 14.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1048 ||:  37%|███▋      | 313/840 [00:21<00:36, 14.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9733, loss: 0.1047 ||:  38%|███▊      | 315/840 [00:22<00:35, 14.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9735, loss: 0.1043 ||:  38%|███▊      | 317/840 [00:22<00:35, 14.54it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9734, loss: 0.1043 ||:  38%|███▊      | 319/840 [00:22<00:35, 14.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9734, loss: 0.1042 ||:  38%|███▊      | 321/840 [00:22<00:34, 14.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9732, loss: 0.1050 ||:  38%|███▊      | 323/840 [00:22<00:35, 14.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9731, loss: 0.1056 ||:  39%|███▊      | 325/840 [00:22<00:36, 14.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9729, loss: 0.1061 ||:  39%|███▉      | 327/840 [00:22<00:35, 14.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9729, loss: 0.1069 ||:  39%|███▉      | 329/840 [00:23<00:35, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9728, loss: 0.1069 ||:  39%|███▉      | 331/840 [00:23<00:34, 14.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9730, loss: 0.1065 ||:  40%|███▉      | 333/840 [00:23<00:34, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9731, loss: 0.1060 ||:  40%|███▉      | 335/840 [00:23<00:35, 14.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9730, loss: 0.1062 ||:  40%|████      | 337/840 [00:23<00:34, 14.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9729, loss: 0.1061 ||:  40%|████      | 339/840 [00:23<00:33, 14.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9729, loss: 0.1062 ||:  41%|████      | 341/840 [00:23<00:38, 13.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9729, loss: 0.1059 ||:  41%|████      | 343/840 [00:24<00:36, 13.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9729, loss: 0.1057 ||:  41%|████      | 345/840 [00:24<00:35, 13.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9728, loss: 0.1056 ||:  41%|████▏     | 347/840 [00:24<00:34, 14.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9729, loss: 0.1052 ||:  42%|████▏     | 349/840 [00:24<00:33, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9731, loss: 0.1049 ||:  42%|████▏     | 351/840 [00:24<00:33, 14.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9727, loss: 0.1055 ||:  42%|████▏     | 353/840 [00:24<00:33, 14.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9724, loss: 0.1058 ||:  42%|████▏     | 355/840 [00:24<00:33, 14.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9725, loss: 0.1054 ||:  42%|████▎     | 357/840 [00:24<00:33, 14.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9727, loss: 0.1050 ||:  43%|████▎     | 359/840 [00:25<00:32, 14.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9723, loss: 0.1057 ||:  43%|████▎     | 361/840 [00:25<00:32, 14.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9722, loss: 0.1058 ||:  43%|████▎     | 363/840 [00:25<00:31, 14.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9719, loss: 0.1068 ||:  43%|████▎     | 365/840 [00:25<00:31, 14.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9719, loss: 0.1065 ||:  44%|████▎     | 367/840 [00:25<00:31, 14.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9721, loss: 0.1061 ||:  44%|████▍     | 369/840 [00:25<00:31, 15.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9718, loss: 0.1067 ||:  44%|████▍     | 371/840 [00:25<00:31, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9718, loss: 0.1071 ||:  44%|████▍     | 373/840 [00:26<00:32, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9717, loss: 0.1075 ||:  45%|████▍     | 375/840 [00:26<00:33, 13.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9718, loss: 0.1074 ||:  45%|████▍     | 377/840 [00:26<00:33, 13.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9716, loss: 0.1080 ||:  45%|████▌     | 379/840 [00:26<00:33, 13.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9717, loss: 0.1083 ||:  45%|████▌     | 381/840 [00:26<00:32, 14.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9715, loss: 0.1086 ||:  46%|████▌     | 383/840 [00:26<00:32, 14.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9716, loss: 0.1089 ||:  46%|████▌     | 385/840 [00:26<00:32, 14.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9717, loss: 0.1084 ||:  46%|████▌     | 387/840 [00:27<00:32, 14.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9717, loss: 0.1082 ||:  46%|████▋     | 389/840 [00:27<00:32, 14.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9717, loss: 0.1086 ||:  47%|████▋     | 391/840 [00:27<00:32, 13.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9716, loss: 0.1086 ||:  47%|████▋     | 393/840 [00:27<00:31, 14.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9718, loss: 0.1081 ||:  47%|████▋     | 395/840 [00:27<00:32, 13.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9717, loss: 0.1081 ||:  47%|████▋     | 397/840 [00:27<00:31, 13.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9717, loss: 0.1078 ||:  48%|████▊     | 399/840 [00:27<00:31, 14.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9717, loss: 0.1081 ||:  48%|████▊     | 401/840 [00:28<00:30, 14.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9716, loss: 0.1088 ||:  48%|████▊     | 403/840 [00:28<00:29, 14.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9717, loss: 0.1084 ||:  48%|████▊     | 405/840 [00:28<00:29, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9716, loss: 0.1096 ||:  48%|████▊     | 407/840 [00:28<00:29, 14.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9715, loss: 0.1104 ||:  49%|████▊     | 409/840 [00:28<00:29, 14.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9713, loss: 0.1113 ||:  49%|████▉     | 411/840 [00:28<00:29, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9714, loss: 0.1109 ||:  49%|████▉     | 413/840 [00:28<00:28, 14.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9714, loss: 0.1108 ||:  49%|████▉     | 415/840 [00:29<00:29, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9713, loss: 0.1112 ||:  50%|████▉     | 417/840 [00:29<00:29, 14.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9714, loss: 0.1111 ||:  50%|████▉     | 419/840 [00:29<00:30, 13.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9714, loss: 0.1113 ||:  50%|█████     | 421/840 [00:29<00:29, 14.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9713, loss: 0.1117 ||:  50%|█████     | 423/840 [00:29<00:29, 14.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9711, loss: 0.1121 ||:  51%|█████     | 425/840 [00:29<00:28, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9707, loss: 0.1124 ||:  51%|█████     | 427/840 [00:29<00:28, 14.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9709, loss: 0.1121 ||:  51%|█████     | 429/840 [00:30<00:27, 14.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9709, loss: 0.1117 ||:  51%|█████▏    | 431/840 [00:30<00:27, 14.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9709, loss: 0.1116 ||:  52%|█████▏    | 433/840 [00:30<00:26, 15.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9710, loss: 0.1113 ||:  52%|█████▏    | 435/840 [00:30<00:25, 15.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9709, loss: 0.1117 ||:  52%|█████▏    | 437/840 [00:30<00:25, 15.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9710, loss: 0.1116 ||:  52%|█████▏    | 439/840 [00:30<00:24, 16.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9709, loss: 0.1115 ||:  52%|█████▎    | 441/840 [00:30<00:27, 14.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9710, loss: 0.1112 ||:  53%|█████▎    | 443/840 [00:30<00:27, 14.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9711, loss: 0.1108 ||:  53%|█████▎    | 445/840 [00:31<00:26, 14.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9711, loss: 0.1109 ||:  53%|█████▎    | 447/840 [00:31<00:25, 15.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9712, loss: 0.1108 ||:  53%|█████▎    | 449/840 [00:31<00:24, 15.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9710, loss: 0.1118 ||:  54%|█████▎    | 451/840 [00:31<00:23, 16.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9708, loss: 0.1123 ||:  54%|█████▍    | 453/840 [00:31<00:23, 16.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9708, loss: 0.1125 ||:  54%|█████▍    | 455/840 [00:31<00:23, 16.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9706, loss: 0.1134 ||:  54%|█████▍    | 457/840 [00:31<00:24, 15.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9707, loss: 0.1130 ||:  55%|█████▍    | 459/840 [00:31<00:25, 15.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9708, loss: 0.1126 ||:  55%|█████▍    | 461/840 [00:32<00:24, 15.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9710, loss: 0.1122 ||:  55%|█████▌    | 463/840 [00:32<00:24, 15.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9710, loss: 0.1122 ||:  55%|█████▌    | 465/840 [00:32<00:24, 15.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9710, loss: 0.1122 ||:  56%|█████▌    | 467/840 [00:32<00:25, 14.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9711, loss: 0.1120 ||:  56%|█████▌    | 469/840 [00:32<00:24, 14.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9711, loss: 0.1118 ||:  56%|█████▌    | 471/840 [00:32<00:24, 15.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9710, loss: 0.1120 ||:  56%|█████▋    | 473/840 [00:32<00:23, 15.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9712, loss: 0.1117 ||:  57%|█████▋    | 475/840 [00:32<00:24, 15.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9711, loss: 0.1121 ||:  57%|█████▋    | 477/840 [00:33<00:23, 15.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9711, loss: 0.1121 ||:  57%|█████▋    | 479/840 [00:33<00:24, 15.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9712, loss: 0.1118 ||:  57%|█████▋    | 481/840 [00:33<00:24, 14.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9712, loss: 0.1116 ||:  57%|█████▊    | 483/840 [00:33<00:24, 14.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9710, loss: 0.1119 ||:  58%|█████▊    | 485/840 [00:33<00:23, 14.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9711, loss: 0.1116 ||:  58%|█████▊    | 487/840 [00:33<00:23, 14.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9710, loss: 0.1126 ||:  58%|█████▊    | 489/840 [00:33<00:23, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9710, loss: 0.1126 ||:  58%|█████▊    | 491/840 [00:34<00:24, 14.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9711, loss: 0.1124 ||:  59%|█████▊    | 493/840 [00:34<00:23, 14.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9709, loss: 0.1133 ||:  59%|█████▉    | 495/840 [00:34<00:23, 14.44it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9709, loss: 0.1137 ||:  59%|█████▉    | 497/840 [00:34<00:24, 14.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9707, loss: 0.1142 ||:  59%|█████▉    | 499/840 [00:34<00:24, 13.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9708, loss: 0.1141 ||:  60%|█████▉    | 501/840 [00:34<00:24, 13.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9708, loss: 0.1139 ||:  60%|█████▉    | 503/840 [00:34<00:24, 13.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9707, loss: 0.1142 ||:  60%|██████    | 505/840 [00:35<00:24, 13.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9708, loss: 0.1139 ||:  60%|██████    | 507/840 [00:35<00:23, 14.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9706, loss: 0.1141 ||:  61%|██████    | 509/840 [00:35<00:22, 14.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9707, loss: 0.1138 ||:  61%|██████    | 511/840 [00:35<00:22, 14.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9708, loss: 0.1138 ||:  61%|██████    | 513/840 [00:35<00:22, 14.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9707, loss: 0.1145 ||:  61%|██████▏   | 515/840 [00:35<00:22, 14.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9707, loss: 0.1145 ||:  62%|██████▏   | 517/840 [00:35<00:22, 14.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9706, loss: 0.1143 ||:  62%|██████▏   | 519/840 [00:36<00:22, 14.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9705, loss: 0.1147 ||:  62%|██████▏   | 521/840 [00:36<00:22, 14.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9707, loss: 0.1144 ||:  62%|██████▏   | 523/840 [00:36<00:21, 14.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9706, loss: 0.1144 ||:  62%|██████▎   | 525/840 [00:36<00:21, 14.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9706, loss: 0.1143 ||:  63%|██████▎   | 527/840 [00:36<00:21, 14.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9703, loss: 0.1147 ||:  63%|██████▎   | 529/840 [00:36<00:21, 14.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9703, loss: 0.1148 ||:  63%|██████▎   | 531/840 [00:36<00:21, 14.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9702, loss: 0.1156 ||:  63%|██████▎   | 533/840 [00:37<00:21, 14.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9703, loss: 0.1153 ||:  64%|██████▎   | 535/840 [00:37<00:21, 14.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9702, loss: 0.1155 ||:  64%|██████▍   | 537/840 [00:37<00:21, 14.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1157 ||:  64%|██████▍   | 539/840 [00:37<00:20, 14.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9702, loss: 0.1155 ||:  64%|██████▍   | 541/840 [00:37<00:23, 12.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9703, loss: 0.1155 ||:  65%|██████▍   | 543/840 [00:37<00:22, 13.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9702, loss: 0.1161 ||:  65%|██████▍   | 545/840 [00:37<00:22, 13.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9700, loss: 0.1166 ||:  65%|██████▌   | 547/840 [00:38<00:20, 13.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9699, loss: 0.1171 ||:  65%|██████▌   | 549/840 [00:38<00:20, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9700, loss: 0.1168 ||:  66%|██████▌   | 551/840 [00:38<00:20, 14.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1165 ||:  66%|██████▌   | 553/840 [00:38<00:19, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1162 ||:  66%|██████▌   | 555/840 [00:38<00:20, 14.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1160 ||:  66%|██████▋   | 557/840 [00:38<00:19, 14.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1161 ||:  67%|██████▋   | 559/840 [00:38<00:19, 14.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9702, loss: 0.1158 ||:  67%|██████▋   | 561/840 [00:38<00:18, 14.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9702, loss: 0.1157 ||:  67%|██████▋   | 563/840 [00:39<00:18, 15.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9700, loss: 0.1162 ||:  67%|██████▋   | 565/840 [00:39<00:18, 14.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1159 ||:  68%|██████▊   | 567/840 [00:39<00:19, 14.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9700, loss: 0.1160 ||:  68%|██████▊   | 569/840 [00:39<00:18, 14.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1157 ||:  68%|██████▊   | 571/840 [00:39<00:18, 14.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1157 ||:  68%|██████▊   | 573/840 [00:39<00:18, 14.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1158 ||:  68%|██████▊   | 575/840 [00:39<00:18, 14.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1160 ||:  69%|██████▊   | 577/840 [00:40<00:18, 14.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9699, loss: 0.1161 ||:  69%|██████▉   | 579/840 [00:40<00:17, 14.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9700, loss: 0.1159 ||:  69%|██████▉   | 581/840 [00:40<00:17, 14.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1156 ||:  69%|██████▉   | 583/840 [00:40<00:17, 14.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9701, loss: 0.1155 ||:  70%|██████▉   | 585/840 [00:40<00:17, 14.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9698, loss: 0.1162 ||:  70%|██████▉   | 587/840 [00:40<00:17, 14.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9697, loss: 0.1166 ||:  70%|███████   | 589/840 [00:40<00:17, 14.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9696, loss: 0.1167 ||:  70%|███████   | 591/840 [00:41<00:17, 14.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9695, loss: 0.1168 ||:  71%|███████   | 593/840 [00:41<00:17, 14.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9695, loss: 0.1166 ||:  71%|███████   | 595/840 [00:41<00:16, 14.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9693, loss: 0.1167 ||:  71%|███████   | 597/840 [00:41<00:16, 14.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9692, loss: 0.1172 ||:  71%|███████▏  | 599/840 [00:41<00:16, 14.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9691, loss: 0.1174 ||:  72%|███████▏  | 601/840 [00:41<00:16, 14.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "accuracy: 0.9692, loss: 0.1170 ||:  72%|███████▏  | 603/840 [00:41<00:15, 14.91it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-986-727ffdc5a623>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m                   \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                   num_epochs=20)\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;31m# get peak of memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    350\u001b[0m                                                        update_norm / (param_norm + 1e-7))\n\u001b[1;32m    351\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;31m# Update moving averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "reader = ClassificationReader()\n",
    "\n",
    "train_dataset = reader.read('train.txt')\n",
    "dev_dataset = reader.read('dev.txt')\n",
    "\n",
    "# First we define the word-vocabulary\n",
    "vocab = Vocabulary.from_instances(train_dataset + dev_dataset)\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "# Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
    "# (usually a sequence of embedded word vectors), processes it, and returns a single\n",
    "# vector. \n",
    "encoder = PytorchSeq2VecWrapper(\n",
    "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "model = LstmClassifier(word_embeddings, encoder, vocab)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "iterator = BasicIterator(batch_size=20)\n",
    "\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=dev_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=20)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Predictor.register(\"sentence_classifier_predictor\")\n",
    "class SentenceClassifierPredictor(Predictor):\n",
    "    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n",
    "        super().__init__(model, dataset_reader)\n",
    "        self._tokenizer = SpacyWordSplitter(language='en_core_web_sm', pos_tags=True)\n",
    "\n",
    "    def predict(self, sentence: str) -> JsonDict:\n",
    "        return self.predict_json({\"sentence\" : sentence})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        sentence = json_dict[\"sentence\"]\n",
    "        tokens = self._tokenizer.split_words(sentence)\n",
    "        return self._dataset_reader.text_to_instance([str(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "accuracy: 0.9692, loss: 0.1170 ||:  72%|███████▏  | 603/840 [01:01<00:15, 14.91it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "reader = ClassificationReader()\n",
    "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
    "logits = predictor.predict('way way go great trade deal go renegotiate trade deal go bring job home go bring job home incompetently work trade deal negotiate probably history world start NAFTA')['logits']\n",
    "label_id = np.argmax(logits)\n",
    "\n",
    "print(model.vocab.get_token_from_index(label_id, 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.th\", 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save_to_files(\"vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
