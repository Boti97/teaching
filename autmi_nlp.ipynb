{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTMI Seminar 2019/2020 Spring\n",
    "\n",
    "## Natural Language Processing\n",
    "\n",
    "## April , 2020\n",
    "\n",
    "# Text representations and analysis\n",
    "\n",
    "## Preparation\n",
    "\n",
    "[Download GLOVE](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "\n",
    "!pip install textacy\n",
    "\n",
    "!pip install flair\n",
    "\n",
    "!pip install torchtext\n",
    "\n",
    "!pip install -U scikit-learn\n",
    "\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations\n",
    "\n",
    "For humans meaningful representation are strings, but the computer needs numerical representations to be able to run machine learning algorithms. The easiest approach is to create a `word ---> id` mapping that is going to map words to integer ids starting from 0. Different words should have a different id.\n",
    "\n",
    "This is called **one-hot encoding**. Let's encode the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"yesterday the lazy dog went to the store to buy food\".split(\" \")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = dict()\n",
    "max_id = 0\n",
    "\n",
    "for word in sentence:\n",
    "    # a word we have not seen before\n",
    "    if word not in mapping:\n",
    "        # assign the smallest unused id\n",
    "        mapping[word] = max_id\n",
    "        # increment the id for the next word\n",
    "        max_id = max_id + 1\n",
    "        \n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "- When representing words with id's we assign them to the words in the order of the encounter. \n",
    "- This means that we will assign different vectors to the words each time we run the algorithm.\n",
    "- We also have no concept of similarity, intuitively: `similarity(cat, dog) > similarity(cat, computer)`\n",
    "- The representation is very sparse and could have very high dimension, which would also slow the computations.\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "- map each word to a small dimensional (around 100-300) continuous vectors.\n",
    "- this means that similar words should have similar vectors.\n",
    "    - what do we mean by word similarity ?\n",
    "    \n",
    "    \n",
    "### Cosine similarity\n",
    "\n",
    "- Now that we have word vectors, we need a way to quantify the similarity between individual words, according to these vectors. One such metric is cosine-similarity. We will be using this to find words that are \"close\" and \"far\" from one another.\n",
    "\n",
    "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$ \n",
    "\n",
    "    \n",
    "### Creating word embeddings\n",
    "\n",
    "\"a word is characterized by the company it keeps\" -- popularized by Firth\n",
    "\n",
    "- A popular theory is that words are as similar as their context is\n",
    "- Word embeddings are also created with neural networks\n",
    "    1. predict a missing word based on its context\n",
    "    2. predict a word's context given the word itself\n",
    "\n",
    "To create word embeddings, a neural network is trained to perform the tasks. But then it is not used actually for the task it was trained it on. The goal is actually to learn the weights of the hidden layer. Then, these weights will be our vectors called \"word embeddings\".\n",
    "\n",
    "Given a specific word, a neural network will look at the words nearby and learn the probability of being the \"nearby word\". The \"nearby\" is actually given by a windows size that is a parameter of the algorithm (dog is more likely to appear next to cat than computer).\n",
    "\n",
    "The training examples are generated from big text corpuses. For example from the sentence “The quick brown fox jumps over the lazy dog.” we can generate the following inputs:\n",
    "\n",
    "![training examples](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "\n",
    "To do this, we first build a vocabulary of words from our training documents–let’s say we have a vocabulary of 10,000 unique words.\n",
    "First we build the vocabulary of our documents, then for representing words, we will use one-hot vectors. The output of the network will be a single vector that contains the probabilities for the \"nearby\" words.\n",
    "\n",
    "![architecture](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "\n",
    "_(images from mccormicklm.com)_\n",
    "\n",
    "### Famous static word embeddings for English\n",
    "\n",
    "- Word2vec\n",
    "- GLOVE\n",
    "\n",
    "### Contextual embeddings?\n",
    "\n",
    "- Elmo\n",
    "- BERT\n",
    "- Flair\n",
    "\n",
    "For static embeddings, we will use a GLOVE embedding of 100 dimensional vectors trained on 6B tokens.\n",
    "\n",
    "[Download GLOVE](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "embedding_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embedding = gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_vector = embedding[\"dog\"]\n",
    "type(dog_vector), dog_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.most_similar(\"president\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.similarity(\"woman\", \"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model, size=500):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "    for word in model.wv.vocab:\n",
    "        if len(tokens) > size:\n",
    "            break\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(embedding, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual embeddings\n",
    "\n",
    "In GloVe and Word2vec representations, words have a static representation. But words can have different meaning in different contexts, e.g. the word \"stick\":\n",
    "\n",
    "1. Find some dry sticks and we'll make a campfire.\n",
    "2. Let's stick with glove embeddings.\n",
    "\n",
    "![elmo](http://jalammar.github.io/images/elmo-embedding-robin-williams.png)\n",
    "\n",
    "_(Peters et. al., 2018 in the ELMo paper)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentence objects holds a sentence that we may want to embed or tag\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "\n",
    "# init embedding\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "\n",
    "# create a sentence\n",
    "sentence1 = Sentence(\"Find some dry sticks and we'll make a campfire.\")\n",
    "sentence2 = Sentence(\"Let's stick with glove embeddings.\")\n",
    "\n",
    "# embed words in sentence\n",
    "flair_embedding_forward.embed(sentence2)\n",
    "for token in sentence2:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Flair, a pretrained NER tagger is also available to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load matplotlib, pandas and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "matplotlib.pyplot.rcParams['figure.figsize'] = (16, 10)\n",
    "matplotlib.pyplot.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.pyplot.rcParams['font.size'] = 20\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analyzation\n",
    "\n",
    "- we use nlp frameworks for the basic tasks\n",
    "- for the preprocessing tasks (lemmatization, tokenization) we use [spaCy](https://spacy.io/)\n",
    "- for keyword extraction and various text analyzation tasks we use [textacy](https://github.com/chartbeat-labs/textacy)\n",
    "- textacy builds on spaCy output\n",
    "- both are open source ython libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext.datasets import text_classification\n",
    "import os\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./data', ngrams=NGRAMS, vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"./data/ag_news_csv/train.csv\",quotechar='\"', names=['label', 'title', 'description'])\n",
    "test_data = pd.read_csv(\"./data/ag_news_csv/test.csv\",quotechar='\"', names=['label', 'title', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"text\"] = train_data.title +  \",\" + train_data.description\n",
    "train_data = train_data.drop(\"title\", axis=1)\n",
    "train_data = train_data.drop(\"description\", axis=1)\n",
    "\n",
    "test_data[\"text\"] = test_data.title +  \",\" + test_data.description\n",
    "test_data = test_data.drop(\"title\", axis=1)\n",
    "test_data = test_data.drop(\"description\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(train_data.label).size().plot.pie(subplots=True,figsize=(5, 10),autopct=\"%.0lf%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Donald Trump called and asked me to serve as his running mate and Vice Presidential nominee.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in doc:\n",
    "    print(tok.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 100})\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sports = train_data[train_data.label == 2]\n",
    "\n",
    "text = \" \".join(text_sports.text.tolist())\n",
    "doc_text = nlp(text[:200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "from textacy.extract import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "Counter([ng.text.lower() for n in [2,4] for ng in ngrams(doc_text, n)]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textacy can use graph based keyword extraction methods.\n",
    "\n",
    "* TextRank (focuses on words)\n",
    "* SingleRank (focueses on phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy import keyterms\n",
    "\n",
    "keyterms.textrank(\n",
    "    doc_text,\n",
    "    normalize = \"lemma\",\n",
    "    n_keyterms=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textacy.keyterms.singlerank(\n",
    "    doc_text,\n",
    "    normalize = \"lemma\",\n",
    "    n_keyterms=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract entities from the doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter \n",
    "words = [tok for tok in doc_text if tok.is_alpha and not tok.is_stop]\n",
    "word_probs = {tok.text.lower(): tok.prob for tok in words}\n",
    "\n",
    "freqs = Counter(tok.text for tok in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "print(len(freqs))\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=30, scale=1.5).generate_from_frequencies(freqs)\n",
    "image = wordcloud.to_image()\n",
    "image.save(\"./wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename='./wordcloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = train_data.groupby('label').apply(lambda x: x.sample(frac=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a new column to the table which will contain the cleaned and preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "clean_text = []\n",
    "for text in tqdm(sample_df['text']):\n",
    "    doc = nlp(text)\n",
    "    words = []\n",
    "    for tok in doc:\n",
    "        if not tok.is_stop and tok.is_alpha:\n",
    "            words.append(tok.lemma_)\n",
    "    clean_text.append(words)\n",
    "\n",
    "# Add cleaned text to dataframe\n",
    "sample_df['clean_text'] = clean_text\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables for dependent and independent variables\n",
    "labels = sample_df.label.tolist()\n",
    "data = sample_df['clean_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the pretrained glove embedding\n",
    "# To handle the Seq2Vec method, we take the mean of the word-vectors\n",
    "def vectorize(tr_data, tst_data):\n",
    "    print('\\nLoading existing glove model...')\n",
    "    embedding_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=False)\n",
    "    vectorizer = model.wv\n",
    "    vocab_length = len(model.wv.vocab)\n",
    "    \n",
    "    tr_vectors = [\n",
    "        np.array(np.mean([vectorizer[word] if word in model else np.zeros((100,)) for word in article], axis=0)) for article in tqdm(tr_data,'Vectorizing')\n",
    "    ]\n",
    "    \n",
    "    tst_vectors = [\n",
    "        np.array(np.mean([vectorizer[word] if word in model else np.zeros((100,)) for word in article], axis=0)) for article in tqdm(tst_data,'Vectorizing')\n",
    "    ]\n",
    "    \n",
    "    return tr_vectors, tst_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(data, labels):\n",
    "    tr_data,tst_data,tr_labels,tst_labels = split(data,labels,test_size=0.3)\n",
    "    \n",
    "    tst_vecs = []\n",
    "    tr_vecs = []\n",
    "    tr_vecs, tst_vecs = vectorize(tr_data, tst_data)    \n",
    "    return tr_vecs, tr_labels, tst_vecs, tst_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vecs, tr_labels, tst_vecs, tst_labels = get_features_and_labels(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can try different classifiers as well\n",
    "- Multiple are available from [scikit-learn](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf  =  RandomForestClassifier(n_estimators=100, verbose=True, n_jobs=-1)\n",
    "svc = SVC()\n",
    "lr  = LogisticRegression(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(tr_vecs, tr_labels)\n",
    "svc.fit(tr_vecs, tr_labels)\n",
    "lr.fit(tr_vecs, tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(type(tst_vecs))\n",
    "rf_pred = rf.predict(tst_vecs)\n",
    "svc_pred = svc.predict(tst_vecs)\n",
    "lr_pred = lr.predict(tst_vecs)\n",
    "print(\"Random Forest Test accuracy : {}\".format(accuracy_score(tst_labels, rf_pred)))\n",
    "print(\"SVC Test accuracy : {}\".format(accuracy_score(tst_labels, svc_pred)))\n",
    "print(\"Logistic Regression Test accuracy : {}\".format(accuracy_score(tst_labels, lr_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Learning model with pytorch and torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"dataset_test.csv\", index=False)\n",
    "train_data.to_csv(\"dataset_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('label',LABEL),('text', TEXT)]\n",
    "\n",
    "train, test = data.TabularDataset.splits(\n",
    "                                        path = '.',\n",
    "                                        train = 'dataset_train.csv',\n",
    "                                        test = 'dataset_test.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars(train.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train, valid = train.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train)}')\n",
    "print(f'Number of validation examples: {len(valid)}')\n",
    "print(f'Number of testing examples: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors =\"glove.6B.100d\")  \n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train, valid, test), batch_size = BATCH_SIZE,\n",
    "                                                                           sort_key = lambda x: len(x.text),\n",
    "                                                                           sort_within_batch = False,\n",
    "                                                                           device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        self.embedding.weight.requires_grad=False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.lstm(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "                \n",
    "        y = self.fc(hidden[-1])\n",
    "        \n",
    "        log_probs = F.log_softmax(y.squeeze(0))\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 4\n",
    "\n",
    "model = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    target_names = ['class 0', 'class 1', 'class 2', 'class 3']\n",
    "    print(classification_report(rounded_preds.cpu().numpy(), y.cpu().numpy(), target_names=target_names))\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.text)\n",
    "\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 15\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
